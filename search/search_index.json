{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Treex A Pytree-based Module system for JAX Intuitive : Modules are simple Python objects that respect Object-Oriented semantics and should make PyTorch users feel at home, with no need for separate dictionary structures or complex apply methods. Pytree-based : Modules are registered as JAX PyTrees, enabling their use with any JAX function. No need for specialized versions of jit , grad , vmap , etc. Expressive : In Treex you use type annotations to define what the different parts of your module represent (submodules, parameters, batch statistics, etc), this leads to a very flexible and powerful state management solution. Flax-based Implementations : Writing high-quality, battle-tested code for common layers is hard. For this reason Modules in treex.nn are wrappers over their Flax counterparts. We keep identical signatures, enabling Flax users to feel at home but still benefiting from the simpler Pytorch-like experience Treex brings. Why Treex? Despite all JAX benefits, current Module systems are not intuitive to new users and add additional complexity not present in frameworks like PyTorch or Keras. Treex takes inspiration from S4TF and delivers an intuitive experience using JAX Pytree infrastructure. Current Alternative's Drawbacks and Solutions Currently we have many alternatives like Flax, Haiku, Objax, that have one or more of the following drawbacks: * Module structure and parameter structure are separate, and parameters have to be manipulated around by the end-user, which is not intuitive. In Treex, parameters are stored in the modules themselves and can be accessed directly. * Monadic architecture adds complexity. Flax and Haiku use an `apply` method to call modules that set a context with parameters, rng, and different metadata, which adds additional overhead to the API and creates an asymmetry in how Modules are being used inside and outside a context. In Treex, modules can be called directly. * Among different frameworks, parameter surgery requires special consideration and is challenging to implement. Consider a standard workflow such as transfer learning, transferring parameters and state from a pre-trained module or submodule as part of a new module; in different frameworks, we have to know precisely how to extract their parameters and how to insert them into the new parameter structure/dictionaries such that it is in agreement with the new module structure. In Treex, just as in PyTorch / Keras, we enable to pass the (sub)module to the new module, and parameters are automatically added to the new structure. * Multiple frameworks deviate from JAX semantics and require particular versions of `jit`, `grad`, `vmap`, etc., which makes it harder to integrate with other JAX libraries. Treex's Modules are plain old JAX PyTrees and are compatible with any JAX library that supports them. * Other Pytree-based approaches like Parallax and Equinox do not have a total state management solution to handle complex states as encountered in Flax. Treex has the Filter and Update API, which is very expressive and can effectively handle systems with a complex state. Installation Install using pip: pip install treex Status At the moment Treex is a proof of concept for what a Pytree-based Module system for JAX could look like. Testing is needed to find bugs and potential issues, however, since Treex layers are numerically equivalent to Flax this borrows some maturity and yields more confidence over its results. Feedback is much appreciated. Roadmap : - [x] Finish prototyping core API - [ ] Wrap all Flax Linen Modules - [ ] Document public API - [ ] Create documentation site Getting Started This is a small appetizer to give you a feel for how using Treex looks like, be sure to checkout the Guide section below for details on more advanced usage. from typing import Sequence, List import jax import jax.numpy as jnp import numpy as np import treex as tx # you can use tx.MLP but we will create our own as an example class MLP (tx . Module): layers: List[tx . Linear] def __init__ ( self , features: Sequence[ int ]): self . layers = [ tx . Linear(din, dout) for din, dout in zip (features[: -1 ], features[ 1 :]) ] def __call__ ( self , x): for linear in self . layers[: -1 ]: x = jax . nn . relu(linear(x)) return self . layers[ -1 ](x) model = MLP([ 1 , 12 , 8 , 1 ]) . init( 42 ) x = np . random . uniform( -1 , 1 , size = ( 100 , 1 )) y = 1.4 * x ** 2 - 0.3 + np . random . normal(scale =0.1 , size = ( 100 , 1 )) @jax . jit @jax . grad def loss_fn (model, x, y): y_pred = model(x) return jnp . mean((y_pred - y) ** 2 ) # in reality use optax def sdg (param, grad): return param - 0.01 * grad # training loop for step in range ( 10_000 ): grads = loss_fn(model, x, y) model = jax . tree_map(sdg, model, grads) model = model . eval() y_pred = model(x) Guide Defining Modules Treex Modules have the following characteristics: * They inherit from tx.Module . * Fields for parameter and submodules MUST be marked using a valid type annotation. class Linear (tx . Module): w: tx . Parameter b: tx . Parameter def __init__ ( self , din, dout): self . w = tx . Initializer( lambda key: jax . random . uniform(key, shape = (din, dout))) self . b = jnp . zeros(shape = (dout,)) def __call__ ( self , x): return jnp . dot(x, self . w) + self . b linear = Linear( 3 , 5 ) . init( 42 ) y = linear(x) Valid type annotations include: * Subtypes of tx.TreePart e.g. tx.Parameter , tx.BatchStat , etc. * Subtypes of tx.Module e.g. tx.Linear , custom Module types, etc. * Generic subtypes from the typing module of the previous e.g. List[tx.Parameter] or Dict[str, tx.Linear] . Type annotations that do not comform to the above rules will be ignored and the field will not be counted as part of the Pytree. class MLP (tx . Module): layers: List[tx . Linear] def __init__ ( self , features: Sequence[ int ]): self . layers = [ tx . Linear(din, dout) for din, dout in zip (features[: -1 ], features[ 1 :]) ] def __call__ ( self , x): for linear in self . layers[: -1 ]: x = jax . nn . relu(linear(x)) return self . layers[ -1 ](x) mlp = MLP([ 3 , 5 , 2 ]) . init( 42 ) Pytrees Since Modules are pytrees they can be arguments to JAX functions such as jit , grad , vmap , etc, and the jax.tree_* function family. @jax . jit @jax . grad def loss_fn (model, x, y): y_pred = model(x) return jnp . mean((y_pred - y) ** 2 ) def sdg (param, grad): return param - 0.01 * grad model = MLP( ... ) . init( 42 ) grads = loss_fn(model, x, y) model = jax . tree_map(sdg, model, grads) This makes Treex Modules compatible with tooling from the broader JAX ecosystem, and enables correct unification of Modules as both parameter containers and the definition of the foward computation. Initialization Initialization in Treex is done by calling the init method on the Module with a seed. This returns a new Module with all fields initialized. There are two initialization mechanisms in Treex. The first one is setting the fields we wish to initialize to an Initializer object. Initializer s contain functions that take a key and return the initial value of the field: class MyModule (tx . Module): a: tx . Parameter b: tx . Parameter def __init__ ( self ): self . a = tx . Initializer( lambda key: jax . random . uniform(key, shape = ( 1 ,))) self . b = 2 module = MyModule() module # MyModule(a=Initializer, b=2) moduel . initialized # False module = module . init( 42 ) module # MyModule(a=array([0.034...]), b=2) module . initialized # True The second is to override the module_init method, which takes a key and can initialize any required fields. This is useful for modules that require complex initialization logic or whose field's initialization depends on each other. class MyModule (tx . Module): a: tx . Parameter b: tx . Parameter def __init__ ( self ): self . a = None self . b = None def module_init ( self , key): # some complex initialization ... module = MyModule() . init( 42 ) module # MyModule(a=array([0.927...]), b=array([0.749...])) We can also mix and match the two strategies, meaning that some parameters can be initialized via Initializer s while others via module_init . The rule is that Initializer s are always going the be called first. Filter and Update API The filter method allows selecting a subtree by filtering based on a type, all leaves that are not a subclass of such type are set to a special Nothing value. class MyModule (tx . Module): a: tx . Parameter = np . array( 1 ) b: tx . BatchStat = np . array( 2 ) ... module = MyModule( ... ) module . filter(tx . Parameter) # MyModule(a=array([1]), b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=array([2])) Nothing much like None is an empty Pytree so it gets ignored by tree operations: jax . tree_leaves(module . filter(tx . Parameter)) # [array([1])] jax . tree_leaves(module . filter(tx . BatchStat)) # [array([2])] A typical use case is to define params as a Parameter filter and pass it as the first argument to grad so that the gradient is computed only that particular subset and immediately update them back to the model before performing any computation: # we take `params` as a Parameter filter from model # but model itself is left untouched params = model . filter(tx . Parameter) @jax . grad def loss_fn (params, model, x, y): # update traced arrays by `grad` from `params` model = model . update(params) ... grads = loss_fn(params, model, x, y) optimizer = tx . Optimizer(optax . adam( 1e-3 )) optimizer = optimizer . init(params) # only needs params Optimizer Optax is an amazing library however, its optimizers are not pytrees, this means that state and computation are separate, and you cannot jit them. To solve this Treex provides an tx.Optimizer class that can wrap any Optax optimizer and make it a Pytree. While in optax you would define something like this: def main (): ... optimizer = optax . adam( 1e-3 ) opt_state = optimizer . init(params) ... @partial (jax . jit, static_argnums = ( 4 ,)) def train_step (model, x, y, opt_state, optimizer): # optimizer has to be static ... updates, opt_state = optimizer . update(grads, opt_state, params) params = optax . apply_updates(params, updates) ... return model, loss, opt_state With tx.Optimizer you it can be simplified to: def main (): ... optimizer = tx . Optimizer(optax . adam( 1e-3 )) . init(params) ... jax . jit def train_step (model, x, y, optimizer): ... params = optimizer . update(grads, params) ... return model, loss, optimizer As you see, tx.Optimizer follows the same API as optax.GradientTransformation except that: 1. There is no opt_state , instead optimizer IS the state. 2. update by default applies the gradient updates to the parameters. 3. update updates the internal state of the optimizer in-place. Notice that since tx.Optimizer is a Pytree, it was passed through jit without the need to specify static_argnums . State Management Treex takes a \"direct\" approach to state management, i.e., state is updated in-place by the Module whenever it needs to. For example, this module will calculate the running average of its input: class Average (tx . Module): count: tx . State total: tx . State def __init__ ( self ): self . count = jnp . array( 0 ) self . total = jnp . array( 0.0 ) def __call__ ( self , x): self . count += np . prod(x . shape) self . total += jnp . sum(x) return self . total / self . count Treex Modules that require random state will often keep a rng key internally and update it in-place when needed: class Dropout (tx . Module): rng: tx . Rng def __init__ ( self , rate: float ): self . rng = tx . Initializer( lambda key: key) ... def __call__ ( self , x): key, self . rng = jax . random . split( self . rng) ... Finally tx.Optimizer also performs inplace updates inside the update method, here is a sketch of how it works: class Optimizer (tx . TreeObject): opt_state: tx . OptState optimizer: optax . GradientTransformation def update ( self , grads, params): ... updates, self . opt_state = self . optimizer . update( grads, self . opt_state, params ) ... What is the catch? State management is one of the most challenging things in JAX, but with the help of Treex it seems effortless, what is the catch? As always there is a trade-off to consider: Treex's approach requires to consider how to propagate state changes properly while taking into account the fact that Pytree operations create new objects, that is, since reference do not persist across calls through these functions changes might be lost. A standard solution to this problem is: always output the module to update state . For example, a typical loss function that contains a stateful model would look like this: @partial (jax . value_and_grad, has_aux = True ) def loss_fn (params, model, x, y): model = model . update(params) y_pred = model(x) loss = jnp . mean((y_pred - y) ** 2 ) return loss, model params = model . filter(tx . Parameter) (loss, model), grads = loss_fn(params, model, x, y) ... Here model is returned along with the loss through value_and_grad to update model on the outside thus persisting any changes to the state performed on the inside. Training State Treex Modules have a training: bool property that specifies whether the module is in training mode or not. This property conditions the behavior of Modules such as Dropout and BatchNorm , which behave differently between training and evaluation. To switch between modes, use the .train() and .eval() methods, they return a new Module whose training state and the state of all of its submodules (recursively) are set to the desired value. # training loop for step in range ( 1000 ): loss, model, opt_state = train_step(model, x, y, opt_state) # prepare for evaluation model = model . eval() # make predictions y_pred = model(X_test) Parameter Annotations The role of each parameter is defined by its annotation. While valid annotations is any type which inherits from tx.TreePart , the default annotations from Treex are currently organized into the following type hierarchy: Graph code graph TD; TreePart-->Parameter; TreePart-->State; State-->Rng; State-->BatchStat; This is useful because you can make specific or more general queries using filter depending on what you want to achive. e.g. rngs = model . filter(tx . Rng) batch_stats = model . filter(tx . BatchStat) all_states = model . filter(tx . State) # union of the previous two You can easily define you own annotations by inheriting from directly tx.TreePart or any of its subclasses. As an example lets create a new Cache state to emulates Flax's cache collection: class Cache (tx . TreePart): pass That is it! Now you can use it in your model: class MyModule (tx . Module): memory: Cache ... Tip : Your static analyzer will probably start complaining if you try to assign an jnp.ndarray to memory in this example because ndarray s are not TreePart s. While this makes sense, we want to trick the static analyzer into thinking Cache represents an ndarray and not a TreePart , the easiest way to do this is to use typing.cast : from typing import cast, Type import jax.numpy as jnp class Cache (tx . TreePart): pass Cache = cast(Type[jnp . ndarray], Cache) cast an identity function, meaning Cache is actually reassigned to itself, however, the static analyzer will now think its an ndarray . This way both the static analyzer and Treex will be happy. Full Example from functools import partial import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import optax import treex as tx x = np . random . uniform(size = ( 500 , 1 )) y = 1.4 * x - 0.3 + np . random . normal(scale =0.1 , size = ( 500 , 1 )) # treex already defines tx.Linear but we can define our own class Linear (tx . Module): w: tx . Parameter b: tx . Parameter def __init__ ( self , din, dout): self . w = tx . Initializer( lambda key: jax . random . uniform(key, shape = (din, dout))) self . b = jnp . zeros(shape = (dout,)) def __call__ ( self , x): return jnp . dot(x, self . w) + self . b model = Linear( 1 , 1 ) . init( 42 ) optimizer = tx . Optimizer(optax . adam( 0.01 )) optimizer = optimizer . init(model . filter(tx . Parameter)) @partial (jax . value_and_grad, has_aux = True ) def loss_fn (params, model, x, y): model = model . update(params) y_pred = model(x) loss = jnp . mean((y_pred - y) ** 2 ) return loss, model @jax . jit def train_step (model, x, y, optimizer): params = model . filter(tx . Parameter) (loss, model), grads = loss_fn(params, model, x, y) # here model == params model = optimizer . update(grads, model) return loss, model, optimizer for step in range ( 1000 ): loss, model, optimizer = train_step(model, x, y, optimizer) if step % 100 == 0 : print ( f\"loss: { loss : .4f } \" ) model = model . eval() X_test = np . linspace(x . min(), x . max(), 100 )[:, None ] y_pred = model(X_test) plt . scatter(x, y, c = \"k\" , label = \"data\" ) plt . plot(X_test, y_pred, c = \"b\" , linewidth =2 , label = \"prediction\" ) plt . legend() plt . show()","title":"Introduction"},{"location":"#treex","text":"A Pytree-based Module system for JAX Intuitive : Modules are simple Python objects that respect Object-Oriented semantics and should make PyTorch users feel at home, with no need for separate dictionary structures or complex apply methods. Pytree-based : Modules are registered as JAX PyTrees, enabling their use with any JAX function. No need for specialized versions of jit , grad , vmap , etc. Expressive : In Treex you use type annotations to define what the different parts of your module represent (submodules, parameters, batch statistics, etc), this leads to a very flexible and powerful state management solution. Flax-based Implementations : Writing high-quality, battle-tested code for common layers is hard. For this reason Modules in treex.nn are wrappers over their Flax counterparts. We keep identical signatures, enabling Flax users to feel at home but still benefiting from the simpler Pytorch-like experience Treex brings.","title":"Treex"},{"location":"#why-treex","text":"Despite all JAX benefits, current Module systems are not intuitive to new users and add additional complexity not present in frameworks like PyTorch or Keras. Treex takes inspiration from S4TF and delivers an intuitive experience using JAX Pytree infrastructure. Current Alternative's Drawbacks and Solutions Currently we have many alternatives like Flax, Haiku, Objax, that have one or more of the following drawbacks: * Module structure and parameter structure are separate, and parameters have to be manipulated around by the end-user, which is not intuitive. In Treex, parameters are stored in the modules themselves and can be accessed directly. * Monadic architecture adds complexity. Flax and Haiku use an `apply` method to call modules that set a context with parameters, rng, and different metadata, which adds additional overhead to the API and creates an asymmetry in how Modules are being used inside and outside a context. In Treex, modules can be called directly. * Among different frameworks, parameter surgery requires special consideration and is challenging to implement. Consider a standard workflow such as transfer learning, transferring parameters and state from a pre-trained module or submodule as part of a new module; in different frameworks, we have to know precisely how to extract their parameters and how to insert them into the new parameter structure/dictionaries such that it is in agreement with the new module structure. In Treex, just as in PyTorch / Keras, we enable to pass the (sub)module to the new module, and parameters are automatically added to the new structure. * Multiple frameworks deviate from JAX semantics and require particular versions of `jit`, `grad`, `vmap`, etc., which makes it harder to integrate with other JAX libraries. Treex's Modules are plain old JAX PyTrees and are compatible with any JAX library that supports them. * Other Pytree-based approaches like Parallax and Equinox do not have a total state management solution to handle complex states as encountered in Flax. Treex has the Filter and Update API, which is very expressive and can effectively handle systems with a complex state.","title":"Why Treex?"},{"location":"#installation","text":"Install using pip: pip install treex","title":"Installation"},{"location":"#status","text":"At the moment Treex is a proof of concept for what a Pytree-based Module system for JAX could look like. Testing is needed to find bugs and potential issues, however, since Treex layers are numerically equivalent to Flax this borrows some maturity and yields more confidence over its results. Feedback is much appreciated. Roadmap : - [x] Finish prototyping core API - [ ] Wrap all Flax Linen Modules - [ ] Document public API - [ ] Create documentation site","title":"Status"},{"location":"#getting-started","text":"This is a small appetizer to give you a feel for how using Treex looks like, be sure to checkout the Guide section below for details on more advanced usage. from typing import Sequence, List import jax import jax.numpy as jnp import numpy as np import treex as tx # you can use tx.MLP but we will create our own as an example class MLP (tx . Module): layers: List[tx . Linear] def __init__ ( self , features: Sequence[ int ]): self . layers = [ tx . Linear(din, dout) for din, dout in zip (features[: -1 ], features[ 1 :]) ] def __call__ ( self , x): for linear in self . layers[: -1 ]: x = jax . nn . relu(linear(x)) return self . layers[ -1 ](x) model = MLP([ 1 , 12 , 8 , 1 ]) . init( 42 ) x = np . random . uniform( -1 , 1 , size = ( 100 , 1 )) y = 1.4 * x ** 2 - 0.3 + np . random . normal(scale =0.1 , size = ( 100 , 1 )) @jax . jit @jax . grad def loss_fn (model, x, y): y_pred = model(x) return jnp . mean((y_pred - y) ** 2 ) # in reality use optax def sdg (param, grad): return param - 0.01 * grad # training loop for step in range ( 10_000 ): grads = loss_fn(model, x, y) model = jax . tree_map(sdg, model, grads) model = model . eval() y_pred = model(x)","title":"Getting Started"},{"location":"#guide","text":"","title":"Guide"},{"location":"#defining-modules","text":"Treex Modules have the following characteristics: * They inherit from tx.Module . * Fields for parameter and submodules MUST be marked using a valid type annotation. class Linear (tx . Module): w: tx . Parameter b: tx . Parameter def __init__ ( self , din, dout): self . w = tx . Initializer( lambda key: jax . random . uniform(key, shape = (din, dout))) self . b = jnp . zeros(shape = (dout,)) def __call__ ( self , x): return jnp . dot(x, self . w) + self . b linear = Linear( 3 , 5 ) . init( 42 ) y = linear(x) Valid type annotations include: * Subtypes of tx.TreePart e.g. tx.Parameter , tx.BatchStat , etc. * Subtypes of tx.Module e.g. tx.Linear , custom Module types, etc. * Generic subtypes from the typing module of the previous e.g. List[tx.Parameter] or Dict[str, tx.Linear] . Type annotations that do not comform to the above rules will be ignored and the field will not be counted as part of the Pytree. class MLP (tx . Module): layers: List[tx . Linear] def __init__ ( self , features: Sequence[ int ]): self . layers = [ tx . Linear(din, dout) for din, dout in zip (features[: -1 ], features[ 1 :]) ] def __call__ ( self , x): for linear in self . layers[: -1 ]: x = jax . nn . relu(linear(x)) return self . layers[ -1 ](x) mlp = MLP([ 3 , 5 , 2 ]) . init( 42 )","title":"Defining Modules"},{"location":"#pytrees","text":"Since Modules are pytrees they can be arguments to JAX functions such as jit , grad , vmap , etc, and the jax.tree_* function family. @jax . jit @jax . grad def loss_fn (model, x, y): y_pred = model(x) return jnp . mean((y_pred - y) ** 2 ) def sdg (param, grad): return param - 0.01 * grad model = MLP( ... ) . init( 42 ) grads = loss_fn(model, x, y) model = jax . tree_map(sdg, model, grads) This makes Treex Modules compatible with tooling from the broader JAX ecosystem, and enables correct unification of Modules as both parameter containers and the definition of the foward computation.","title":"Pytrees"},{"location":"#initialization","text":"Initialization in Treex is done by calling the init method on the Module with a seed. This returns a new Module with all fields initialized. There are two initialization mechanisms in Treex. The first one is setting the fields we wish to initialize to an Initializer object. Initializer s contain functions that take a key and return the initial value of the field: class MyModule (tx . Module): a: tx . Parameter b: tx . Parameter def __init__ ( self ): self . a = tx . Initializer( lambda key: jax . random . uniform(key, shape = ( 1 ,))) self . b = 2 module = MyModule() module # MyModule(a=Initializer, b=2) moduel . initialized # False module = module . init( 42 ) module # MyModule(a=array([0.034...]), b=2) module . initialized # True The second is to override the module_init method, which takes a key and can initialize any required fields. This is useful for modules that require complex initialization logic or whose field's initialization depends on each other. class MyModule (tx . Module): a: tx . Parameter b: tx . Parameter def __init__ ( self ): self . a = None self . b = None def module_init ( self , key): # some complex initialization ... module = MyModule() . init( 42 ) module # MyModule(a=array([0.927...]), b=array([0.749...])) We can also mix and match the two strategies, meaning that some parameters can be initialized via Initializer s while others via module_init . The rule is that Initializer s are always going the be called first.","title":"Initialization"},{"location":"#filter-and-update-api","text":"The filter method allows selecting a subtree by filtering based on a type, all leaves that are not a subclass of such type are set to a special Nothing value. class MyModule (tx . Module): a: tx . Parameter = np . array( 1 ) b: tx . BatchStat = np . array( 2 ) ... module = MyModule( ... ) module . filter(tx . Parameter) # MyModule(a=array([1]), b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=array([2])) Nothing much like None is an empty Pytree so it gets ignored by tree operations: jax . tree_leaves(module . filter(tx . Parameter)) # [array([1])] jax . tree_leaves(module . filter(tx . BatchStat)) # [array([2])] A typical use case is to define params as a Parameter filter and pass it as the first argument to grad so that the gradient is computed only that particular subset and immediately update them back to the model before performing any computation: # we take `params` as a Parameter filter from model # but model itself is left untouched params = model . filter(tx . Parameter) @jax . grad def loss_fn (params, model, x, y): # update traced arrays by `grad` from `params` model = model . update(params) ... grads = loss_fn(params, model, x, y) optimizer = tx . Optimizer(optax . adam( 1e-3 )) optimizer = optimizer . init(params) # only needs params","title":"Filter and Update API"},{"location":"#optimizer","text":"Optax is an amazing library however, its optimizers are not pytrees, this means that state and computation are separate, and you cannot jit them. To solve this Treex provides an tx.Optimizer class that can wrap any Optax optimizer and make it a Pytree. While in optax you would define something like this: def main (): ... optimizer = optax . adam( 1e-3 ) opt_state = optimizer . init(params) ... @partial (jax . jit, static_argnums = ( 4 ,)) def train_step (model, x, y, opt_state, optimizer): # optimizer has to be static ... updates, opt_state = optimizer . update(grads, opt_state, params) params = optax . apply_updates(params, updates) ... return model, loss, opt_state With tx.Optimizer you it can be simplified to: def main (): ... optimizer = tx . Optimizer(optax . adam( 1e-3 )) . init(params) ... jax . jit def train_step (model, x, y, optimizer): ... params = optimizer . update(grads, params) ... return model, loss, optimizer As you see, tx.Optimizer follows the same API as optax.GradientTransformation except that: 1. There is no opt_state , instead optimizer IS the state. 2. update by default applies the gradient updates to the parameters. 3. update updates the internal state of the optimizer in-place. Notice that since tx.Optimizer is a Pytree, it was passed through jit without the need to specify static_argnums .","title":"Optimizer"},{"location":"#state-management","text":"Treex takes a \"direct\" approach to state management, i.e., state is updated in-place by the Module whenever it needs to. For example, this module will calculate the running average of its input: class Average (tx . Module): count: tx . State total: tx . State def __init__ ( self ): self . count = jnp . array( 0 ) self . total = jnp . array( 0.0 ) def __call__ ( self , x): self . count += np . prod(x . shape) self . total += jnp . sum(x) return self . total / self . count Treex Modules that require random state will often keep a rng key internally and update it in-place when needed: class Dropout (tx . Module): rng: tx . Rng def __init__ ( self , rate: float ): self . rng = tx . Initializer( lambda key: key) ... def __call__ ( self , x): key, self . rng = jax . random . split( self . rng) ... Finally tx.Optimizer also performs inplace updates inside the update method, here is a sketch of how it works: class Optimizer (tx . TreeObject): opt_state: tx . OptState optimizer: optax . GradientTransformation def update ( self , grads, params): ... updates, self . opt_state = self . optimizer . update( grads, self . opt_state, params ) ...","title":"State Management"},{"location":"#what-is-the-catch","text":"State management is one of the most challenging things in JAX, but with the help of Treex it seems effortless, what is the catch? As always there is a trade-off to consider: Treex's approach requires to consider how to propagate state changes properly while taking into account the fact that Pytree operations create new objects, that is, since reference do not persist across calls through these functions changes might be lost. A standard solution to this problem is: always output the module to update state . For example, a typical loss function that contains a stateful model would look like this: @partial (jax . value_and_grad, has_aux = True ) def loss_fn (params, model, x, y): model = model . update(params) y_pred = model(x) loss = jnp . mean((y_pred - y) ** 2 ) return loss, model params = model . filter(tx . Parameter) (loss, model), grads = loss_fn(params, model, x, y) ... Here model is returned along with the loss through value_and_grad to update model on the outside thus persisting any changes to the state performed on the inside.","title":"What is the catch?"},{"location":"#training-state","text":"Treex Modules have a training: bool property that specifies whether the module is in training mode or not. This property conditions the behavior of Modules such as Dropout and BatchNorm , which behave differently between training and evaluation. To switch between modes, use the .train() and .eval() methods, they return a new Module whose training state and the state of all of its submodules (recursively) are set to the desired value. # training loop for step in range ( 1000 ): loss, model, opt_state = train_step(model, x, y, opt_state) # prepare for evaluation model = model . eval() # make predictions y_pred = model(X_test)","title":"Training State"},{"location":"#parameter-annotations","text":"The role of each parameter is defined by its annotation. While valid annotations is any type which inherits from tx.TreePart , the default annotations from Treex are currently organized into the following type hierarchy: Graph code graph TD; TreePart-->Parameter; TreePart-->State; State-->Rng; State-->BatchStat; This is useful because you can make specific or more general queries using filter depending on what you want to achive. e.g. rngs = model . filter(tx . Rng) batch_stats = model . filter(tx . BatchStat) all_states = model . filter(tx . State) # union of the previous two You can easily define you own annotations by inheriting from directly tx.TreePart or any of its subclasses. As an example lets create a new Cache state to emulates Flax's cache collection: class Cache (tx . TreePart): pass That is it! Now you can use it in your model: class MyModule (tx . Module): memory: Cache ... Tip : Your static analyzer will probably start complaining if you try to assign an jnp.ndarray to memory in this example because ndarray s are not TreePart s. While this makes sense, we want to trick the static analyzer into thinking Cache represents an ndarray and not a TreePart , the easiest way to do this is to use typing.cast : from typing import cast, Type import jax.numpy as jnp class Cache (tx . TreePart): pass Cache = cast(Type[jnp . ndarray], Cache) cast an identity function, meaning Cache is actually reassigned to itself, however, the static analyzer will now think its an ndarray . This way both the static analyzer and Treex will be happy.","title":"Parameter Annotations"},{"location":"#full-example","text":"from functools import partial import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import optax import treex as tx x = np . random . uniform(size = ( 500 , 1 )) y = 1.4 * x - 0.3 + np . random . normal(scale =0.1 , size = ( 500 , 1 )) # treex already defines tx.Linear but we can define our own class Linear (tx . Module): w: tx . Parameter b: tx . Parameter def __init__ ( self , din, dout): self . w = tx . Initializer( lambda key: jax . random . uniform(key, shape = (din, dout))) self . b = jnp . zeros(shape = (dout,)) def __call__ ( self , x): return jnp . dot(x, self . w) + self . b model = Linear( 1 , 1 ) . init( 42 ) optimizer = tx . Optimizer(optax . adam( 0.01 )) optimizer = optimizer . init(model . filter(tx . Parameter)) @partial (jax . value_and_grad, has_aux = True ) def loss_fn (params, model, x, y): model = model . update(params) y_pred = model(x) loss = jnp . mean((y_pred - y) ** 2 ) return loss, model @jax . jit def train_step (model, x, y, optimizer): params = model . filter(tx . Parameter) (loss, model), grads = loss_fn(params, model, x, y) # here model == params model = optimizer . update(grads, model) return loss, model, optimizer for step in range ( 1000 ): loss, model, optimizer = train_step(model, x, y, optimizer) if step % 100 == 0 : print ( f\"loss: { loss : .4f } \" ) model = model . eval() X_test = np . linspace(x . min(), x . max(), 100 )[:, None ] y_pred = model(X_test) plt . scatter(x, y, c = \"k\" , label = \"data\" ) plt . plot(X_test, y_pred, c = \"b\" , linewidth =2 , label = \"prediction\" ) plt . legend() plt . show()","title":"Full Example"},{"location":"api/BatchNorm/","text":"treex.BatchNorm BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training is used unless use_running_average is explicitly passed via __call__ . __call__ ( self , x, use_running_average = None ) special Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/batch_norm.py def __call__ ( self , x: np . ndarray, use_running_average: tp . Optional[ bool ] = None ) -> jnp . ndarray: \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" assert self . batch_stats is not None , \"BatchNorm module not initialized\" variables = dict ( params = self . params if self . params is not None else {}, batch_stats = self . batch_stats, ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training ) # call apply output, variables = self . module . apply( variables, x, mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training, ) # update batch_stats if \"batch_stats\" in variables: self . batch_stats = variables[ \"batch_stats\" ] . unfreeze() return tp . cast(jnp . ndarray, output) __init__ ( self , features_in, axis =-1 , momentum =0.99 , epsilon =1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7f0632eed3a0>, scale_init=<function ones at 0x7f0632eed4c0>, axis_name=None, axis_index_groups=None) special Parameters: Name Type Description Default features_in int the number of input features. required use_running_average if True, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. required axis int the feature or non-batch axis of the input. -1 momentum float decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7f0632eed3a0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7f0632eed4c0> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/batch_norm.py def __init__ ( self , features_in: int , axis: int = -1 , momentum: float = 0.99 , epsilon: float = 1e-5 , dtype: flax_module . Dtype = jnp . float32, use_bias: bool = True , use_scale: bool = True , bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . initializers . zeros, scale_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . initializers . ones, axis_name: tp . Optional[ str ] = None , axis_index_groups: tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. use_running_average: if True, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . features_in = features_in self . module = flax_module . BatchNorm( use_running_average = None , axis = axis, momentum = momentum, epsilon = epsilon, dtype = dtype, use_bias = use_bias, use_scale = use_scale, bias_init = bias_init, scale_init = scale_init, axis_name = axis_name, axis_index_groups = axis_index_groups, ) self . params = None self . batch_stats = None copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/batch_norm.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/batch_norm.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/batch_norm.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/batch_norm.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/batch_norm.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/batch_norm.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/batch_norm.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"BatchNorm"},{"location":"api/BatchNorm/#treexbatchnorm","text":"BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training is used unless use_running_average is explicitly passed via __call__ .","title":"treex.BatchNorm"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.__call__","text":"Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/batch_norm.py def __call__ ( self , x: np . ndarray, use_running_average: tp . Optional[ bool ] = None ) -> jnp . ndarray: \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" assert self . batch_stats is not None , \"BatchNorm module not initialized\" variables = dict ( params = self . params if self . params is not None else {}, batch_stats = self . batch_stats, ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training ) # call apply output, variables = self . module . apply( variables, x, mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training, ) # update batch_stats if \"batch_stats\" in variables: self . batch_stats = variables[ \"batch_stats\" ] . unfreeze() return tp . cast(jnp . ndarray, output)","title":"__call__()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.__init__","text":"Parameters: Name Type Description Default features_in int the number of input features. required use_running_average if True, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. required axis int the feature or non-batch axis of the input. -1 momentum float decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7f0632eed3a0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7f0632eed4c0> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/batch_norm.py def __init__ ( self , features_in: int , axis: int = -1 , momentum: float = 0.99 , epsilon: float = 1e-5 , dtype: flax_module . Dtype = jnp . float32, use_bias: bool = True , use_scale: bool = True , bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . initializers . zeros, scale_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . initializers . ones, axis_name: tp . Optional[ str ] = None , axis_index_groups: tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. use_running_average: if True, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . features_in = features_in self . module = flax_module . BatchNorm( use_running_average = None , axis = axis, momentum = momentum, epsilon = epsilon, dtype = dtype, use_bias = use_bias, use_scale = use_scale, bias_init = bias_init, scale_init = scale_init, axis_name = axis_name, axis_index_groups = axis_index_groups, ) self . params = None self . batch_stats = None","title":"__init__()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/batch_norm.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/batch_norm.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/batch_norm.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/batch_norm.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/batch_norm.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/batch_norm.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/BatchNorm/#treex.nn.batch_norm.BatchNorm.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/batch_norm.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/BatchStat/","text":"treex.BatchStat","title":"BatchStat"},{"location":"api/BatchStat/#treexbatchstat","text":"","title":"treex.BatchStat"},{"location":"api/Conv/","text":"treex.Conv Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x) special Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" assert self . params is not None , \"Module not initialized\" variables = dict (params = self . params) output = self . module . apply(variables, x) return tp . cast(jnp . ndarray, output) __init__ ( self , features_in, features_out, kernel_size, strides = None , padding = 'SAME' , input_dilation = None , kernel_dilation = None , feature_group_count =1 , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7f0628ee7280>, bias_init=<function zeros at 0x7f0632eed3a0>) special Parameters: Name Type Description Default features_in int the number of input features. required features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/conv.py def __init__ ( self , features_in: int , features_out: int , kernel_size: tp . Union[ int , tp . Iterable[ int ]], strides: tp . Optional[tp . Iterable[ int ]] = None , padding: tp . Union[ str , tp . Iterable[tp . Tuple[ int , int ]]] = \"SAME\" , input_dilation: tp . Optional[tp . Iterable[ int ]] = None , kernel_dilation: tp . Optional[tp . Iterable[ int ]] = None , feature_group_count: int = 1 , use_bias: bool = True , dtype: flax_module . Dtype = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features_in: the number of input features. features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_in = features_in self . module = flax_module . Conv( features = features_out, kernel_size = kernel_size, strides = strides, padding = padding, input_dilation = input_dilation, kernel_dilation = kernel_dilation, feature_group_count = feature_group_count, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) self . params = None copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/conv.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/conv.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/conv.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/conv.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/conv.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/conv.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/conv.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Conv"},{"location":"api/Conv/#treexconv","text":"Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.Conv"},{"location":"api/Conv/#treex.nn.conv.Conv.__call__","text":"Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" assert self . params is not None , \"Module not initialized\" variables = dict (params = self . params) output = self . module . apply(variables, x) return tp . cast(jnp . ndarray, output)","title":"__call__()"},{"location":"api/Conv/#treex.nn.conv.Conv.__init__","text":"Parameters: Name Type Description Default features_in int the number of input features. required features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/conv.py def __init__ ( self , features_in: int , features_out: int , kernel_size: tp . Union[ int , tp . Iterable[ int ]], strides: tp . Optional[tp . Iterable[ int ]] = None , padding: tp . Union[ str , tp . Iterable[tp . Tuple[ int , int ]]] = \"SAME\" , input_dilation: tp . Optional[tp . Iterable[ int ]] = None , kernel_dilation: tp . Optional[tp . Iterable[ int ]] = None , feature_group_count: int = 1 , use_bias: bool = True , dtype: flax_module . Dtype = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features_in: the number of input features. features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_in = features_in self . module = flax_module . Conv( features = features_out, kernel_size = kernel_size, strides = strides, padding = padding, input_dilation = input_dilation, kernel_dilation = kernel_dilation, feature_group_count = feature_group_count, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) self . params = None","title":"__init__()"},{"location":"api/Conv/#treex.nn.conv.Conv.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/conv.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Conv/#treex.nn.conv.Conv.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/conv.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Conv/#treex.nn.conv.Conv.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/conv.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Conv/#treex.nn.conv.Conv.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/conv.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Conv/#treex.nn.conv.Conv.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/conv.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Conv/#treex.nn.conv.Conv.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/conv.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Conv/#treex.nn.conv.Conv.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/conv.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/Dropout/","text":"treex.Dropout Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ . __call__ ( self , x, deterministic = None , rng = None ) special Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x: np . ndarray, deterministic: tp . Optional[ bool ] = None , rng = None ) -> jnp . ndarray: \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = not deterministic if deterministic is not None else self . training deterministic = not training if rng is None : assert isinstance ( self . rng, jnp . ndarray) rng, self . rng = jax . random . split( self . rng) # call apply output = self . module . apply( variables, x, deterministic = deterministic, rng = rng, ) return tp . cast(jnp . ndarray, output) __init__ ( self , rate, broadcast_dims = ()) special Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate: float , broadcast_dims: tp . Iterable[ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . module = flax_module . Dropout( rate = rate, broadcast_dims = broadcast_dims, deterministic = None , ) self . rng = types . Initializer( lambda key: key) copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/dropout.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/dropout.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/dropout.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/dropout.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/dropout.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/dropout.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/dropout.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Dropout"},{"location":"api/Dropout/#treexdropout","text":"Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ .","title":"treex.Dropout"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.__call__","text":"Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x: np . ndarray, deterministic: tp . Optional[ bool ] = None , rng = None ) -> jnp . ndarray: \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = not deterministic if deterministic is not None else self . training deterministic = not training if rng is None : assert isinstance ( self . rng, jnp . ndarray) rng, self . rng = jax . random . split( self . rng) # call apply output = self . module . apply( variables, x, deterministic = deterministic, rng = rng, ) return tp . cast(jnp . ndarray, output)","title":"__call__()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.__init__","text":"Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate: float , broadcast_dims: tp . Iterable[ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . module = flax_module . Dropout( rate = rate, broadcast_dims = broadcast_dims, deterministic = None , ) self . rng = types . Initializer( lambda key: key)","title":"__init__()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/dropout.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/dropout.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/dropout.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/dropout.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/dropout.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/dropout.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/dropout.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/Initializer/","text":"treex.Initializer Initialize a field from a function that expects a single argument with a PRNGKey. Initializers are called by Module.init and replace the value of the field they are assigned to. __init__ ( self , f) special Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], Any] A function that takes a PRNGKey and returns the initial value of the field. required Source code in treex/types.py def __init__ ( self , f: tp . Callable[[jnp . ndarray], tp . Any]): \"\"\" Arguments: f: A function that takes a PRNGKey and returns the initial value of the field. \"\"\" self . f = f","title":"Initializer"},{"location":"api/Initializer/#treexinitializer","text":"Initialize a field from a function that expects a single argument with a PRNGKey. Initializers are called by Module.init and replace the value of the field they are assigned to.","title":"treex.Initializer"},{"location":"api/Initializer/#treex.types.Initializer.__init__","text":"Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], Any] A function that takes a PRNGKey and returns the initial value of the field. required Source code in treex/types.py def __init__ ( self , f: tp . Callable[[jnp . ndarray], tp . Any]): \"\"\" Arguments: f: A function that takes a PRNGKey and returns the initial value of the field. \"\"\" self . f = f","title":"__init__()"},{"location":"api/Lambda/","text":"treex.Lambda A Module that applies a pure function to its input. __call__ ( self , x) special Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequence.py def __call__ ( self , x: np . ndarray) -> np . ndarray: \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f(x) __init__ ( self , f) special Parameters: Name Type Description Default f Callable[[numpy.ndarray], numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequence.py def __init__ ( self , f: tp . Callable[[np . ndarray], np . ndarray]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f self . f = f copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/sequence.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/sequence.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/sequence.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/sequence.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/sequence.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/sequence.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/sequence.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Lambda"},{"location":"api/Lambda/#treexlambda","text":"A Module that applies a pure function to its input.","title":"treex.Lambda"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.__call__","text":"Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequence.py def __call__ ( self , x: np . ndarray) -> np . ndarray: \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f(x)","title":"__call__()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.__init__","text":"Parameters: Name Type Description Default f Callable[[numpy.ndarray], numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequence.py def __init__ ( self , f: tp . Callable[[np . ndarray], np . ndarray]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f self . f = f","title":"__init__()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/sequence.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/sequence.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/sequence.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/sequence.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/sequence.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/sequence.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Lambda/#treex.nn.sequence.Lambda.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/sequence.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/Linear/","text":"treex.Linear A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x) special Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" assert self . params is not None , \"Module not initialized\" variables = dict (params = self . params) output = self . module . apply(variables, x) return tp . cast(jnp . ndarray, output) __init__ ( self , features_in, features_out, use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7f0628ee7280>, bias_init=<function zeros at 0x7f0632eed3a0>) special Parameters: Name Type Description Default features_in int the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/linear.py def __init__ ( self , features_in: int , features_out: int , use_bias: bool = True , dtype: tp . Any = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" self . features_in = features_in self . module = flax_module . Dense( features = features_out, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) self . params = None copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/linear.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/linear.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/linear.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/linear.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/linear.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/linear.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/linear.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Linear"},{"location":"api/Linear/#treexlinear","text":"A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.Linear"},{"location":"api/Linear/#treex.nn.linear.Linear.__call__","text":"Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" assert self . params is not None , \"Module not initialized\" variables = dict (params = self . params) output = self . module . apply(variables, x) return tp . cast(jnp . ndarray, output)","title":"__call__()"},{"location":"api/Linear/#treex.nn.linear.Linear.__init__","text":"Parameters: Name Type Description Default features_in int the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/linear.py def __init__ ( self , features_in: int , features_out: int , use_bias: bool = True , dtype: tp . Any = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" self . features_in = features_in self . module = flax_module . Dense( features = features_out, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) self . params = None","title":"__init__()"},{"location":"api/Linear/#treex.nn.linear.Linear.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/linear.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Linear/#treex.nn.linear.Linear.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/linear.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Linear/#treex.nn.linear.Linear.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/linear.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Linear/#treex.nn.linear.Linear.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/linear.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Linear/#treex.nn.linear.Linear.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/linear.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Linear/#treex.nn.linear.Linear.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/linear.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Linear/#treex.nn.linear.Linear.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/linear.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/MLP/","text":"treex.MLP A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear. __call__ ( self , x) special Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" for layer in self . layers[: -1 ]: x = self . activation(layer(x)) return self . layers[ -1 ](x) __init__ ( self , features, activation =< jax . _src . custom_derivatives . custom_jvp object at 0x7f0632e9de80> , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7f0628ee7280>, bias_init=<function zeros at 0x7f0632eed3a0>) special Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7f0632e9de80> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/mlp.py def __init__ ( self , features: tp . Sequence[ int ], activation: tp . Callable[[jnp . ndarray], jnp . ndarray] = jax . nn . relu, use_bias: bool = True , dtype: tp . Any = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len (features) < 2 : raise ValueError ( \"features must have at least 2 elements\" ) self . features = features self . activation = activation self . layers = [ Linear( features_in = features_in, features_out = features_out, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) for features_in, features_out in zip (features[: -1 ], features[ 1 :]) ] copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/mlp.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/mlp.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/mlp.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/mlp.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/mlp.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/mlp.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/mlp.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"MLP"},{"location":"api/MLP/#treexmlp","text":"A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear.","title":"treex.MLP"},{"location":"api/MLP/#treex.nn.mlp.MLP.__call__","text":"Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py def __call__ ( self , x: np . ndarray) -> jnp . ndarray: \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" for layer in self . layers[: -1 ]: x = self . activation(layer(x)) return self . layers[ -1 ](x)","title":"__call__()"},{"location":"api/MLP/#treex.nn.mlp.MLP.__init__","text":"Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7f0632e9de80> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7f0628ee7280> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7f0632eed3a0> Source code in treex/nn/mlp.py def __init__ ( self , features: tp . Sequence[ int ], activation: tp . Callable[[jnp . ndarray], jnp . ndarray] = jax . nn . relu, use_bias: bool = True , dtype: tp . Any = jnp . float32, precision: tp . Any = None , kernel_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . default_kernel_init, bias_init: tp . Callable[ [flax_module . PRNGKey, flax_module . Shape, flax_module . Dtype], flax_module . Array, ] = flax_module . zeros, ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len (features) < 2 : raise ValueError ( \"features must have at least 2 elements\" ) self . features = features self . activation = activation self . layers = [ Linear( features_in = features_in, features_out = features_out, use_bias = use_bias, dtype = dtype, precision = precision, kernel_init = kernel_init, bias_init = bias_init, ) for features_in, features_out in zip (features[: -1 ], features[ 1 :]) ]","title":"__init__()"},{"location":"api/MLP/#treex.nn.mlp.MLP.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/mlp.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/MLP/#treex.nn.mlp.MLP.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/mlp.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/MLP/#treex.nn.mlp.MLP.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/mlp.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/MLP/#treex.nn.mlp.MLP.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/mlp.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/MLP/#treex.nn.mlp.MLP.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/mlp.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/MLP/#treex.nn.mlp.MLP.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/mlp.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/MLP/#treex.nn.mlp.MLP.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/mlp.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/Module/","text":"treex.Module copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/module.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/module.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/module.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/module.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/module.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/module.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/module.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Module"},{"location":"api/Module/#treexmodule","text":"","title":"treex.Module"},{"location":"api/Module/#treex.module.Module.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/module.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Module/#treex.module.Module.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/module.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Module/#treex.module.Module.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/module.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Module/#treex.module.Module.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/module.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Module/#treex.module.Module.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/module.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Module/#treex.module.Module.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/module.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Module/#treex.module.Module.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/module.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/Nothing/","text":"treex.Nothing","title":"Nothing"},{"location":"api/Nothing/#treexnothing","text":"","title":"treex.Nothing"},{"location":"api/OptState/","text":"treex.OptState","title":"OptState"},{"location":"api/OptState/#treexoptstate","text":"","title":"treex.OptState"},{"location":"api/Optimizer/","text":"treex.Optimizer Wraps an optax optimizer and turn it into a Pytree while maintaining a similar API. The main difference with optax is that tx.Optimizer contains its own state, thus, there is no opt_state . Examples: def main (): ... optimizer = tx . Optimizer(optax . adam( 1e-3 )) optimizer = optimizer . init(params) ... jax . jit def train_step (model, x, y, optimizer): ... params = optimizer . update(grads, params) ... return model, loss, optimizer Notice that since the optimizer is a Pytree it can naturally pass through jit . Differences with Optax init return a new optimizer instance, there is no opt_state . update doesn't get opt_state as an argument, instead it performs updates to its internal state inplace. update applies the updates to the params and returns them by default, use apply_updates=False to to get the param updates instead. __init__ ( self , optimizer) special Parameters: Name Type Description Default optimizer GradientTransformation An optax optimizer. required Source code in treex/optimizer.py def __init__ ( self , optimizer: optax . GradientTransformation): \"\"\" Arguments: optimizer: An optax optimizer. \"\"\" self . opt_state = None self . optimizer = optimizer copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/optimizer.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) init( self , params) Initialize the optimizer from an initial set of parameters. Parameters: Name Type Description Default params Any An initial set of parameters. required Returns: Type Description ~T A new optimizer instance. Source code in treex/optimizer.py def init ( self : T, params: tp . Any) -> T: \"\"\" Initialize the optimizer from an initial set of parameters. Arguments: params: An initial set of parameters. Returns: A new optimizer instance. \"\"\" module = self . copy() module . opt_state = module . optimizer . init(params) module . _initialized = True return module update( self , grads, params = None , apply_updates = True ) Applies the parameters updates and updates the optimizers internal state inplace. Parameters: Name Type Description Default grads ~A the gradients to perform the update. required params Optional[~A] the parameters to update. If None then apply_updates has to be False . None apply_updates bool whether to apply the updates to the parameters. True Returns: Type Description ~A The updated parameters. If apply_updates is False then the updates are returned instead. Source code in treex/optimizer.py def update ( self , grads: A, params: tp . Optional[A] = None , apply_updates: bool = True ) -> A: \"\"\" Applies the parameters updates and updates the optimizers internal state inplace. Arguments: grads: the gradients to perform the update. params: the parameters to update. If `None` then `apply_updates` has to be `False`. apply_updates: whether to apply the updates to the parameters. Returns: The updated parameters. If `apply_updates` is `False` then the updates are returned instead. \"\"\" assert self . opt_state is not None if apply_updates and params is None : raise ValueError ( \"params must be provided to apply update\" ) param_updates: A param_updates, self . opt_state = self . optimizer . update( grads, self . opt_state, params ) if apply_updates: return optax . apply_updates(params, param_updates) return param_updates","title":"Optimizer"},{"location":"api/Optimizer/#treexoptimizer","text":"Wraps an optax optimizer and turn it into a Pytree while maintaining a similar API. The main difference with optax is that tx.Optimizer contains its own state, thus, there is no opt_state . Examples: def main (): ... optimizer = tx . Optimizer(optax . adam( 1e-3 )) optimizer = optimizer . init(params) ... jax . jit def train_step (model, x, y, optimizer): ... params = optimizer . update(grads, params) ... return model, loss, optimizer Notice that since the optimizer is a Pytree it can naturally pass through jit .","title":"treex.Optimizer"},{"location":"api/Optimizer/#treex.optimizer.Optimizer--differences-with-optax","text":"init return a new optimizer instance, there is no opt_state . update doesn't get opt_state as an argument, instead it performs updates to its internal state inplace. update applies the updates to the params and returns them by default, use apply_updates=False to to get the param updates instead.","title":"Differences with Optax"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.__init__","text":"Parameters: Name Type Description Default optimizer GradientTransformation An optax optimizer. required Source code in treex/optimizer.py def __init__ ( self , optimizer: optax . GradientTransformation): \"\"\" Arguments: optimizer: An optax optimizer. \"\"\" self . opt_state = None self . optimizer = optimizer","title":"__init__()"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/optimizer.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.init","text":"Initialize the optimizer from an initial set of parameters. Parameters: Name Type Description Default params Any An initial set of parameters. required Returns: Type Description ~T A new optimizer instance. Source code in treex/optimizer.py def init ( self : T, params: tp . Any) -> T: \"\"\" Initialize the optimizer from an initial set of parameters. Arguments: params: An initial set of parameters. Returns: A new optimizer instance. \"\"\" module = self . copy() module . opt_state = module . optimizer . init(params) module . _initialized = True return module","title":"init()"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.update","text":"Applies the parameters updates and updates the optimizers internal state inplace. Parameters: Name Type Description Default grads ~A the gradients to perform the update. required params Optional[~A] the parameters to update. If None then apply_updates has to be False . None apply_updates bool whether to apply the updates to the parameters. True Returns: Type Description ~A The updated parameters. If apply_updates is False then the updates are returned instead. Source code in treex/optimizer.py def update ( self , grads: A, params: tp . Optional[A] = None , apply_updates: bool = True ) -> A: \"\"\" Applies the parameters updates and updates the optimizers internal state inplace. Arguments: grads: the gradients to perform the update. params: the parameters to update. If `None` then `apply_updates` has to be `False`. apply_updates: whether to apply the updates to the parameters. Returns: The updated parameters. If `apply_updates` is `False` then the updates are returned instead. \"\"\" assert self . opt_state is not None if apply_updates and params is None : raise ValueError ( \"params must be provided to apply update\" ) param_updates: A param_updates, self . opt_state = self . optimizer . update( grads, self . opt_state, params ) if apply_updates: return optax . apply_updates(params, param_updates) return param_updates","title":"update()"},{"location":"api/Parameter/","text":"treex.Parameter","title":"Parameter"},{"location":"api/Parameter/#treexparameter","text":"","title":"treex.Parameter"},{"location":"api/Rng/","text":"treex.Rng","title":"Rng"},{"location":"api/Rng/#treexrng","text":"","title":"treex.Rng"},{"location":"api/Sequence/","text":"treex.Sequence A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequence( tx . Linear( 2 , 32 ), jax . nn . relu, tx . Linear( 32 , 8 ), jax . nn . relu, tx . Linear( 8 , 4 ), ) . init( 42 ) x = np . random . uniform(size = ( 10 , 2 )) y = mlp(x) assert y . shape == ( 10 , 4 ) __init__ ( self , * layers) special Parameters: Name Type Description Default *layers Union[List[treex.module.Module], Callable[[numpy.ndarray], numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequence.py def __init__ ( self , * layers: tp . Union[CallableModule, tp . Callable[[np . ndarray], np . ndarray]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance (layer, Module) else Lambda(layer) for layer in layers ] copy( self ) inherited Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/sequence.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self ) eval ( self ) inherited Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/sequence.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False ) filter ( self , * filters) inherited Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/sequence.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module init( self , key) inherited Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/sequence.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module tabulate( self , depth =-1 , signature = False , param_types = True ) inherited Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/sequence.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table) train( self , mode = True ) inherited Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/sequence.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module update( self , other, * rest, * , inplace = False ) inherited Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/sequence.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"Sequence"},{"location":"api/Sequence/#treexsequence","text":"A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequence( tx . Linear( 2 , 32 ), jax . nn . relu, tx . Linear( 32 , 8 ), jax . nn . relu, tx . Linear( 8 , 4 ), ) . init( 42 ) x = np . random . uniform(size = ( 10 , 2 )) y = mlp(x) assert y . shape == ( 10 , 4 )","title":"treex.Sequence"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.__init__","text":"Parameters: Name Type Description Default *layers Union[List[treex.module.Module], Callable[[numpy.ndarray], numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequence.py def __init__ ( self , * layers: tp . Union[CallableModule, tp . Callable[[np . ndarray], np . ndarray]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance (layer, Module) else Lambda(layer) for layer in layers ]","title":"__init__()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/nn/sequence.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/nn/sequence.py def eval ( self : T) -> T: \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train( False )","title":"eval()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.filter","text":"Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by issubclass ) as set to Nothing . Examples: class MyModule (tx . TreeObject): a: tx . Parameter = 1 b: tx . BatchStat = 2 module = MyModule() module . filter(tx . Parameter) # MyModule(a=1, b=Nothing) module . filter(tx . BatchStat) # MyModule(a=Nothing, b=2) Parameters: Name Type Description Default filters Type Types to filter by, membership is determined by issubclass . () Returns: Type Description ~T The new module with the filtered fields. Source code in treex/nn/sequence.py def filter ( self : T, * filters: tp . Type) -> T: \"\"\" Creates a new module with the same structure, but leaves whose type annotations are not subtypes of the given filters (as determined by `issubclass`) as set to `Nothing`. Example: ```python class MyModule(tx.TreeObject): a: tx.Parameter = 1 b: tx.BatchStat = 2 module = MyModule() module.filter(tx.Parameter) # MyModule(a=1, b=Nothing) module.filter(tx.BatchStat) # MyModule(a=Nothing, b=2) ``` Arguments: filters: Types to filter by, membership is determined by `issubclass`. Returns: The new module with the filtered fields. \"\"\" flat: tp . List[types . _ValueAnnotation] old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, treedef = jax . tree_flatten( self ) flat_out = [ value_annotation . value if issubclass (value_annotation . annotation, filters) else types . Nothing() for value_annotation in flat ] module = jax . tree_unflatten(treedef, flat_out) finally : _LOCAL . is_slicing = old_slicing return module","title":"filter()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.init","text":"Creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. TreeObject.module_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~T The new module with the fields initialized. Source code in treex/nn/sequence.py def init ( self : T, key: tp . Union[ int , jnp . ndarray]) -> T: \"\"\" Creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `TreeObject.module_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" if isinstance (key, int ): key = jax . random . PRNGKey(key) old_initializing = _LOCAL . is_initializing old_key = _LOCAL . key _LOCAL . is_initializing = True _LOCAL . key = key try : module = jax . tree_map( lambda initializer: ( initializer(_LOCAL . next_key()) if isinstance (initializer, types . Initializer) else initializer ), self , ) finally : _LOCAL . is_initializing = old_initializing _LOCAL . key = old_key return module","title":"init()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the TreeObject. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/nn/sequence.py def tabulate ( self , depth: int = -1 , signature: bool = False , param_types: bool = True ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the TreeObject. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" old_slicing = _LOCAL . is_slicing _LOCAL . is_slicing = True try : flat, _ = jax . tree_flatten( self ) tree_part_types: tp . Tuple[tp . Type[types . TreePart], ... ] = tuple ( {value_annotation . annotation for value_annotation in flat} ) finally : _LOCAL . is_slicing = old_slicing path = () rows = list ( _get_tabulate_rows( path, self , depth, tree_part_types, signature, param_types ) ) rows[ 0 ][ 0 ] = \"*\" rows . append( [ \"\" , \"\" , \"Total:\" ] + [ _format_obj_size( self . filter(tree_type), add_padding = True ) for tree_type in tree_part_types ] ) _add_padding(rows) table = Table( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column( \"path\" ) table . add_column( \"module\" ) table . add_column( \"params\" ) for tree_part_type in tree_part_types: type_name = tree_part_type . __name__ if type_name . startswith( \"_\" ): type_name = type_name[ 1 :] table . add_column(type_name) for row in rows[: -1 ]: table . add_row( * row) table . columns[ 2 ] . footer = Text . from_markup(rows[ -1 ][ 2 ], justify = \"right\" ) for i in range ( len (tree_part_types)): table . columns[ 3 + i] . footer = rows[ -1 ][ 3 + i] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + _format_obj_size( self , add_padding = False ) return _get_rich_repr(table)","title":"tabulate()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.train","text":"Creates a new module with the same structure, but with TreeObject.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True Returns: Type Description ~T The new module in with the training mode is set to the given value. Source code in treex/nn/sequence.py def train ( self : T, mode: bool = True ) -> T: \"\"\" Creates a new module with the same structure, but with `TreeObject.training` set to the given value. Arguments: mode: The new training mode. Returns: The new module in with the training mode is set to the given value. \"\"\" old_training = _LOCAL . training _LOCAL . training = mode try : module = self . copy() # trigger flatten / unflatten finally : _LOCAL . training = old_training return module","title":"train()"},{"location":"api/Sequence/#treex.nn.sequence.Sequence.update","text":"Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: Updates are performed in the order of the input modules from left to right. If a leaf value from an incoming module is Nothing , it wont update the corresponding value on the currently aggregated module. The static state of the output module ( initialized , training , and user defined static fields) is the same as the current module ( self ). Examples: a = MyModule(x = Nothing, y =2 , z =3 ) b = MyModule(x =1 , y = Nothing, z =4 ) a . update(b) # MyModule(x=1, y=2, z=4) Notice the following: The value of x and z were updated since they were present in b . The value of y was not updated since b.y was Nothing . When using update with multiple modules the following equivalence holds: m1.update(m2, m3) = m1.update(m2).update(m3) If you want to update the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map( lambda x: 2 * x, self ) Instead do this: def double_params ( self ): doubled = jax . tree_map( lambda x: 2 * x, self ) self . update(doubled, inplace = True ) Parameters: Name Type Description Default other ~T The first to get the values to update from. required rest ~T Additional modules to perform the update in order from left to right. () inplace bool If True , the current module is modified with the updated values. False Returns: Type Description Optional[~T] A new module with the updated values or None if inplace is True . Source code in treex/nn/sequence.py def update ( self : T, other: T, * rest: T, inplace: bool = False ) -> tp . Optional[T]: \"\"\" Creates a new module with the same structure, but its values updated based on the values from the incoming modules. Updates are performed using the following rules: * Updates are performed in the order of the input modules from left to right. * If a leaf value from an incoming module is `Nothing`, it wont update the corresponding value on the currently aggregated module. * The static state of the output module (`initialized`, `training`, and user defined static fields) is the same as the current module (`self`). Example: ```python a = MyModule(x=Nothing, y=2, z=3) b = MyModule(x=1, y=Nothing, z=4) a.update(b) # MyModule(x=1, y=2, z=4) ``` Notice the following: * The value of `x` and `z` were updated since they were present in `b`. * The value of `y` was not updated since `b.y` was `Nothing`. When using `update` with multiple modules the following equivalence holds: ``` m1.update(m2, m3) = m1.update(m2).update(m3) ``` If you want to update the current module instead of creating a new one use `inplace=True`. This is useful when applying transformation inside a method where reassigning `self` is not possible: ```python def double_params(self): # this is not doing what you expect self = jax.tree_map(lambda x: 2 * x, self) ``` Instead do this: ```python def double_params(self): doubled = jax.tree_map(lambda x: 2 * x, self) self.update(doubled, inplace=True) ``` Arguments: other: The first to get the values to update from. rest: Additional modules to perform the update in order from left to right. inplace: If `True`, the current module is modified with the updated values. Returns: A new module with the updated values or `None` if `inplace` is `True`. \"\"\" modules = ( self , other) + rest def merge_fn (xs): acc, * xs = xs for x in xs: if not isinstance (x, types . Nothing): acc = x return acc flats, treedefs = zip ( * [ jax . tree_flatten(m, is_leaf = lambda x: isinstance (x, types . Nothing)) for m in modules ] ) # flat_out = jax.tree_util.tree_map(merge_fn, *flats) flat_out = [merge_fn(values) for values in zip ( * flats)] module = jax . tree_unflatten(treedefs[ 0 ], flat_out) if inplace: self . __dict__ . update(module . __dict__ ) return None else : return module","title":"update()"},{"location":"api/State/","text":"treex.State","title":"State"},{"location":"api/State/#treexstate","text":"","title":"treex.State"},{"location":"api/TreeObject/","text":"treex.TreeObject __init_subclass__() classmethod special This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/module.py def __init_subclass__ ( cls ): jax . tree_util . register_pytree_node_class( cls ) copy( self ) Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/module.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"TreeObject"},{"location":"api/TreeObject/#treextreeobject","text":"","title":"treex.TreeObject"},{"location":"api/TreeObject/#treex.module.TreeObject.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/module.py def __init_subclass__ ( cls ): jax . tree_util . register_pytree_node_class( cls )","title":"__init_subclass__()"},{"location":"api/TreeObject/#treex.module.TreeObject.copy","text":"Returns a deep copy of the module, implemented as: jax . tree_map( lambda x: x, self ) Source code in treex/module.py def copy ( self : T) -> T: \"\"\" Returns a deep copy of the module, implemented as: ```python jax.tree_map(lambda x: x, self) ``` \"\"\" return jax . tree_map( lambda x: x, self )","title":"copy()"},{"location":"api/TreePart/","text":"treex.TreePart","title":"TreePart"},{"location":"api/TreePart/#treextreepart","text":"","title":"treex.TreePart"},{"location":"api/sequence/","text":"treex.sequence Creates a function that applies a sequence of callables to an input. Examples: class Block (tx . Module): linear: tx . Linear batch_norm: tx . BatchNorm dropout: tx . Dropout ... def __call__ ( self , x: jnp . ndarray) -> jnp . ndarray: return tx . sequence( self . linear, self . batch_norm, self . dropout, jax . nn . relu, )(x) Parameters: Name Type Description Default *layers Callable[[numpy.ndarray], numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequence.py def sequence ( * layers: tp . Callable[[np . ndarray], np . ndarray] ) -> tp . Callable[[np . ndarray], np . ndarray]: \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence (x: np . ndarray) -> np . ndarray: for layer in layers: x = layer(x) return x return _sequence","title":"sequence"},{"location":"api/sequence/#treexsequence","text":"Creates a function that applies a sequence of callables to an input. Examples: class Block (tx . Module): linear: tx . Linear batch_norm: tx . BatchNorm dropout: tx . Dropout ... def __call__ ( self , x: jnp . ndarray) -> jnp . ndarray: return tx . sequence( self . linear, self . batch_norm, self . dropout, jax . nn . relu, )(x) Parameters: Name Type Description Default *layers Callable[[numpy.ndarray], numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequence.py def sequence ( * layers: tp . Callable[[np . ndarray], np . ndarray] ) -> tp . Callable[[np . ndarray], np . ndarray]: \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence (x: np . ndarray) -> np . ndarray: for layer in layers: x = layer(x) return x return _sequence","title":"treex.sequence"}]}