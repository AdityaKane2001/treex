{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Treex A Pytree Module system for Deep Learning in JAX Main Features \ud83d\udca1 Intuitive : Modules contain their own parameters and respect Object Oriented semantics like in PyTorch and Keras. \ud83c\udf33 Pytree-based : Modules are Pytrees whose leaves are its parameters, meaning they are fully compatible with jit , grad , vmap , etc. Treex is implemented on top of Treeo and reexports all of its API for convenience. Getting Started | User Guide | Examples | Documentation What is included? A base Module class. A nn module for with common layers implemented as wrappers over Flax layers. A losses module with common loss functions. A metrics module with common metrics. An Optimizer class that can wrap any optax optimizer. Why Treex? Show Despite all JAX benefits, current Module systems are not intuitive to new users and add additional complexity not present in frameworks like PyTorch or Keras. Treex takes inspiration from S4TF and delivers an intuitive experience using JAX Pytree infrastructure. Current Alternative's Drawbacks and Solutions Currently we have many alternatives like Flax, Haiku, Objax, that have one or more of the following drawbacks: * Module structure and parameter structure are separate, and parameters have to be manipulated around by the end-user, which is not intuitive. In Treex, parameters are stored in the modules themselves and can be accessed directly. * Monadic architecture adds complexity. Flax and Haiku use an `apply` method to call modules that set a context with parameters, rng, and different metadata, which adds additional overhead to the API and creates an asymmetry in how Modules are being used inside and outside a context. In Treex, modules can be called directly. * Among different frameworks, parameter surgery requires special consideration and is challenging to implement. Consider a standard workflow such as transfer learning, transferring parameters and state from a pre-trained module or submodule as part of a new module; in different frameworks, we have to know precisely how to extract their parameters and how to insert them into the new parameter structure/dictionaries such that it is in agreement with the new module structure. In Treex, just as in PyTorch / Keras, we enable to pass the (sub)module to the new module, and parameters are automatically added to the new structure. * Multiple frameworks deviate from JAX semantics and require particular versions of `jit`, `grad`, `vmap`, etc., which makes it harder to integrate with other JAX libraries. Treex's Modules are plain old JAX PyTrees and are compatible with any JAX library that supports them. * Other Pytree-based approaches like Parallax and Equinox do not have a total state management solution to handle complex states as encountered in Flax. Treex has the Filter and Update API, which is very expressive and can effectively handle systems with a complex state. Installation Install using pip: pip install treex Getting Started This is a small appetizer to give you a feel for how using Treex looks like, be sure to checkout the User Guide for a more in-depth explanation. import treex as tx import numpy as np import jax , optax # create some data x = np . random . uniform ( size = ( 50 , 1 )) y = 1.3 * x ** 2 - 0.3 + np . random . normal ( size = x . shape ) # initialize a Module, its simple model = tx . MLP ([ 64 , 1 ]) . init ( key = 42 , inputs = x ) # define an optimizer, init with model params optimizer = tx . Optimizer ( optax . adam ( 4e-3 )) . init ( model ) # define loss function, notice # Modules are jit-abel and differentiable \ud83e\udd2f @jax . jit @jax . grad def loss_fn ( model : tx . MLP , x , y ): # forward is a simple call preds = model ( x ) # MSE return (( preds - y ) ** 2 ) . mean () # basic training loop for step in range ( 500 ): # grads have the same type as model grads : tx . MLP = loss_fn ( model , x , y ) # apply the gradient updates model = optimizer . update ( grads , model ) # Pytorch-like eval mode model = model . eval () preds = model ( x ) Custom Modules Show Modules are Treeo `Tree`s, which are Pytrees. When creating core layers you often mark fields that will contain state that JAX should be aware as `nodes` by assigning class variables to the output of functions like `tx.Parameter.node()`: import treex as tx class Linear ( tx . Module ): # use Treeo's API to define Parameter nodes w : jnp . ndarray = tx . Parameter . node () b : jnp . ndarray = tx . Parameter . node () def __init__ ( self , features_out : int ): self . features_out = features_out def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : # init will call forward, we can know if we are inside it if self . initializing (): # `next_key` only available during `init` key = tx . next_key () # leverage shape inference self . w = jax . random . uniform ( key , shape = [ x . shape [ - 1 ], self . features_out ] ) self . b = jnp . zeros ( shape = [ self . features_out ]) # linear forward return jnp . dot ( x , self . w ) + self . b model = Linear ( 10 ) . init ( key = 42 , inputs = x ) Node field types (e.g. `tx.Parameter`) are called Kinds and Treex exports a whole family of Kinds which serve for differente purposes such as holding non-differentiable state (`tx.BatchStats`), metric's state (`tx.MetricState`), logging, etc. Checkout the [kinds](https://cgarciae.github.io/treex/user-guide/kinds) section for more information. Composite Modules Show Composite Modules usually hold and call other Modules within them, while they would be instantiate inside `__init__` and used later in `__call__` like in Pytorch / Keras, in Treex you usually leverage the `@tx.compact` decorator over the `__call__` method to define the submodules inline. class MLP ( tx . Module ): def __init__ ( self , features : Sequence [ int ]): self . features = features # compact lets you define submodules on the fly @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for units in self . features [: - 1 ]: x = Linear ( units )( x ) x = jax . nn . relu ( x ) return Linear ( self . features [ - 1 ])( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) Under the hood all calls to submodule constructors (e.g. `Linear(...)`) inside `compact` are assigned to fields in the parent Module (`MLP`) so they are part of the same Pytree, their field names are available under the `._subtrees` attribute. `compact` must always define submodules in the same order. Status Treex is in an early stage, things might break between versions but we will respect semanting versioning. Since Treex layers are numerically equivalent to Flax, it borrows some maturity and yields more confidence over its results. Feedback is much appreciated. Roadmap : Wrap all Flax Linen Modules Implement more layers, losses, and metrics. Create applications and pretrained Modules. Contributions are welcomed! Sponsors \ud83d\udc9a Quansight - paid development time Examples Checkout the /examples directory for more detailed examples. Here are a few additional toy examples: Linear Regression This is a simple but realistic example of how Treex is used. from functools import partial from typing import Union import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import optax import treex as tx x = np . random . uniform ( size = ( 500 , 1 )) y = 1.4 * x - 0.3 + np . random . normal ( scale = 0.1 , size = ( 500 , 1 )) # differentiate only w.r.t. parameters def loss_fn ( params , model , x , y ): # merge params into model model = model . merge ( params ) preds = model ( x ) loss = jnp . mean (( preds - y ) ** 2 ) # the model may contain state updates # so it should be returned return loss , model grad_fn = jax . value_and_grad ( loss_fn , has_aux = True ) # both model and optimizer are jit-able @jax . jit def train_step ( model , x , y , optimizer ): # select only the parameters params = model . parameters () ( loss , model ), grads = grad_fn ( params , model , x , y ) # update params and model params = optimizer . update ( grads , params ) model = model . merge ( params ) # return new model and optimizer return loss , model , optimizer model = tx . Linear ( 1 ) . init ( 42 , x ) optimizer = tx . Optimizer ( optax . adam ( 0.01 )) . init ( model ) for step in range ( 300 ): loss , model , optimizer = train_step ( model , x , y , optimizer ) if step % 50 == 0 : print ( f \"loss: { loss : .4f } \" ) # eval mode \"turns off\" layers like Dropout / BatchNorm model = model . eval () X_test = np . linspace ( x . min (), x . max (), 100 )[:, None ] preds = model ( X_test ) plt . scatter ( x , y , c = \"k\" , label = \"data\" ) plt . plot ( X_test , preds , c = \"b\" , linewidth = 2 , label = \"prediction\" ) plt . legend () plt . show () A Stateful Module Here is an example of creating a stateful module of a RollingMean metric and using them with jax.jit . For a real use cases use the metrics inside treex.metrics . class RollingMean ( tx . Module ): count : jnp . ndarray = tx . State . node () total : jnp . ndarray = tx . State . node () def __init__ ( self ): self . count = jnp . array ( 0 , dtype = jnp . int32 ) self . total = jnp . array ( 0.0 , dtype = jnp . float32 ) def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : self . count += np . prod ( x . shape ) self . total += x . sum () return self . total / self . count @jax . jit def update ( x : jnp . ndarray , metric : RollingMean ) -> Tuple [ jnp . ndarray , RollingMean ]: mean = metric ( x ) return mean , metric # return mean value and updated metric metric = RollingMean () for i in range ( 10 ): x = np . random . uniform ( - 1 , 1 , size = ( 100 , 1 )) mean , metric = update ( x , metric ) print ( mean )","title":"Treex"},{"location":"#treex","text":"A Pytree Module system for Deep Learning in JAX","title":"Treex"},{"location":"#main-features","text":"\ud83d\udca1 Intuitive : Modules contain their own parameters and respect Object Oriented semantics like in PyTorch and Keras. \ud83c\udf33 Pytree-based : Modules are Pytrees whose leaves are its parameters, meaning they are fully compatible with jit , grad , vmap , etc. Treex is implemented on top of Treeo and reexports all of its API for convenience. Getting Started | User Guide | Examples | Documentation","title":"Main Features"},{"location":"#what-is-included","text":"A base Module class. A nn module for with common layers implemented as wrappers over Flax layers. A losses module with common loss functions. A metrics module with common metrics. An Optimizer class that can wrap any optax optimizer.","title":"What is included?"},{"location":"#why-treex","text":"Show Despite all JAX benefits, current Module systems are not intuitive to new users and add additional complexity not present in frameworks like PyTorch or Keras. Treex takes inspiration from S4TF and delivers an intuitive experience using JAX Pytree infrastructure. Current Alternative's Drawbacks and Solutions Currently we have many alternatives like Flax, Haiku, Objax, that have one or more of the following drawbacks: * Module structure and parameter structure are separate, and parameters have to be manipulated around by the end-user, which is not intuitive. In Treex, parameters are stored in the modules themselves and can be accessed directly. * Monadic architecture adds complexity. Flax and Haiku use an `apply` method to call modules that set a context with parameters, rng, and different metadata, which adds additional overhead to the API and creates an asymmetry in how Modules are being used inside and outside a context. In Treex, modules can be called directly. * Among different frameworks, parameter surgery requires special consideration and is challenging to implement. Consider a standard workflow such as transfer learning, transferring parameters and state from a pre-trained module or submodule as part of a new module; in different frameworks, we have to know precisely how to extract their parameters and how to insert them into the new parameter structure/dictionaries such that it is in agreement with the new module structure. In Treex, just as in PyTorch / Keras, we enable to pass the (sub)module to the new module, and parameters are automatically added to the new structure. * Multiple frameworks deviate from JAX semantics and require particular versions of `jit`, `grad`, `vmap`, etc., which makes it harder to integrate with other JAX libraries. Treex's Modules are plain old JAX PyTrees and are compatible with any JAX library that supports them. * Other Pytree-based approaches like Parallax and Equinox do not have a total state management solution to handle complex states as encountered in Flax. Treex has the Filter and Update API, which is very expressive and can effectively handle systems with a complex state.","title":"Why Treex?"},{"location":"#installation","text":"Install using pip: pip install treex","title":"Installation"},{"location":"#getting-started","text":"This is a small appetizer to give you a feel for how using Treex looks like, be sure to checkout the User Guide for a more in-depth explanation. import treex as tx import numpy as np import jax , optax # create some data x = np . random . uniform ( size = ( 50 , 1 )) y = 1.3 * x ** 2 - 0.3 + np . random . normal ( size = x . shape ) # initialize a Module, its simple model = tx . MLP ([ 64 , 1 ]) . init ( key = 42 , inputs = x ) # define an optimizer, init with model params optimizer = tx . Optimizer ( optax . adam ( 4e-3 )) . init ( model ) # define loss function, notice # Modules are jit-abel and differentiable \ud83e\udd2f @jax . jit @jax . grad def loss_fn ( model : tx . MLP , x , y ): # forward is a simple call preds = model ( x ) # MSE return (( preds - y ) ** 2 ) . mean () # basic training loop for step in range ( 500 ): # grads have the same type as model grads : tx . MLP = loss_fn ( model , x , y ) # apply the gradient updates model = optimizer . update ( grads , model ) # Pytorch-like eval mode model = model . eval () preds = model ( x )","title":"Getting Started"},{"location":"#custom-modules","text":"Show Modules are Treeo `Tree`s, which are Pytrees. When creating core layers you often mark fields that will contain state that JAX should be aware as `nodes` by assigning class variables to the output of functions like `tx.Parameter.node()`: import treex as tx class Linear ( tx . Module ): # use Treeo's API to define Parameter nodes w : jnp . ndarray = tx . Parameter . node () b : jnp . ndarray = tx . Parameter . node () def __init__ ( self , features_out : int ): self . features_out = features_out def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : # init will call forward, we can know if we are inside it if self . initializing (): # `next_key` only available during `init` key = tx . next_key () # leverage shape inference self . w = jax . random . uniform ( key , shape = [ x . shape [ - 1 ], self . features_out ] ) self . b = jnp . zeros ( shape = [ self . features_out ]) # linear forward return jnp . dot ( x , self . w ) + self . b model = Linear ( 10 ) . init ( key = 42 , inputs = x ) Node field types (e.g. `tx.Parameter`) are called Kinds and Treex exports a whole family of Kinds which serve for differente purposes such as holding non-differentiable state (`tx.BatchStats`), metric's state (`tx.MetricState`), logging, etc. Checkout the [kinds](https://cgarciae.github.io/treex/user-guide/kinds) section for more information.","title":"Custom Modules"},{"location":"#composite-modules","text":"Show Composite Modules usually hold and call other Modules within them, while they would be instantiate inside `__init__` and used later in `__call__` like in Pytorch / Keras, in Treex you usually leverage the `@tx.compact` decorator over the `__call__` method to define the submodules inline. class MLP ( tx . Module ): def __init__ ( self , features : Sequence [ int ]): self . features = features # compact lets you define submodules on the fly @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for units in self . features [: - 1 ]: x = Linear ( units )( x ) x = jax . nn . relu ( x ) return Linear ( self . features [ - 1 ])( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) Under the hood all calls to submodule constructors (e.g. `Linear(...)`) inside `compact` are assigned to fields in the parent Module (`MLP`) so they are part of the same Pytree, their field names are available under the `._subtrees` attribute. `compact` must always define submodules in the same order.","title":"Composite Modules"},{"location":"#status","text":"Treex is in an early stage, things might break between versions but we will respect semanting versioning. Since Treex layers are numerically equivalent to Flax, it borrows some maturity and yields more confidence over its results. Feedback is much appreciated. Roadmap : Wrap all Flax Linen Modules Implement more layers, losses, and metrics. Create applications and pretrained Modules. Contributions are welcomed!","title":"Status"},{"location":"#sponsors","text":"Quansight - paid development time","title":"Sponsors \ud83d\udc9a"},{"location":"#examples","text":"Checkout the /examples directory for more detailed examples. Here are a few additional toy examples:","title":"Examples"},{"location":"#linear-regression","text":"This is a simple but realistic example of how Treex is used. from functools import partial from typing import Union import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpy as np import optax import treex as tx x = np . random . uniform ( size = ( 500 , 1 )) y = 1.4 * x - 0.3 + np . random . normal ( scale = 0.1 , size = ( 500 , 1 )) # differentiate only w.r.t. parameters def loss_fn ( params , model , x , y ): # merge params into model model = model . merge ( params ) preds = model ( x ) loss = jnp . mean (( preds - y ) ** 2 ) # the model may contain state updates # so it should be returned return loss , model grad_fn = jax . value_and_grad ( loss_fn , has_aux = True ) # both model and optimizer are jit-able @jax . jit def train_step ( model , x , y , optimizer ): # select only the parameters params = model . parameters () ( loss , model ), grads = grad_fn ( params , model , x , y ) # update params and model params = optimizer . update ( grads , params ) model = model . merge ( params ) # return new model and optimizer return loss , model , optimizer model = tx . Linear ( 1 ) . init ( 42 , x ) optimizer = tx . Optimizer ( optax . adam ( 0.01 )) . init ( model ) for step in range ( 300 ): loss , model , optimizer = train_step ( model , x , y , optimizer ) if step % 50 == 0 : print ( f \"loss: { loss : .4f } \" ) # eval mode \"turns off\" layers like Dropout / BatchNorm model = model . eval () X_test = np . linspace ( x . min (), x . max (), 100 )[:, None ] preds = model ( X_test ) plt . scatter ( x , y , c = \"k\" , label = \"data\" ) plt . plot ( X_test , preds , c = \"b\" , linewidth = 2 , label = \"prediction\" ) plt . legend () plt . show ()","title":"Linear Regression"},{"location":"#a-stateful-module","text":"Here is an example of creating a stateful module of a RollingMean metric and using them with jax.jit . For a real use cases use the metrics inside treex.metrics . class RollingMean ( tx . Module ): count : jnp . ndarray = tx . State . node () total : jnp . ndarray = tx . State . node () def __init__ ( self ): self . count = jnp . array ( 0 , dtype = jnp . int32 ) self . total = jnp . array ( 0.0 , dtype = jnp . float32 ) def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : self . count += np . prod ( x . shape ) self . total += x . sum () return self . total / self . count @jax . jit def update ( x : jnp . ndarray , metric : RollingMean ) -> Tuple [ jnp . ndarray , RollingMean ]: mean = metric ( x ) return mean , metric # return mean value and updated metric metric = RollingMean () for i in range ( 10 ): x = np . random . uniform ( - 1 , 1 , size = ( 100 , 1 )) mean , metric = update ( x , metric ) print ( mean )","title":"A Stateful Module"},{"location":"api/Apply/","text":"treex.Apply Mixin that adds a .apply() method to the class. apply ( self , f , * rest , * , inplace = False ) apply is a wrapper over treeo.apply that passes self as the second argument. Parameters: Name Type Description Default f Callable[..., NoneType] The function to apply. required *rest ~A additional pytrees. () inplace bool If True , the input obj is mutated. False Returns: Type Description ~A A new pytree with the updated Trees or the same input obj if inplace is True . Source code in treeo/mixins.py def apply ( self : A , f : tp . Callable [ ... , None ], * rest : A , inplace : bool = False ) -> A : \"\"\" `apply` is a wrapper over `treeo.apply` that passes `self` as the second argument. Arguments: f: The function to apply. *rest: additional pytrees. inplace: If `True`, the input `obj` is mutated. Returns: A new pytree with the updated Trees or the same input `obj` if `inplace` is `True`. \"\"\" return api . apply ( f , self , * rest , inplace = inplace )","title":"Apply"},{"location":"api/Apply/#treexapply","text":"Mixin that adds a .apply() method to the class.","title":"treex.Apply"},{"location":"api/Apply/#treeo.mixins.Apply.apply","text":"apply is a wrapper over treeo.apply that passes self as the second argument. Parameters: Name Type Description Default f Callable[..., NoneType] The function to apply. required *rest ~A additional pytrees. () inplace bool If True , the input obj is mutated. False Returns: Type Description ~A A new pytree with the updated Trees or the same input obj if inplace is True . Source code in treeo/mixins.py def apply ( self : A , f : tp . Callable [ ... , None ], * rest : A , inplace : bool = False ) -> A : \"\"\" `apply` is a wrapper over `treeo.apply` that passes `self` as the second argument. Arguments: f: The function to apply. *rest: additional pytrees. inplace: If `True`, the input `obj` is mutated. Returns: A new pytree with the updated Trees or the same input `obj` if `inplace` is `True`. \"\"\" return api . apply ( f , self , * rest , inplace = inplace )","title":"apply()"},{"location":"api/ArrayLike/","text":"treex.ArrayLike","title":"ArrayLike"},{"location":"api/ArrayLike/#treexarraylike","text":"","title":"treex.ArrayLike"},{"location":"api/BatchNorm/","text":"treex.BatchNorm BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training or self.frozen is used unless use_running_average is explicitly passed via __call__ . __call__ ( self , x , use_running_average = None ) special Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , use_running_average : tp . Optional [ bool ] = None ) -> jnp . ndarray : \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , use_running_average = True , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( batch_stats = dict ( mean = self . mean , var = self . var , ), params = params , ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training and not self . frozen and self . initialized ) # call apply output , variables = self . module . apply ( variables , x , mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training , ) variables = variables . unfreeze () # update batch_stats if \"batch_stats\" in variables : self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , axis =- 1 , momentum = 0.99 , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>, axis_name=None, axis_index_groups=None) special Parameters: Name Type Description Default features_in the number of input features. required axis int the feature or non-batch axis of the input. -1 momentum Union[float, jax._src.numpy.lax_numpy.ndarray] decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/norm.py def __init__ ( self , * , axis : int = - 1 , momentum : tp . Union [ float , jnp . ndarray ] = 0.99 , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , axis_name : tp . Optional [ str ] = None , axis_index_groups : tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . axis = axis self . momentum = jnp . asarray ( momentum ) self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . axis_name = axis_name self . axis_index_groups = axis_index_groups self . mean = None self . var = None self . scale = None self . bias = None","title":"BatchNorm"},{"location":"api/BatchNorm/#treexbatchnorm","text":"BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training or self.frozen is used unless use_running_average is explicitly passed via __call__ .","title":"treex.BatchNorm"},{"location":"api/BatchNorm/#treex.nn.norm.BatchNorm.__call__","text":"Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , use_running_average : tp . Optional [ bool ] = None ) -> jnp . ndarray : \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , use_running_average = True , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( batch_stats = dict ( mean = self . mean , var = self . var , ), params = params , ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training and not self . frozen and self . initialized ) # call apply output , variables = self . module . apply ( variables , x , mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training , ) variables = variables . unfreeze () # update batch_stats if \"batch_stats\" in variables : self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/BatchNorm/#treex.nn.norm.BatchNorm.__init__","text":"Parameters: Name Type Description Default features_in the number of input features. required axis int the feature or non-batch axis of the input. -1 momentum Union[float, jax._src.numpy.lax_numpy.ndarray] decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/norm.py def __init__ ( self , * , axis : int = - 1 , momentum : tp . Union [ float , jnp . ndarray ] = 0.99 , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , axis_name : tp . Optional [ str ] = None , axis_index_groups : tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . axis = axis self . momentum = jnp . asarray ( momentum ) self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . axis_name = axis_name self . axis_index_groups = axis_index_groups self . mean = None self . var = None self . scale = None self . bias = None","title":"__init__()"},{"location":"api/BatchStat/","text":"treex.BatchStat","title":"BatchStat"},{"location":"api/BatchStat/#treexbatchstat","text":"","title":"treex.BatchStat"},{"location":"api/Cache/","text":"treex.Cache","title":"Cache"},{"location":"api/Cache/#treexcache","text":"","title":"treex.Cache"},{"location":"api/Compact/","text":"treex.Compact first_run : bool property readonly Returns: Type Description bool True if its currently the first run of a compact method. get_field ( self , field_name , initializer ) A method that gets a field with the given name if exists, otherwise it initializes it and returns it. Currently the follow restrictions apply: The field must be declared in the class definition. The method can only be called inside a compact context. Parameters: Name Type Description Default field_name str The name of the field to get. required initializer Callable[[], ~A] The function to initialize the field if it does not exist. required Returns: Type Description ~A The field value. Source code in treeo/mixins.py def get_field ( self , field_name : str , initializer : tp . Callable [[], A ], ) -> A : \"\"\" A method that gets a field with the given name if exists, otherwise it initializes it and returns it. Currently the follow restrictions apply: * The field must be declared in the class definition. * The method can only be called inside a `compact` context. Arguments: field_name: The name of the field to get. initializer: The function to initialize the field if it does not exist. Returns: The field value. \"\"\" value : A if field_name not in self . _field_metadata : raise ValueError ( f \"Metadata for field ' { field_name } ' does not exist.\" ) if field_name in vars ( self ): value = getattr ( self , field_name ) else : if tree_m . _COMPACT_CONTEXT . in_compact and not self . first_run : raise RuntimeError ( f \"Trying to initialize field ' { field_name } ' after the first run of `compact`.\" ) value = initializer () setattr ( self , field_name , value ) return value","title":"Compact"},{"location":"api/Compact/#treexcompact","text":"","title":"treex.Compact"},{"location":"api/Compact/#treeo.mixins.Compact.first_run","text":"Returns: Type Description bool True if its currently the first run of a compact method.","title":"first_run"},{"location":"api/Compact/#treeo.mixins.Compact.get_field","text":"A method that gets a field with the given name if exists, otherwise it initializes it and returns it. Currently the follow restrictions apply: The field must be declared in the class definition. The method can only be called inside a compact context. Parameters: Name Type Description Default field_name str The name of the field to get. required initializer Callable[[], ~A] The function to initialize the field if it does not exist. required Returns: Type Description ~A The field value. Source code in treeo/mixins.py def get_field ( self , field_name : str , initializer : tp . Callable [[], A ], ) -> A : \"\"\" A method that gets a field with the given name if exists, otherwise it initializes it and returns it. Currently the follow restrictions apply: * The field must be declared in the class definition. * The method can only be called inside a `compact` context. Arguments: field_name: The name of the field to get. initializer: The function to initialize the field if it does not exist. Returns: The field value. \"\"\" value : A if field_name not in self . _field_metadata : raise ValueError ( f \"Metadata for field ' { field_name } ' does not exist.\" ) if field_name in vars ( self ): value = getattr ( self , field_name ) else : if tree_m . _COMPACT_CONTEXT . in_compact and not self . first_run : raise RuntimeError ( f \"Trying to initialize field ' { field_name } ' after the first run of `compact`.\" ) value = initializer () setattr ( self , field_name , value ) return value","title":"get_field()"},{"location":"api/Conv/","text":"treex.Conv Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x ) special Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" if self . initializing (): variables = self . module . init ({ \"params\" : next_key ()}, x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , features_out , kernel_size , * , strides = None , padding = 'SAME' , input_dilation = None , kernel_dilation = None , feature_group_count = 1 , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>) special Parameters: Name Type Description Default features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/conv.py def __init__ ( self , features_out : int , kernel_size : tp . Union [ int , tp . Iterable [ int ]], * , strides : tp . Optional [ tp . Iterable [ int ]] = None , padding : tp . Union [ str , tp . Iterable [ tp . Tuple [ int , int ]]] = \"SAME\" , input_dilation : tp . Optional [ tp . Iterable [ int ]] = None , kernel_dilation : tp . Optional [ tp . Iterable [ int ]] = None , feature_group_count : int = 1 , use_bias : bool = True , dtype : flax_module . Dtype = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_out = features_out self . kernel_size = kernel_size self . strides = strides self . padding = padding self . input_dilation = input_dilation self . kernel_dilation = kernel_dilation self . feature_group_count = feature_group_count self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . kernel = None self . bias = None","title":"Conv"},{"location":"api/Conv/#treexconv","text":"Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.Conv"},{"location":"api/Conv/#treex.nn.conv.Conv.__call__","text":"Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" if self . initializing (): variables = self . module . init ({ \"params\" : next_key ()}, x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/Conv/#treex.nn.conv.Conv.__init__","text":"Parameters: Name Type Description Default features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/conv.py def __init__ ( self , features_out : int , kernel_size : tp . Union [ int , tp . Iterable [ int ]], * , strides : tp . Optional [ tp . Iterable [ int ]] = None , padding : tp . Union [ str , tp . Iterable [ tp . Tuple [ int , int ]]] = \"SAME\" , input_dilation : tp . Optional [ tp . Iterable [ int ]] = None , kernel_dilation : tp . Optional [ tp . Iterable [ int ]] = None , feature_group_count : int = 1 , use_bias : bool = True , dtype : flax_module . Dtype = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_out = features_out self . kernel_size = kernel_size self . strides = strides self . padding = padding self . input_dilation = input_dilation self . kernel_dilation = kernel_dilation self . feature_group_count = feature_group_count self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . kernel = None self . bias = None","title":"__init__()"},{"location":"api/Copy/","text":"treex.Copy Mixin that adds a .copy() method to the class. copy ( self ) copy is a wrapper over treeo.copy that passes self as the first argument. Source code in treeo/mixins.py def copy ( self : A ) -> A : \"\"\" `copy` is a wrapper over `treeo.copy` that passes `self` as the first argument. \"\"\" return tree_m . copy ( self )","title":"Copy"},{"location":"api/Copy/#treexcopy","text":"Mixin that adds a .copy() method to the class.","title":"treex.Copy"},{"location":"api/Copy/#treeo.mixins.Copy.copy","text":"copy is a wrapper over treeo.copy that passes self as the first argument. Source code in treeo/mixins.py def copy ( self : A ) -> A : \"\"\" `copy` is a wrapper over `treeo.copy` that passes `self` as the first argument. \"\"\" return tree_m . copy ( self )","title":"copy()"},{"location":"api/Dropout/","text":"treex.Dropout Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training or self.frozen is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ . __call__ ( self , x , deterministic = None , rng = None ) special Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x : jnp . ndarray , deterministic : tp . Optional [ bool ] = None , rng = None ) -> jnp . ndarray : \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = ( not deterministic if deterministic is not None else self . training and not self . frozen ) if rng is None : rng = self . next_key () if training else self . next_key . key # call apply output = self . module . apply ( variables , x , deterministic = not training , rngs = { \"dropout\" : rng }, ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , rate , broadcast_dims = ()) special Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate : float , broadcast_dims : tp . Iterable [ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . rate = rate self . broadcast_dims = broadcast_dims self . next_key = KeySeq ()","title":"Dropout"},{"location":"api/Dropout/#treexdropout","text":"Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training or self.frozen is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ .","title":"treex.Dropout"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.__call__","text":"Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x : jnp . ndarray , deterministic : tp . Optional [ bool ] = None , rng = None ) -> jnp . ndarray : \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = ( not deterministic if deterministic is not None else self . training and not self . frozen ) if rng is None : rng = self . next_key () if training else self . next_key . key # call apply output = self . module . apply ( variables , x , deterministic = not training , rngs = { \"dropout\" : rng }, ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/Dropout/#treex.nn.dropout.Dropout.__init__","text":"Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate : float , broadcast_dims : tp . Iterable [ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . rate = rate self . broadcast_dims = broadcast_dims self . next_key = KeySeq ()","title":"__init__()"},{"location":"api/Embed/","text":"treex.Embed A linear transformation applied over the last dimension of the input. Embed is implemented as a wrapper over flax.linen.Embed , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. __call__ ( self , x ) special Embeds the inputs along the last dimension. Parameters: Name Type Description Default inputs input data, all dimensions are considered batch dimensions. required Returns: Type Description ndarray Output which is embedded input data. The output shape follows the input, with an additional features dimension appended. Source code in treex/nn/embed.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Embeds the inputs along the last dimension. Arguments: inputs: input data, all dimensions are considered batch dimensions. Returns: Output which is embedded input data. The output shape follows the input, with an additional `features` dimension appended. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ()} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . embedding = params [ \"embedding\" ] assert self . embedding is not None params = { \"embedding\" : self . embedding } output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , num_embeddings , features , * , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, embedding_init=<function variance_scaling.<locals>.init at 0x7fec3ff74040>, name=None) special Parameters: Name Type Description Default num_embeddings int number of embeddings. required features int number of feature dimensions for each embedding. required dtype Any the dtype of the embedding vectors (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> embedding_init Callable[[Any, Iterable[int], Any], Any] embedding initializer. <function variance_scaling.<locals>.init at 0x7fec3ff74040> Source code in treex/nn/embed.py def __init__ ( self , num_embeddings : int , features : int , * , dtype : flax_module . Dtype = jnp . float32 , embedding_init : tp . Callable [ [ PRNGKey , Shape , Dtype ], Array ] = flax_module . default_embed_init , name : tp . Optional [ str ] = None , ): \"\"\" Arguments: num_embeddings: number of embeddings. features: number of feature dimensions for each embedding. dtype: the dtype of the embedding vectors (default: float32). embedding_init: embedding initializer. \"\"\" super () . __init__ ( name = name ) self . num_embeddings = num_embeddings self . features = features self . dtype = dtype self . embedding_init = embedding_init self . embedding = None","title":"Embed"},{"location":"api/Embed/#treexembed","text":"A linear transformation applied over the last dimension of the input. Embed is implemented as a wrapper over flax.linen.Embed , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers.","title":"treex.Embed"},{"location":"api/Embed/#treex.nn.embed.Embed.__call__","text":"Embeds the inputs along the last dimension. Parameters: Name Type Description Default inputs input data, all dimensions are considered batch dimensions. required Returns: Type Description ndarray Output which is embedded input data. The output shape follows the input, with an additional features dimension appended. Source code in treex/nn/embed.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Embeds the inputs along the last dimension. Arguments: inputs: input data, all dimensions are considered batch dimensions. Returns: Output which is embedded input data. The output shape follows the input, with an additional `features` dimension appended. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ()} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . embedding = params [ \"embedding\" ] assert self . embedding is not None params = { \"embedding\" : self . embedding } output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/Embed/#treex.nn.embed.Embed.__init__","text":"Parameters: Name Type Description Default num_embeddings int number of embeddings. required features int number of feature dimensions for each embedding. required dtype Any the dtype of the embedding vectors (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> embedding_init Callable[[Any, Iterable[int], Any], Any] embedding initializer. <function variance_scaling.<locals>.init at 0x7fec3ff74040> Source code in treex/nn/embed.py def __init__ ( self , num_embeddings : int , features : int , * , dtype : flax_module . Dtype = jnp . float32 , embedding_init : tp . Callable [ [ PRNGKey , Shape , Dtype ], Array ] = flax_module . default_embed_init , name : tp . Optional [ str ] = None , ): \"\"\" Arguments: num_embeddings: number of embeddings. features: number of feature dimensions for each embedding. dtype: the dtype of the embedding vectors (default: float32). embedding_init: embedding initializer. \"\"\" super () . __init__ ( name = name ) self . num_embeddings = num_embeddings self . features = features self . dtype = dtype self . embedding_init = embedding_init self . embedding = None","title":"__init__()"},{"location":"api/Extensions/","text":"treex.Extensions Mixin that adds all available mixins from treeo.mixins except KindMixin .","title":"Extensions"},{"location":"api/Extensions/#treexextensions","text":"Mixin that adds all available mixins from treeo.mixins except KindMixin .","title":"treex.Extensions"},{"location":"api/FieldInfo/","text":"treex.FieldInfo","title":"FieldInfo"},{"location":"api/FieldInfo/#treexfieldinfo","text":"","title":"treex.FieldInfo"},{"location":"api/FieldMetadata/","text":"treex.FieldMetadata","title":"FieldMetadata"},{"location":"api/FieldMetadata/#treexfieldmetadata","text":"","title":"treex.FieldMetadata"},{"location":"api/Filter/","text":"treex.Filter Mixin that adds a .filter() method to the class. filter ( self , * filters , * , inplace = False , flatten_mode = None ) filter is a wrapper over treeo.filter that passes self as the first argument. Parameters: Name Type Description Default *filters Union[Type[Any], Callable[[FieldInfo], bool]] Types to filter by, membership is determined by issubclass , or callables that take in a FieldInfo and return a bool . () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation. None Returns: Type Description ~A A new pytree with the filtered fields. If inplace is True , obj is returned. Source code in treeo/mixins.py def filter ( self : A , * filters : api . Filter , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , ) -> A : \"\"\" `filter` is a wrapper over `treeo.filter` that passes `self` as the first argument. Arguments: *filters: Types to filter by, membership is determined by `issubclass`, or callables that take in a `FieldInfo` and return a `bool`. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation. Returns: A new pytree with the filtered fields. If `inplace` is `True`, `obj` is returned. \"\"\" return api . filter ( self , * filters , inplace = inplace , flatten_mode = flatten_mode )","title":"Filter"},{"location":"api/Filter/#treexfilter","text":"Mixin that adds a .filter() method to the class.","title":"treex.Filter"},{"location":"api/Filter/#treeo.mixins.Filter.filter","text":"filter is a wrapper over treeo.filter that passes self as the first argument. Parameters: Name Type Description Default *filters Union[Type[Any], Callable[[FieldInfo], bool]] Types to filter by, membership is determined by issubclass , or callables that take in a FieldInfo and return a bool . () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation. None Returns: Type Description ~A A new pytree with the filtered fields. If inplace is True , obj is returned. Source code in treeo/mixins.py def filter ( self : A , * filters : api . Filter , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , ) -> A : \"\"\" `filter` is a wrapper over `treeo.filter` that passes `self` as the first argument. Arguments: *filters: Types to filter by, membership is determined by `issubclass`, or callables that take in a `FieldInfo` and return a `bool`. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation. Returns: A new pytree with the filtered fields. If `inplace` is `True`, `obj` is returned. \"\"\" return api . filter ( self , * filters , inplace = inplace , flatten_mode = flatten_mode )","title":"filter()"},{"location":"api/Filters/","text":"treex.Filters batch_stats ( self , * filters ) Returns a copy of the Module with only tx.BatchStat TreeParts, alias for filter(tx.BatchStat) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def batch_stats ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.BatchStat TreeParts, alias for `filter(tx.BatchStat)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . BatchStat , * filters ) caches ( self , * filters ) Returns a copy of the Module with only tx.Cache TreeParts, alias for filter(tx.Cache) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def caches ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Cache TreeParts, alias for `filter(tx.Cache)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Cache , * filters ) logs ( self , * filters ) Returns a copy of the Module with only tx.Log TreeParts, alias for filter(tx.Log) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Log TreeParts, alias for `filter(tx.Log)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Log , * filters ) loss_logs ( self , * filters ) Returns a copy of the Module with only tx.Loss TreeParts, alias for filter(tx.Loss) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def loss_logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Loss TreeParts, alias for `filter(tx.Loss)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . LossLog , * filters ) metric_logs ( self , * filters ) Returns a copy of the Module with only tx.Metric TreeParts, alias for filter(tx.Metric) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def metric_logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Metric TreeParts, alias for `filter(tx.Metric)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . MetricLog , * filters ) model_states ( self , * filters ) Returns a copy of the Module with only tx.ModelState TreeParts, alias for filter(tx.ModelState) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def model_states ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.ModelState TreeParts, alias for `filter(tx.ModelState)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . ModelState , * filters ) parameters ( self , * filters ) Returns a copy of the Module with only tx.Parameter TreeParts, alias for filter(tx.Parameter) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def parameters ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Parameter TreeParts, alias for `filter(tx.Parameter)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Parameter , * filters ) rngs ( self , * filters ) Returns a copy of the Module with only tx.Rng TreeParts, alias for filter(tx.Rng) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def rngs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Rng TreeParts, alias for `filter(tx.Rng)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Rng , * filters ) states ( self , * filters ) Returns a copy of the Module with only tx.State TreeParts, alias for filter(tx.State) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def states ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.State TreeParts, alias for `filter(tx.State)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . State , * filters ) trainable_parameters ( self , * filters ) Returns a copy of the Module with only tx.Parameter TreeParts which are not frozen, alias for filter(tx.Parameter, lambda field: not field.module.frozen) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def trainable_parameters ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Parameter TreeParts which are not frozen, alias for `filter(tx.Parameter, lambda field: not field.module.frozen)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return self . parameters ( lambda field : not field . module . frozen , * filters )","title":"Filters"},{"location":"api/Filters/#treexfilters","text":"","title":"treex.Filters"},{"location":"api/Filters/#treex.treex.Filters.batch_stats","text":"Returns a copy of the Module with only tx.BatchStat TreeParts, alias for filter(tx.BatchStat) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def batch_stats ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.BatchStat TreeParts, alias for `filter(tx.BatchStat)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . BatchStat , * filters )","title":"batch_stats()"},{"location":"api/Filters/#treex.treex.Filters.caches","text":"Returns a copy of the Module with only tx.Cache TreeParts, alias for filter(tx.Cache) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def caches ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Cache TreeParts, alias for `filter(tx.Cache)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Cache , * filters )","title":"caches()"},{"location":"api/Filters/#treex.treex.Filters.logs","text":"Returns a copy of the Module with only tx.Log TreeParts, alias for filter(tx.Log) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Log TreeParts, alias for `filter(tx.Log)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Log , * filters )","title":"logs()"},{"location":"api/Filters/#treex.treex.Filters.loss_logs","text":"Returns a copy of the Module with only tx.Loss TreeParts, alias for filter(tx.Loss) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def loss_logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Loss TreeParts, alias for `filter(tx.Loss)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . LossLog , * filters )","title":"loss_logs()"},{"location":"api/Filters/#treex.treex.Filters.metric_logs","text":"Returns a copy of the Module with only tx.Metric TreeParts, alias for filter(tx.Metric) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def metric_logs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Metric TreeParts, alias for `filter(tx.Metric)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . MetricLog , * filters )","title":"metric_logs()"},{"location":"api/Filters/#treex.treex.Filters.model_states","text":"Returns a copy of the Module with only tx.ModelState TreeParts, alias for filter(tx.ModelState) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def model_states ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.ModelState TreeParts, alias for `filter(tx.ModelState)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . ModelState , * filters )","title":"model_states()"},{"location":"api/Filters/#treex.treex.Filters.parameters","text":"Returns a copy of the Module with only tx.Parameter TreeParts, alias for filter(tx.Parameter) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def parameters ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Parameter TreeParts, alias for `filter(tx.Parameter)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Parameter , * filters )","title":"parameters()"},{"location":"api/Filters/#treex.treex.Filters.rngs","text":"Returns a copy of the Module with only tx.Rng TreeParts, alias for filter(tx.Rng) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def rngs ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Rng TreeParts, alias for `filter(tx.Rng)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . Rng , * filters )","title":"rngs()"},{"location":"api/Filters/#treex.treex.Filters.states","text":"Returns a copy of the Module with only tx.State TreeParts, alias for filter(tx.State) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def states ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.State TreeParts, alias for `filter(tx.State)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return to . filter ( self , types . State , * filters )","title":"states()"},{"location":"api/Filters/#treex.treex.Filters.trainable_parameters","text":"Returns a copy of the Module with only tx.Parameter TreeParts which are not frozen, alias for filter(tx.Parameter, lambda field: not field.module.frozen) . Parameters: Name Type Description Default filters Union[Type[Type[Any]], Callable[[treeo.tree.FieldInfo], bool]] additional filters passed to filter . () Source code in treex/treex.py def trainable_parameters ( self : A , * filters : types . Filter ) -> A : \"\"\" Returns a copy of the Module with only tx.Parameter TreeParts which are not frozen, alias for `filter(tx.Parameter, lambda field: not field.module.frozen)`. Arguments: filters: additional filters passed to `filter`. \"\"\" return self . parameters ( lambda field : not field . module . frozen , * filters )","title":"trainable_parameters()"},{"location":"api/Flatten/","text":"treex.Flatten","title":"Flatten"},{"location":"api/Flatten/#treexflatten","text":"","title":"treex.Flatten"},{"location":"api/FlattenMode/","text":"treex.FlattenMode An enumeration.","title":"FlattenMode"},{"location":"api/FlattenMode/#treexflattenmode","text":"An enumeration.","title":"treex.FlattenMode"},{"location":"api/FlaxModule/","text":"treex.FlaxModule","title":"FlaxModule"},{"location":"api/FlaxModule/#treexflaxmodule","text":"","title":"treex.FlaxModule"},{"location":"api/GroupNorm/","text":"treex.GroupNorm Group normalization Module (arxiv.org/abs/1803.08494). GroupNorm is implemented as a wrapper over flax.linen.GroupNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics. The user should either specify the total number of channel groups or the number of channels per group.. __call__ ( self , x ) special Normalizes the individual input over equally-sized group of channels. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes the individual input over equally-sized group of channels. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , num_groups = 32 , group_size = None , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>) special Parameters: Name Type Description Default num_groups Optional[int] the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. 32 group_size Optional[int] the number of channels in a group. None epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , num_groups : tp . Optional [ int ] = 32 , group_size : tp . Optional [ int ] = None , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: num_groups: the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. group_size: the number of channels in a group. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . num_groups = num_groups self . group_size = group_size self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"GroupNorm"},{"location":"api/GroupNorm/#treexgroupnorm","text":"Group normalization Module (arxiv.org/abs/1803.08494). GroupNorm is implemented as a wrapper over flax.linen.GroupNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics. The user should either specify the total number of channel groups or the number of channels per group..","title":"treex.GroupNorm"},{"location":"api/GroupNorm/#treex.nn.norm.GroupNorm.__call__","text":"Normalizes the individual input over equally-sized group of channels. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes the individual input over equally-sized group of channels. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/GroupNorm/#treex.nn.norm.GroupNorm.__init__","text":"Parameters: Name Type Description Default num_groups Optional[int] the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. 32 group_size Optional[int] the number of channels in a group. None epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , num_groups : tp . Optional [ int ] = 32 , group_size : tp . Optional [ int ] = None , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: num_groups: the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. group_size: the number of channels in a group. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . num_groups = num_groups self . group_size = group_size self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"__init__()"},{"location":"api/HaikuModule/","text":"treex.HaikuModule","title":"HaikuModule"},{"location":"api/HaikuModule/#treexhaikumodule","text":"","title":"treex.HaikuModule"},{"location":"api/Hashable/","text":"treex.Hashable A hashable immutable wrapper around non-hashable values","title":"Hashable"},{"location":"api/Hashable/#treexhashable","text":"A hashable immutable wrapper around non-hashable values","title":"treex.Hashable"},{"location":"api/Initializer/","text":"treex.Initializer Initialize a field from a function that expects a single argument with a PRNGKey. Initializers are called by Module.init and replace the value of the field they are assigned to. __init__ ( self , f ) special Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], Any] A function that takes a PRNGKey and returns the initial value of the field. required Source code in treex/types.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], tp . Any ]): \"\"\" Arguments: f: A function that takes a PRNGKey and returns the initial value of the field. \"\"\" self . f = f","title":"Initializer"},{"location":"api/Initializer/#treexinitializer","text":"Initialize a field from a function that expects a single argument with a PRNGKey. Initializers are called by Module.init and replace the value of the field they are assigned to.","title":"treex.Initializer"},{"location":"api/Initializer/#treex.types.Initializer.__init__","text":"Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], Any] A function that takes a PRNGKey and returns the initial value of the field. required Source code in treex/types.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], tp . Any ]): \"\"\" Arguments: f: A function that takes a PRNGKey and returns the initial value of the field. \"\"\" self . f = f","title":"__init__()"},{"location":"api/Inputs/","text":"treex.Inputs","title":"Inputs"},{"location":"api/Inputs/#treexinputs","text":"","title":"treex.Inputs"},{"location":"api/KeySeq/","text":"treex.KeySeq KeySeq is simple module that can produce a sequence of PRNGKeys. Examples: class Dropout ( Module ): rng : KeySeq () def __init__ ( self , rate : float ): self . next_key = KeySeq () ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : key = self . next_key () mask = jax . random . bernoulli ( key , 1.0 - self . rate ) ... __call__ ( self , * , axis_name = None ) special Return a new PRNGKey and updates the internal rng state. Returns: Type Description ndarray A PRNGKey. Source code in treex/key_seq.py def __call__ ( self , * , axis_name : tp . Optional [ tp . Any ] = None ) -> jnp . ndarray : \"\"\" Return a new PRNGKey and updates the internal rng state. Returns: A PRNGKey. \"\"\" key : jnp . ndarray assert isinstance ( self . key , jnp . ndarray ) key , self . key = utils . iter_split ( self . key ) if axis_name is None : axis_name = self . axis_name if axis_name is not None : axis_index = jax . lax . axis_index ( axis_name ) key = jax . random . fold_in ( key , axis_index ) return key __init__ ( self , key = None , * , axis_name = None ) special Parameters: Name Type Description Default key Union[jax._src.numpy.lax_numpy.ndarray, int] An optional PRNGKey to initialize the KeySeq with. None Source code in treex/key_seq.py def __init__ ( self , key : tp . Optional [ tp . Union [ jnp . ndarray , int ]] = None , * , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: key: An optional PRNGKey to initialize the KeySeq with. \"\"\" self . key = ( utils . Key ( key ) if isinstance ( key , int ) else key if isinstance ( key , ( jnp . ndarray , np . ndarray )) else types . Initializer ( lambda key : key ) ) self . axis_name = axis_name","title":"KeySeq"},{"location":"api/KeySeq/#treexkeyseq","text":"KeySeq is simple module that can produce a sequence of PRNGKeys. Examples: class Dropout ( Module ): rng : KeySeq () def __init__ ( self , rate : float ): self . next_key = KeySeq () ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : key = self . next_key () mask = jax . random . bernoulli ( key , 1.0 - self . rate ) ...","title":"treex.KeySeq"},{"location":"api/KeySeq/#treex.key_seq.KeySeq.__call__","text":"Return a new PRNGKey and updates the internal rng state. Returns: Type Description ndarray A PRNGKey. Source code in treex/key_seq.py def __call__ ( self , * , axis_name : tp . Optional [ tp . Any ] = None ) -> jnp . ndarray : \"\"\" Return a new PRNGKey and updates the internal rng state. Returns: A PRNGKey. \"\"\" key : jnp . ndarray assert isinstance ( self . key , jnp . ndarray ) key , self . key = utils . iter_split ( self . key ) if axis_name is None : axis_name = self . axis_name if axis_name is not None : axis_index = jax . lax . axis_index ( axis_name ) key = jax . random . fold_in ( key , axis_index ) return key","title":"__call__()"},{"location":"api/KeySeq/#treex.key_seq.KeySeq.__init__","text":"Parameters: Name Type Description Default key Union[jax._src.numpy.lax_numpy.ndarray, int] An optional PRNGKey to initialize the KeySeq with. None Source code in treex/key_seq.py def __init__ ( self , key : tp . Optional [ tp . Union [ jnp . ndarray , int ]] = None , * , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: key: An optional PRNGKey to initialize the KeySeq with. \"\"\" self . key = ( utils . Key ( key ) if isinstance ( key , int ) else key if isinstance ( key , ( jnp . ndarray , np . ndarray )) else types . Initializer ( lambda key : key ) ) self . axis_name = axis_name","title":"__init__()"},{"location":"api/KindMixin/","text":"treex.KindMixin","title":"KindMixin"},{"location":"api/KindMixin/#treexkindmixin","text":"","title":"treex.KindMixin"},{"location":"api/Lambda/","text":"treex.Lambda A Module that applies a pure function to its input. __call__ ( self , x ) special Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequential.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f ( x ) __init__ ( self , f ) special Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequential.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], jnp . ndarray ]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f","title":"Lambda"},{"location":"api/Lambda/#treexlambda","text":"A Module that applies a pure function to its input.","title":"treex.Lambda"},{"location":"api/Lambda/#treex.nn.sequential.Lambda.__call__","text":"Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequential.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f ( x )","title":"__call__()"},{"location":"api/Lambda/#treex.nn.sequential.Lambda.__init__","text":"Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequential.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], jnp . ndarray ]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f","title":"__init__()"},{"location":"api/LayerNorm/","text":"treex.LayerNorm LayerNorm Module. LayerNorm is implemented as a wrapper over flax.linen.LayerNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. It normalizes the activations of the layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1. __call__ ( self , x ) special Normalizes individual input on the last axis (channels) of the input data. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes individual input on the last axis (channels) of the input data. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>) special Parameters: Name Type Description Default epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"LayerNorm"},{"location":"api/LayerNorm/#treexlayernorm","text":"LayerNorm Module. LayerNorm is implemented as a wrapper over flax.linen.LayerNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. It normalizes the activations of the layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.","title":"treex.LayerNorm"},{"location":"api/LayerNorm/#treex.nn.norm.LayerNorm.__call__","text":"Normalizes individual input on the last axis (channels) of the input data. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes individual input on the last axis (channels) of the input data. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/LayerNorm/#treex.nn.norm.LayerNorm.__init__","text":"Parameters: Name Type Description Default epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"__init__()"},{"location":"api/Linear/","text":"treex.Linear A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x ) special Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ( axis_name = self . axis_name )} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , features_out , * , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>, name=None, axis_name=None) special Parameters: Name Type Description Default features_in the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/linear.py def __init__ ( self , features_out : int , * , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , name : tp . Optional [ str ] = None , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" super () . __init__ ( name = name ) self . features_out = features_out self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . axis_name = axis_name self . kernel = None self . bias = None","title":"Linear"},{"location":"api/Linear/#treexlinear","text":"A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.Linear"},{"location":"api/Linear/#treex.nn.linear.Linear.__call__","text":"Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ( axis_name = self . axis_name )} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/Linear/#treex.nn.linear.Linear.__init__","text":"Parameters: Name Type Description Default features_in the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/linear.py def __init__ ( self , features_out : int , * , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , name : tp . Optional [ str ] = None , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" super () . __init__ ( name = name ) self . features_out = features_out self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . axis_name = axis_name self . kernel = None self . bias = None","title":"__init__()"},{"location":"api/Log/","text":"treex.Log","title":"Log"},{"location":"api/Log/#treexlog","text":"","title":"treex.Log"},{"location":"api/Loss/","text":"treex.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in treex/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"Loss"},{"location":"api/Loss/#treexloss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"treex.Loss"},{"location":"api/Loss/#treex.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in treex/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"__init__()"},{"location":"api/LossAndLogs/","text":"treex.LossAndLogs","title":"LossAndLogs"},{"location":"api/LossAndLogs/#treexlossandlogs","text":"","title":"treex.LossAndLogs"},{"location":"api/LossLog/","text":"treex.LossLog","title":"LossLog"},{"location":"api/LossLog/#treexlosslog","text":"","title":"treex.LossLog"},{"location":"api/MISSING/","text":"treex.MISSING","title":"MISSING"},{"location":"api/MISSING/#treexmissing","text":"","title":"treex.MISSING"},{"location":"api/MLP/","text":"treex.MLP A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear. __call__ ( self , x ) special Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py @to . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" last_layer_idx = len ( self . features ) - 1 for i , features_out in enumerate ( self . features ): x = Linear ( features_out = features_out , use_bias = self . use_bias , dtype = self . dtype , precision = self . precision , kernel_init = self . kernel_init , bias_init = self . bias_init , )( x ) if i < last_layer_idx : x = self . activation ( x ) return x __init__ ( self , features , activation =< jax . _src . custom_derivatives . custom_jvp object at 0x7fec48f27040 > , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>) special Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7fec48f27040> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/mlp.py def __init__ ( self , features : tp . Sequence [ int ], activation : tp . Callable [[ jnp . ndarray ], jnp . ndarray ] = jax . nn . relu , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len ( features ) == 0 : raise ValueError ( \"features must have at least 1 element\" ) self . features = features self . activation = activation self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init","title":"MLP"},{"location":"api/MLP/#treexmlp","text":"A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear.","title":"treex.MLP"},{"location":"api/MLP/#treex.nn.mlp.MLP.__call__","text":"Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py @to . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" last_layer_idx = len ( self . features ) - 1 for i , features_out in enumerate ( self . features ): x = Linear ( features_out = features_out , use_bias = self . use_bias , dtype = self . dtype , precision = self . precision , kernel_init = self . kernel_init , bias_init = self . bias_init , )( x ) if i < last_layer_idx : x = self . activation ( x ) return x","title":"__call__()"},{"location":"api/MLP/#treex.nn.mlp.MLP.__init__","text":"Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7fec48f27040> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/mlp.py def __init__ ( self , features : tp . Sequence [ int ], activation : tp . Callable [[ jnp . ndarray ], jnp . ndarray ] = jax . nn . relu , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len ( features ) == 0 : raise ValueError ( \"features must have at least 1 element\" ) self . features = features self . activation = activation self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init","title":"__init__()"},{"location":"api/Map/","text":"treex.Map Mixin that adds a .map() method to the class. map ( self , f , * filters , * , inplace = False , flatten_mode = None , is_leaf = None ) map is a wrapper over treeo.map that passes self as the second argument. Parameters: Name Type Description Default f Callable The function to apply to the leaves. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] The filters used to select the leaves to which the function will be applied. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. None Returns: Type Description ~A A new pytree with the changes applied. If inplace is True , the input obj is returned. Source code in treeo/mixins.py def map ( self : A , f : tp . Callable , * filters : api . Filter , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , is_leaf : tp . Callable [[ tp . Any ], bool ] = None , ) -> A : \"\"\" `map` is a wrapper over `treeo.map` that passes `self` as the second argument. Arguments: f: The function to apply to the leaves. *filters: The filters used to select the leaves to which the function will be applied. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. Returns: A new pytree with the changes applied. If `inplace` is `True`, the input `obj` is returned. \"\"\" return api . map ( f , self , * filters , inplace = inplace , flatten_mode = flatten_mode , is_leaf = is_leaf , )","title":"Map"},{"location":"api/Map/#treexmap","text":"Mixin that adds a .map() method to the class.","title":"treex.Map"},{"location":"api/Map/#treeo.mixins.Map.map","text":"map is a wrapper over treeo.map that passes self as the second argument. Parameters: Name Type Description Default f Callable The function to apply to the leaves. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] The filters used to select the leaves to which the function will be applied. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. None Returns: Type Description ~A A new pytree with the changes applied. If inplace is True , the input obj is returned. Source code in treeo/mixins.py def map ( self : A , f : tp . Callable , * filters : api . Filter , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , is_leaf : tp . Callable [[ tp . Any ], bool ] = None , ) -> A : \"\"\" `map` is a wrapper over `treeo.map` that passes `self` as the second argument. Arguments: f: The function to apply to the leaves. *filters: The filters used to select the leaves to which the function will be applied. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. Returns: A new pytree with the changes applied. If `inplace` is `True`, the input `obj` is returned. \"\"\" return api . map ( f , self , * filters , inplace = inplace , flatten_mode = flatten_mode , is_leaf = is_leaf , )","title":"map()"},{"location":"api/Merge/","text":"treex.Merge Mixin that adds a .merge() method to the class. merge ( self , other , * rest , * , inplace = False , flatten_mode = None , ignore_static = False ) merge is a wrapper over treeo.merge that passes self as the first argument. Parameters: Name Type Description Default other ~A The pytree first to get the values to merge with. required *rest ~A Additional pytree to perform the merge in order from left to right. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. If the current flatten context is None and flatten_mode is not passed then FlattenMode.all_fields is used. None ignore_static bool If True , bypasses static fields during the process and the statics fields for output are taken from the first input ( obj ). False Returns: Type Description ~A A new pytree with the merged values. If inplace is True , obj is returned. Source code in treeo/mixins.py def merge ( self : A , other : A , * rest : A , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , ignore_static : bool = False , ) -> A : \"\"\" `merge` is a wrapper over `treeo.merge` that passes `self` as the first argument. Arguments: other: The pytree first to get the values to merge with. *rest: Additional pytree to perform the merge in order from left to right. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. If the current flatten context is `None` and `flatten_mode` is not passed then `FlattenMode.all_fields` is used. ignore_static: If `True`, bypasses static fields during the process and the statics fields for output are taken from the first input (`obj`). Returns: A new pytree with the merged values. If `inplace` is `True`, `obj` is returned. \"\"\" return api . merge ( self , other , * rest , inplace = inplace , flatten_mode = flatten_mode , ignore_static = ignore_static , )","title":"Merge"},{"location":"api/Merge/#treexmerge","text":"Mixin that adds a .merge() method to the class.","title":"treex.Merge"},{"location":"api/Merge/#treeo.mixins.Merge.merge","text":"merge is a wrapper over treeo.merge that passes self as the first argument. Parameters: Name Type Description Default other ~A The pytree first to get the values to merge with. required *rest ~A Additional pytree to perform the merge in order from left to right. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. If the current flatten context is None and flatten_mode is not passed then FlattenMode.all_fields is used. None ignore_static bool If True , bypasses static fields during the process and the statics fields for output are taken from the first input ( obj ). False Returns: Type Description ~A A new pytree with the merged values. If inplace is True , obj is returned. Source code in treeo/mixins.py def merge ( self : A , other : A , * rest : A , inplace : bool = False , flatten_mode : tp . Union [ api . FlattenMode , str , None ] = None , ignore_static : bool = False , ) -> A : \"\"\" `merge` is a wrapper over `treeo.merge` that passes `self` as the first argument. Arguments: other: The pytree first to get the values to merge with. *rest: Additional pytree to perform the merge in order from left to right. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. If the current flatten context is `None` and `flatten_mode` is not passed then `FlattenMode.all_fields` is used. ignore_static: If `True`, bypasses static fields during the process and the statics fields for output are taken from the first input (`obj`). Returns: A new pytree with the merged values. If `inplace` is `True`, `obj` is returned. \"\"\" return api . merge ( self , other , * rest , inplace = inplace , flatten_mode = flatten_mode , ignore_static = ignore_static , )","title":"merge()"},{"location":"api/Metric/","text":"treex.Metric Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. __init__ ( self , on = None , name = None , dtype = None ) special Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 __init_subclass__ () classmethod special This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/metrics/metric.py def __init_subclass__ ( cls ): super () . __init_subclass__ () # add call signature old_call = cls . __call__ @functools . wraps ( cls . update ) def new_call ( self : M , * args , ** kwargs ) -> M : if len ( args ) > 0 : raise TypeError ( f \"All arguments to { cls . __name__ } .__call__ should be passed as keyword arguments.\" ) return old_call ( self , * args , ** kwargs ) cls . __call__ = new_call","title":"Metric"},{"location":"api/Metric/#treexmetric","text":"Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point.","title":"treex.Metric"},{"location":"api/Metric/#treex.metrics.metric.Metric.__init__","text":"Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32","title":"__init__()"},{"location":"api/Metric/#treex.metrics.metric.Metric.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/metrics/metric.py def __init_subclass__ ( cls ): super () . __init_subclass__ () # add call signature old_call = cls . __call__ @functools . wraps ( cls . update ) def new_call ( self : M , * args , ** kwargs ) -> M : if len ( args ) > 0 : raise TypeError ( f \"All arguments to { cls . __name__ } .__call__ should be passed as keyword arguments.\" ) return old_call ( self , * args , ** kwargs ) cls . __call__ = new_call","title":"__init_subclass__()"},{"location":"api/MetricLog/","text":"treex.MetricLog","title":"MetricLog"},{"location":"api/MetricLog/#treexmetriclog","text":"","title":"treex.MetricLog"},{"location":"api/MetricState/","text":"treex.MetricState","title":"MetricState"},{"location":"api/MetricState/#treexmetricstate","text":"","title":"treex.MetricState"},{"location":"api/Missing/","text":"treex.Missing","title":"Missing"},{"location":"api/Missing/#treexmissing","text":"","title":"treex.Missing"},{"location":"api/ModelState/","text":"treex.ModelState","title":"ModelState"},{"location":"api/ModelState/#treexmodelstate","text":"","title":"treex.ModelState"},{"location":"api/Module/","text":"treex.Module __init_subclass__ () classmethod special This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/module.py def __init_subclass__ ( cls ): if issubclass ( cls , tp . Callable ): orig_call = cls . __call__ @functools . wraps ( cls . __call__ ) def new_call ( self : Module , * args , ** kwargs ): outputs = orig_call ( self , * args , ** kwargs ) if ( contexts . _CONTEXT . call_info is not None and self not in contexts . _CONTEXT . call_info ): inputs = types . Inputs ( * args , ** kwargs ) contexts . _CONTEXT . call_info [ self ] = ( inputs , outputs ) return outputs cls . __call__ = new_call return super () . __init_subclass__ () init ( self , key , inputs =< treeo . types . Missing object at 0x7fec40513be0 > , call_method = '__call__' , * , inplace = False , _set_initialize = True ) Method version of tx.init , it applies self as first argument. init creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. Module.rng_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~M The new module with the fields initialized. Source code in treex/module.py def init ( self : M , key : tp . Union [ int , jnp . ndarray ], inputs : types . InputLike = to . MISSING , call_method : str = \"__call__\" , * , inplace : bool = False , _set_initialize : bool = True , ) -> M : \"\"\" Method version of `tx.init`, it applies `self` as first argument. `init` creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `Module.rng_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" module = self . copy () if not inplace else self key = utils . Key ( key ) with _INIT_CONTEXT . update ( key = key , initializing = True ): module : M = module . map ( lambda initializer : ( initializer ( next_key ()) if isinstance ( initializer , types . Initializer ) else initializer ), is_leaf = lambda x : isinstance ( x , types . Initializer ), inplace = True , ) def call_rng_init ( module : Module ): if isinstance ( module , Module ) and not module . _initialized : module . rng_init () module = to . apply ( call_rng_init , module , inplace = True ) if inputs is not to . MISSING : inputs = types . Inputs . from_value ( inputs ) method = getattr ( module , call_method ) method ( * inputs . args , ** inputs . kwargs ) if _set_initialize : def set_initialized ( module : Module ): if isinstance ( module , Module ) and not module . _initialized : module . _initialized = True module = to . apply ( set_initialized , module , inplace = True ) return module tabulate ( self , inputs =< treeo . types . Missing object at 0x7fec40513be0 > , depth =- 1 , signature = False , param_types = True ) Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the Module. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/module.py def tabulate ( self , inputs : tp . Union [ types . InputLike , to . Missing ] = to . MISSING , depth : int = - 1 , signature : bool = False , param_types : bool = True , ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the Module. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" self = to . copy ( self ) if inputs is not to . MISSING : inputs = types . Inputs . from_value ( inputs ) if not isinstance ( self , tp . Callable ): raise TypeError ( \"`inputs` can only be specified if the module is a callable.\" ) with contexts . _Context ( call_info = {}): # call using self to preserve references def eval_call ( args , kwargs ): assert isinstance ( self , tp . Callable ) return self ( * args , ** kwargs ) jax . eval_shape ( eval_call , inputs . args , inputs . kwargs , ) call_info = contexts . _CONTEXT . call_info else : call_info = None with to . add_field_info (): flat : tp . List [ to . FieldInfo ] flat , _ = jax . tree_flatten ( self ) tree_part_types : tp . Tuple [ tp . Type [ types . TreePart ], ... ] = tuple ( { field_info . kind for field_info in flat if utils . _generic_issubclass ( field_info . kind , types . TreePart ) } ) path = () rows = list ( utils . _get_tabulate_rows ( path , self , depth , tree_part_types , signature , param_types ) ) modules = [ row [ 0 ] for row in rows ] rows = [ row [ 1 :] for row in rows ] if call_info is not None : for module , row in zip ( modules , rows ): if module in call_info : inputs , outputs = call_info [ module ] simplified_inputs = ( inputs . args [ 0 ] if len ( inputs . kwargs ) == 0 and len ( inputs . args ) == 1 else inputs . kwargs if len ( inputs . kwargs ) == 0 else inputs . kwargs if len ( inputs . args ) == 0 else ( inputs . args , inputs . kwargs ) ) inputs_repr = utils . _format_param_tree ( simplified_inputs ) outputs_repr = utils . _format_param_tree ( outputs ) else : inputs_repr = \"\" outputs_repr = \"\" row . insert ( 3 , outputs_repr ) row . insert ( 3 , inputs_repr ) n_non_treepart_cols = 2 if call_info is None else 4 rows [ 0 ][ 0 ] = \"*\" rows . append ( [ \"\" ] * n_non_treepart_cols + [ \"Total:\" ] + [ utils . _format_obj_size ( self . filter ( kind ), add_padding = True ) for kind in tree_part_types ] ) utils . _add_padding ( rows ) table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column ( \"path\" ) table . add_column ( \"module\" ) table . add_column ( \"params\" ) if call_info is not None : table . add_column ( \"inputs\" ) table . add_column ( \"outputs\" ) for tree_part_type in tree_part_types : type_name = tree_part_type . __name__ if type_name . startswith ( \"_\" ): type_name = type_name [ 1 :] table . add_column ( type_name ) for row in rows [: - 1 ]: table . add_row ( * row ) table . columns [ n_non_treepart_cols ] . footer = Text . from_markup ( rows [ - 1 ][ n_non_treepart_cols ], justify = \"right\" ) for i in range ( len ( tree_part_types )): table . columns [ n_non_treepart_cols + 1 + i ] . footer = rows [ - 1 ][ n_non_treepart_cols + 1 + i ] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + utils . _format_obj_size ( self , add_padding = False ) return utils . _get_rich_repr ( table )","title":"Module"},{"location":"api/Module/#treexmodule","text":"","title":"treex.Module"},{"location":"api/Module/#treex.module.Module.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/module.py def __init_subclass__ ( cls ): if issubclass ( cls , tp . Callable ): orig_call = cls . __call__ @functools . wraps ( cls . __call__ ) def new_call ( self : Module , * args , ** kwargs ): outputs = orig_call ( self , * args , ** kwargs ) if ( contexts . _CONTEXT . call_info is not None and self not in contexts . _CONTEXT . call_info ): inputs = types . Inputs ( * args , ** kwargs ) contexts . _CONTEXT . call_info [ self ] = ( inputs , outputs ) return outputs cls . __call__ = new_call return super () . __init_subclass__ ()","title":"__init_subclass__()"},{"location":"api/Module/#treex.module.Module.init","text":"Method version of tx.init , it applies self as first argument. init creates a new module with the same structure, but with its fields initialized given a seed key . The following procedure is used: The input key is split and iteratively updated before passing a derived value to any process that requires initialization. Initializer s are called and applied to the module first. Module.rng_init methods are called last. Parameters: Name Type Description Default key Union[int, jax._src.numpy.lax_numpy.ndarray] The seed to use for initialization. required Returns: Type Description ~M The new module with the fields initialized. Source code in treex/module.py def init ( self : M , key : tp . Union [ int , jnp . ndarray ], inputs : types . InputLike = to . MISSING , call_method : str = \"__call__\" , * , inplace : bool = False , _set_initialize : bool = True , ) -> M : \"\"\" Method version of `tx.init`, it applies `self` as first argument. `init` creates a new module with the same structure, but with its fields initialized given a seed `key`. The following procedure is used: 1. The input `key` is split and iteratively updated before passing a derived value to any process that requires initialization. 2. `Initializer`s are called and applied to the module first. 3. `Module.rng_init` methods are called last. Arguments: key: The seed to use for initialization. Returns: The new module with the fields initialized. \"\"\" module = self . copy () if not inplace else self key = utils . Key ( key ) with _INIT_CONTEXT . update ( key = key , initializing = True ): module : M = module . map ( lambda initializer : ( initializer ( next_key ()) if isinstance ( initializer , types . Initializer ) else initializer ), is_leaf = lambda x : isinstance ( x , types . Initializer ), inplace = True , ) def call_rng_init ( module : Module ): if isinstance ( module , Module ) and not module . _initialized : module . rng_init () module = to . apply ( call_rng_init , module , inplace = True ) if inputs is not to . MISSING : inputs = types . Inputs . from_value ( inputs ) method = getattr ( module , call_method ) method ( * inputs . args , ** inputs . kwargs ) if _set_initialize : def set_initialized ( module : Module ): if isinstance ( module , Module ) and not module . _initialized : module . _initialized = True module = to . apply ( set_initialized , module , inplace = True ) return module","title":"init()"},{"location":"api/Module/#treex.module.Module.tabulate","text":"Returns a tabular representation of the module. Parameters: Name Type Description Default depth int The maximum depth of the representation in terms of nested Modules, -1 means no limit. -1 signature bool Whether to show the signature of the Module. False param_types bool Whether to show the types of the parameters. True Returns: Type Description str A string containing the tabular representation. Source code in treex/module.py def tabulate ( self , inputs : tp . Union [ types . InputLike , to . Missing ] = to . MISSING , depth : int = - 1 , signature : bool = False , param_types : bool = True , ) -> str : \"\"\" Returns a tabular representation of the module. Arguments: depth: The maximum depth of the representation in terms of nested Modules, -1 means no limit. signature: Whether to show the signature of the Module. param_types: Whether to show the types of the parameters. Returns: A string containing the tabular representation. \"\"\" self = to . copy ( self ) if inputs is not to . MISSING : inputs = types . Inputs . from_value ( inputs ) if not isinstance ( self , tp . Callable ): raise TypeError ( \"`inputs` can only be specified if the module is a callable.\" ) with contexts . _Context ( call_info = {}): # call using self to preserve references def eval_call ( args , kwargs ): assert isinstance ( self , tp . Callable ) return self ( * args , ** kwargs ) jax . eval_shape ( eval_call , inputs . args , inputs . kwargs , ) call_info = contexts . _CONTEXT . call_info else : call_info = None with to . add_field_info (): flat : tp . List [ to . FieldInfo ] flat , _ = jax . tree_flatten ( self ) tree_part_types : tp . Tuple [ tp . Type [ types . TreePart ], ... ] = tuple ( { field_info . kind for field_info in flat if utils . _generic_issubclass ( field_info . kind , types . TreePart ) } ) path = () rows = list ( utils . _get_tabulate_rows ( path , self , depth , tree_part_types , signature , param_types ) ) modules = [ row [ 0 ] for row in rows ] rows = [ row [ 1 :] for row in rows ] if call_info is not None : for module , row in zip ( modules , rows ): if module in call_info : inputs , outputs = call_info [ module ] simplified_inputs = ( inputs . args [ 0 ] if len ( inputs . kwargs ) == 0 and len ( inputs . args ) == 1 else inputs . kwargs if len ( inputs . kwargs ) == 0 else inputs . kwargs if len ( inputs . args ) == 0 else ( inputs . args , inputs . kwargs ) ) inputs_repr = utils . _format_param_tree ( simplified_inputs ) outputs_repr = utils . _format_param_tree ( outputs ) else : inputs_repr = \"\" outputs_repr = \"\" row . insert ( 3 , outputs_repr ) row . insert ( 3 , inputs_repr ) n_non_treepart_cols = 2 if call_info is None else 4 rows [ 0 ][ 0 ] = \"*\" rows . append ( [ \"\" ] * n_non_treepart_cols + [ \"Total:\" ] + [ utils . _format_obj_size ( self . filter ( kind ), add_padding = True ) for kind in tree_part_types ] ) utils . _add_padding ( rows ) table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) table . add_column ( \"path\" ) table . add_column ( \"module\" ) table . add_column ( \"params\" ) if call_info is not None : table . add_column ( \"inputs\" ) table . add_column ( \"outputs\" ) for tree_part_type in tree_part_types : type_name = tree_part_type . __name__ if type_name . startswith ( \"_\" ): type_name = type_name [ 1 :] table . add_column ( type_name ) for row in rows [: - 1 ]: table . add_row ( * row ) table . columns [ n_non_treepart_cols ] . footer = Text . from_markup ( rows [ - 1 ][ n_non_treepart_cols ], justify = \"right\" ) for i in range ( len ( tree_part_types )): table . columns [ n_non_treepart_cols + 1 + i ] . footer = rows [ - 1 ][ n_non_treepart_cols + 1 + i ] table . caption_style = \"bold\" table . caption = \" \\n Total Parameters: \" + utils . _format_obj_size ( self , add_padding = False ) return utils . _get_rich_repr ( table )","title":"tabulate()"},{"location":"api/ModuleMeta/","text":"treex.ModuleMeta","title":"ModuleMeta"},{"location":"api/ModuleMeta/#treexmodulemeta","text":"","title":"treex.ModuleMeta"},{"location":"api/NOTHING/","text":"treex.NOTHING","title":"NOTHING"},{"location":"api/NOTHING/#treexnothing","text":"","title":"treex.NOTHING"},{"location":"api/Named/","text":"treex.Named Named( args, *kwds)","title":"Named"},{"location":"api/Named/#treexnamed","text":"Named( args, *kwds)","title":"treex.Named"},{"location":"api/Nothing/","text":"treex.Nothing","title":"Nothing"},{"location":"api/Nothing/#treexnothing","text":"","title":"treex.Nothing"},{"location":"api/Opaque/","text":"treex.Opaque","title":"Opaque"},{"location":"api/Opaque/#treexopaque","text":"","title":"treex.Opaque"},{"location":"api/OpaquePredicate/","text":"treex.OpaquePredicate","title":"OpaquePredicate"},{"location":"api/OpaquePredicate/#treexopaquepredicate","text":"","title":"treex.OpaquePredicate"},{"location":"api/OptState/","text":"treex.OptState","title":"OptState"},{"location":"api/OptState/#treexoptstate","text":"","title":"treex.OptState"},{"location":"api/Optimizer/","text":"treex.Optimizer Wraps an optax optimizer and turn it into a Pytree while maintaining a similar API. The main difference with optax is that tx.Optimizer contains its own state, thus, there is no opt_state . Examples: def main (): ... optimizer = tx . Optimizer ( optax . adam ( 1e-3 )) optimizer = optimizer . init ( params ) ... jax . jit def train_step ( model , x , y , optimizer ): ... params = optimizer . update ( grads , params ) ... return model , loss , optimizer Notice that since the optimizer is a Pytree it can naturally pass through jit . Differences with Optax init return a new optimizer instance, there is no opt_state . update doesn't get opt_state as an argument, instead it performs updates to its internal state inplace. update applies the updates to the params and returns them by default, use update=False to to get the param updates instead. Parameters: Name Type Description Default optimizer An optax optimizer. required init ( self , params ) Initialize the optimizer from an initial set of parameters. Parameters: Name Type Description Default params Any An initial set of parameters. required Returns: Type Description ~O A new optimizer instance. Source code in treex/optimizer.py def init ( self : O , params : tp . Any ) -> O : \"\"\" Initialize the optimizer from an initial set of parameters. Arguments: params: An initial set of parameters. Returns: A new optimizer instance. \"\"\" module = to . copy ( self ) params = jax . tree_leaves ( params ) module . opt_state = module . optimizer . init ( params ) module . _n_params = len ( params ) module . _initialized = True return module update ( self , grads , params = None , apply_updates = True ) Applies the parameters updates and updates the optimizers internal state inplace. Parameters: Name Type Description Default grads ~A the gradients to perform the update. required params Optional[~A] the parameters to update. If None then update has to be False . None apply_updates bool if False then the updates are returned instead of being applied. True Returns: Type Description ~A The updated parameters. If apply_updates is False then the updates are returned instead. Source code in treex/optimizer.py def update ( self , grads : A , params : tp . Optional [ A ] = None , apply_updates : bool = True ) -> A : \"\"\" Applies the parameters updates and updates the optimizers internal state inplace. Arguments: grads: the gradients to perform the update. params: the parameters to update. If `None` then `update` has to be `False`. apply_updates: if `False` then the updates are returned instead of being applied. Returns: The updated parameters. If `apply_updates` is `False` then the updates are returned instead. \"\"\" if not self . initialized : raise RuntimeError ( \"Optimizer is not initialized\" ) assert self . opt_state is not None if apply_updates and params is None : raise ValueError ( \"params must be provided if updates are being applied\" ) opt_grads , treedef = jax . tree_flatten ( grads ) opt_params = jax . tree_leaves ( params ) if len ( opt_params ) != self . _n_params : raise ValueError ( f \"params must have length { self . _n_params } , got { len ( opt_params ) } \" ) if len ( opt_grads ) != self . _n_params : raise ValueError ( f \"grads must have length { self . _n_params } , got { len ( opt_grads ) } \" ) param_updates : A param_updates , self . opt_state = self . optimizer . update ( opt_grads , self . opt_state , opt_params , ) output : A if apply_updates : output = optax . apply_updates ( opt_params , param_updates ) else : output = param_updates return jax . tree_unflatten ( treedef , output )","title":"Optimizer"},{"location":"api/Optimizer/#treexoptimizer","text":"Wraps an optax optimizer and turn it into a Pytree while maintaining a similar API. The main difference with optax is that tx.Optimizer contains its own state, thus, there is no opt_state . Examples: def main (): ... optimizer = tx . Optimizer ( optax . adam ( 1e-3 )) optimizer = optimizer . init ( params ) ... jax . jit def train_step ( model , x , y , optimizer ): ... params = optimizer . update ( grads , params ) ... return model , loss , optimizer Notice that since the optimizer is a Pytree it can naturally pass through jit .","title":"treex.Optimizer"},{"location":"api/Optimizer/#treex.optimizer.Optimizer--differences-with-optax","text":"init return a new optimizer instance, there is no opt_state . update doesn't get opt_state as an argument, instead it performs updates to its internal state inplace. update applies the updates to the params and returns them by default, use update=False to to get the param updates instead. Parameters: Name Type Description Default optimizer An optax optimizer. required","title":"Differences with Optax"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.init","text":"Initialize the optimizer from an initial set of parameters. Parameters: Name Type Description Default params Any An initial set of parameters. required Returns: Type Description ~O A new optimizer instance. Source code in treex/optimizer.py def init ( self : O , params : tp . Any ) -> O : \"\"\" Initialize the optimizer from an initial set of parameters. Arguments: params: An initial set of parameters. Returns: A new optimizer instance. \"\"\" module = to . copy ( self ) params = jax . tree_leaves ( params ) module . opt_state = module . optimizer . init ( params ) module . _n_params = len ( params ) module . _initialized = True return module","title":"init()"},{"location":"api/Optimizer/#treex.optimizer.Optimizer.update","text":"Applies the parameters updates and updates the optimizers internal state inplace. Parameters: Name Type Description Default grads ~A the gradients to perform the update. required params Optional[~A] the parameters to update. If None then update has to be False . None apply_updates bool if False then the updates are returned instead of being applied. True Returns: Type Description ~A The updated parameters. If apply_updates is False then the updates are returned instead. Source code in treex/optimizer.py def update ( self , grads : A , params : tp . Optional [ A ] = None , apply_updates : bool = True ) -> A : \"\"\" Applies the parameters updates and updates the optimizers internal state inplace. Arguments: grads: the gradients to perform the update. params: the parameters to update. If `None` then `update` has to be `False`. apply_updates: if `False` then the updates are returned instead of being applied. Returns: The updated parameters. If `apply_updates` is `False` then the updates are returned instead. \"\"\" if not self . initialized : raise RuntimeError ( \"Optimizer is not initialized\" ) assert self . opt_state is not None if apply_updates and params is None : raise ValueError ( \"params must be provided if updates are being applied\" ) opt_grads , treedef = jax . tree_flatten ( grads ) opt_params = jax . tree_leaves ( params ) if len ( opt_params ) != self . _n_params : raise ValueError ( f \"params must have length { self . _n_params } , got { len ( opt_params ) } \" ) if len ( opt_grads ) != self . _n_params : raise ValueError ( f \"grads must have length { self . _n_params } , got { len ( opt_grads ) } \" ) param_updates : A param_updates , self . opt_state = self . optimizer . update ( opt_grads , self . opt_state , opt_params , ) output : A if apply_updates : output = optax . apply_updates ( opt_params , param_updates ) else : output = param_updates return jax . tree_unflatten ( treedef , output )","title":"update()"},{"location":"api/Parameter/","text":"treex.Parameter","title":"Parameter"},{"location":"api/Parameter/#treexparameter","text":"","title":"treex.Parameter"},{"location":"api/Repr/","text":"treex.Repr Mixin that adds a __repr__ method to the class. __repr__ ( self ) special Uses treeo.to_string to generate a string representation of the object. Source code in treeo/mixins.py def __repr__ ( self ) -> tp . Any : \"\"\" Uses `treeo.to_string` to generate a string representation of the object. \"\"\" return api . to_string ( self , private_fields = False , static_fields = True , color = False , )","title":"Repr"},{"location":"api/Repr/#treexrepr","text":"Mixin that adds a __repr__ method to the class.","title":"treex.Repr"},{"location":"api/Repr/#treeo.mixins.Repr.__repr__","text":"Uses treeo.to_string to generate a string representation of the object. Source code in treeo/mixins.py def __repr__ ( self ) -> tp . Any : \"\"\" Uses `treeo.to_string` to generate a string representation of the object. \"\"\" return api . to_string ( self , private_fields = False , static_fields = True , color = False , )","title":"__repr__()"},{"location":"api/Rng/","text":"treex.Rng","title":"Rng"},{"location":"api/Rng/#treexrng","text":"","title":"treex.Rng"},{"location":"api/Sequential/","text":"treex.Sequential A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequential ( tx . Linear ( 2 , 32 ), jax . nn . relu , tx . Linear ( 32 , 8 ), jax . nn . relu , tx . Linear ( 8 , 4 ), ) . init ( 42 ) x = np . random . uniform ( size = ( 10 , 2 )) y = mlp ( x ) assert y . shape == ( 10 , 4 ) __init__ ( self , * layers ) special Parameters: Name Type Description Default *layers Union[Callable[..., jax._src.numpy.lax_numpy.ndarray], Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequential.py def __init__ ( self , * layers : tp . Union [ CallableModule , tp . Callable [[ jnp . ndarray ], jnp . ndarray ]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance ( layer , Module ) else Lambda ( layer ) for layer in layers ]","title":"Sequential"},{"location":"api/Sequential/#treexsequential","text":"A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequential ( tx . Linear ( 2 , 32 ), jax . nn . relu , tx . Linear ( 32 , 8 ), jax . nn . relu , tx . Linear ( 8 , 4 ), ) . init ( 42 ) x = np . random . uniform ( size = ( 10 , 2 )) y = mlp ( x ) assert y . shape == ( 10 , 4 )","title":"treex.Sequential"},{"location":"api/Sequential/#treex.nn.sequential.Sequential.__init__","text":"Parameters: Name Type Description Default *layers Union[Callable[..., jax._src.numpy.lax_numpy.ndarray], Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequential.py def __init__ ( self , * layers : tp . Union [ CallableModule , tp . Callable [[ jnp . ndarray ], jnp . ndarray ]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance ( layer , Module ) else Lambda ( layer ) for layer in layers ]","title":"__init__()"},{"location":"api/State/","text":"treex.State","title":"State"},{"location":"api/State/#treexstate","text":"","title":"treex.State"},{"location":"api/ToDict/","text":"treex.ToDict Mixin that adds a .to_dict() method to the class. to_dict ( self , * , private_fields = False , static_fields = True , type_info = False , field_info = False ) to_dict is a wrapper over treeo.to_dict that passes self as the first argument. Parameters: Name Type Description Default private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True type_info bool If True , type information is included. False field_info bool If True , field information is included. False Returns: Type Description Any A dict representation of the object. Source code in treeo/mixins.py def to_dict ( self , * , private_fields : bool = False , static_fields : bool = True , type_info : bool = False , field_info : bool = False , ) -> tp . Any : \"\"\" `to_dict` is a wrapper over `treeo.to_dict` that passes `self` as the first argument. Arguments: private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. type_info: If `True`, type information is included. field_info: If `True`, field information is included. Returns: A dict representation of the object. \"\"\" return api . to_dict ( self , private_fields = private_fields , static_fields = static_fields , type_info = type_info , field_info = field_info , )","title":"ToDict"},{"location":"api/ToDict/#treextodict","text":"Mixin that adds a .to_dict() method to the class.","title":"treex.ToDict"},{"location":"api/ToDict/#treeo.mixins.ToDict.to_dict","text":"to_dict is a wrapper over treeo.to_dict that passes self as the first argument. Parameters: Name Type Description Default private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True type_info bool If True , type information is included. False field_info bool If True , field information is included. False Returns: Type Description Any A dict representation of the object. Source code in treeo/mixins.py def to_dict ( self , * , private_fields : bool = False , static_fields : bool = True , type_info : bool = False , field_info : bool = False , ) -> tp . Any : \"\"\" `to_dict` is a wrapper over `treeo.to_dict` that passes `self` as the first argument. Arguments: private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. type_info: If `True`, type information is included. field_info: If `True`, field information is included. Returns: A dict representation of the object. \"\"\" return api . to_dict ( self , private_fields = private_fields , static_fields = static_fields , type_info = type_info , field_info = field_info , )","title":"to_dict()"},{"location":"api/ToString/","text":"treex.ToString Mixin that adds a .to_string() method to the class. to_string ( self , * , private_fields = False , static_fields = True , color = False ) to_string is a wrapper over treeo.to_string that passes self as the first argument. Parameters: Name Type Description Default private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True color bool If True , color is included. False Returns: Type Description str A string representation of the object. Source code in treeo/mixins.py def to_string ( self : A , * , private_fields : bool = False , static_fields : bool = True , color : bool = False , ) -> str : \"\"\" `to_string` is a wrapper over `treeo.to_string` that passes `self` as the first argument. Arguments: private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. color: If `True`, color is included. Returns: A string representation of the object. \"\"\" return api . to_string ( self , private_fields = private_fields , static_fields = static_fields , color = color , )","title":"ToString"},{"location":"api/ToString/#treextostring","text":"Mixin that adds a .to_string() method to the class.","title":"treex.ToString"},{"location":"api/ToString/#treeo.mixins.ToString.to_string","text":"to_string is a wrapper over treeo.to_string that passes self as the first argument. Parameters: Name Type Description Default private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True color bool If True , color is included. False Returns: Type Description str A string representation of the object. Source code in treeo/mixins.py def to_string ( self : A , * , private_fields : bool = False , static_fields : bool = True , color : bool = False , ) -> str : \"\"\" `to_string` is a wrapper over `treeo.to_string` that passes `self` as the first argument. Arguments: private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. color: If `True`, color is included. Returns: A string representation of the object. \"\"\" return api . to_string ( self , private_fields = private_fields , static_fields = static_fields , color = color , )","title":"to_string()"},{"location":"api/Tree/","text":"treex.Tree __init_subclass__ () classmethod special This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treeo/tree.py def __init_subclass__ ( cls ): jax . tree_util . register_pytree_node_class ( cls ) # Restore the signature sig = inspect . signature ( cls . __init__ ) parameters = tuple ( sig . parameters . values ()) cls . __signature__ = sig . replace ( parameters = parameters [ 1 :]) annotations = utils . _get_all_annotations ( cls ) class_vars = utils . _get_all_vars ( cls ) # init class variables cls . _field_metadata = {} cls . _factory_fields = {} cls . _default_field_values = {} cls . _subtrees = None for field , value in class_vars . items (): if isinstance ( value , dataclasses . Field ): # save defaults if value . default is not dataclasses . MISSING : cls . _default_field_values [ field ] = value . default elif value . default_factory is not dataclasses . MISSING : cls . _factory_fields [ field ] = value . default_factory # extract metadata if value . metadata is not None and \"node\" in value . metadata : cls . _field_metadata [ field ] = types . FieldMetadata ( node = value . metadata [ \"node\" ], kind = value . metadata [ \"kind\" ], opaque = value . metadata [ \"opaque\" ], ) for field , value in annotations . items (): if field not in cls . _field_metadata : is_node = any ( issubclass ( t , Tree ) for t in utils . _all_types ( value )) cls . _field_metadata [ field ] = types . FieldMetadata ( node = is_node , kind = type ( None ), opaque = False , ) check_metadata_updates ( self ) Checks for new fields, if found, adds them to the metadata. Source code in treeo/tree.py def check_metadata_updates ( self ): \"\"\" Checks for new fields, if found, adds them to the metadata. \"\"\" with _CONTEXT . update ( flatten_mode = FlattenMode . all_fields ): jax . tree_flatten ( self )","title":"Tree"},{"location":"api/Tree/#treextree","text":"","title":"treex.Tree"},{"location":"api/Tree/#treeo.tree.Tree.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treeo/tree.py def __init_subclass__ ( cls ): jax . tree_util . register_pytree_node_class ( cls ) # Restore the signature sig = inspect . signature ( cls . __init__ ) parameters = tuple ( sig . parameters . values ()) cls . __signature__ = sig . replace ( parameters = parameters [ 1 :]) annotations = utils . _get_all_annotations ( cls ) class_vars = utils . _get_all_vars ( cls ) # init class variables cls . _field_metadata = {} cls . _factory_fields = {} cls . _default_field_values = {} cls . _subtrees = None for field , value in class_vars . items (): if isinstance ( value , dataclasses . Field ): # save defaults if value . default is not dataclasses . MISSING : cls . _default_field_values [ field ] = value . default elif value . default_factory is not dataclasses . MISSING : cls . _factory_fields [ field ] = value . default_factory # extract metadata if value . metadata is not None and \"node\" in value . metadata : cls . _field_metadata [ field ] = types . FieldMetadata ( node = value . metadata [ \"node\" ], kind = value . metadata [ \"kind\" ], opaque = value . metadata [ \"opaque\" ], ) for field , value in annotations . items (): if field not in cls . _field_metadata : is_node = any ( issubclass ( t , Tree ) for t in utils . _all_types ( value )) cls . _field_metadata [ field ] = types . FieldMetadata ( node = is_node , kind = type ( None ), opaque = False , )","title":"__init_subclass__()"},{"location":"api/Tree/#treeo.tree.Tree.check_metadata_updates","text":"Checks for new fields, if found, adds them to the metadata. Source code in treeo/tree.py def check_metadata_updates ( self ): \"\"\" Checks for new fields, if found, adds them to the metadata. \"\"\" with _CONTEXT . update ( flatten_mode = FlattenMode . all_fields ): jax . tree_flatten ( self )","title":"check_metadata_updates()"},{"location":"api/TreeMeta/","text":"treex.TreeMeta","title":"TreeMeta"},{"location":"api/TreeMeta/#treextreemeta","text":"","title":"treex.TreeMeta"},{"location":"api/TreePart/","text":"treex.TreePart","title":"TreePart"},{"location":"api/TreePart/#treextreepart","text":"","title":"treex.TreePart"},{"location":"api/Treex/","text":"treex.Treex A Tree class with all Mixin Extensions. Base class for all Treex classes. eval ( self , inplace = False ) Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/treex.py def eval ( self : T , inplace : bool = False ) -> T : \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train ( False , inplace = inplace ) freeze ( self , mode = True , inplace = False ) Creates a new module with the same structure, but with Module.frozen set to the given value. Parameters: Name Type Description Default mode bool The new frozen mode. True inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module in with the frozen mode is set to the given value, if inplace is True then self is returned. Source code in treex/treex.py def freeze ( self : T , mode : bool = True , inplace : bool = False ) -> T : \"\"\" Creates a new module with the same structure, but with `Module.frozen` set to the given value. Arguments: mode: The new `frozen` mode. inplace: Whether to update the module inplace. Returns: The new module in with the `frozen` mode is set to the given value, if `inplace` is `True` then `self` is returned. \"\"\" def set_frozen ( tree ): if isinstance ( tree , Treex ) and hasattr ( tree , \"_frozen\" ): tree . _frozen = mode return to . apply ( set_frozen , self , inplace = inplace ) train ( self , mode = True , inplace = False ) Creates a new module with the same structure, but with Module.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module in with the training mode is set to the given value, if inplace is True then self is returned. Source code in treex/treex.py def train ( self : T , mode : bool = True , inplace : bool = False ) -> T : \"\"\" Creates a new module with the same structure, but with `Module.training` set to the given value. Arguments: mode: The new training mode. inplace: Whether to update the module inplace. Returns: The new module in with the training mode is set to the given value, if `inplace` is `True` then `self` is returned. \"\"\" def set_training ( tree ): if isinstance ( tree , Treex ) and hasattr ( tree , \"_training\" ): tree . _training = mode return to . apply ( set_training , self , inplace = inplace ) unfreeze ( self , inplace = False ) Creates a new module with .frozen set to False, equivalent to calling freeze(False) . Parameters: Name Type Description Default inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module with .frozen set to False, if inplace is True then self is returned. Source code in treex/treex.py def unfreeze ( self : T , inplace : bool = False ) -> T : \"\"\" Creates a new module with `.frozen` set to False, equivalent to calling `freeze(False)`. Arguments: inplace: Whether to update the module inplace. Returns: The new module with `.frozen` set to False, if `inplace` is `True` then `self` is returned. \"\"\" return self . freeze ( False , inplace = inplace )","title":"Treex"},{"location":"api/Treex/#treextreex","text":"A Tree class with all Mixin Extensions. Base class for all Treex classes.","title":"treex.Treex"},{"location":"api/Treex/#treex.treex.Treex.eval","text":"Creates a new module with the training mode set to False, equivalent to calling train(False) . Returns: Type Description ~T The new module with the training mode set to False. Source code in treex/treex.py def eval ( self : T , inplace : bool = False ) -> T : \"\"\" Creates a new module with the training mode set to False, equivalent to calling `train(False)`. Returns: The new module with the training mode set to False. \"\"\" return self . train ( False , inplace = inplace )","title":"eval()"},{"location":"api/Treex/#treex.treex.Treex.freeze","text":"Creates a new module with the same structure, but with Module.frozen set to the given value. Parameters: Name Type Description Default mode bool The new frozen mode. True inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module in with the frozen mode is set to the given value, if inplace is True then self is returned. Source code in treex/treex.py def freeze ( self : T , mode : bool = True , inplace : bool = False ) -> T : \"\"\" Creates a new module with the same structure, but with `Module.frozen` set to the given value. Arguments: mode: The new `frozen` mode. inplace: Whether to update the module inplace. Returns: The new module in with the `frozen` mode is set to the given value, if `inplace` is `True` then `self` is returned. \"\"\" def set_frozen ( tree ): if isinstance ( tree , Treex ) and hasattr ( tree , \"_frozen\" ): tree . _frozen = mode return to . apply ( set_frozen , self , inplace = inplace )","title":"freeze()"},{"location":"api/Treex/#treex.treex.Treex.train","text":"Creates a new module with the same structure, but with Module.training set to the given value. Parameters: Name Type Description Default mode bool The new training mode. True inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module in with the training mode is set to the given value, if inplace is True then self is returned. Source code in treex/treex.py def train ( self : T , mode : bool = True , inplace : bool = False ) -> T : \"\"\" Creates a new module with the same structure, but with `Module.training` set to the given value. Arguments: mode: The new training mode. inplace: Whether to update the module inplace. Returns: The new module in with the training mode is set to the given value, if `inplace` is `True` then `self` is returned. \"\"\" def set_training ( tree ): if isinstance ( tree , Treex ) and hasattr ( tree , \"_training\" ): tree . _training = mode return to . apply ( set_training , self , inplace = inplace )","title":"train()"},{"location":"api/Treex/#treex.treex.Treex.unfreeze","text":"Creates a new module with .frozen set to False, equivalent to calling freeze(False) . Parameters: Name Type Description Default inplace bool Whether to update the module inplace. False Returns: Type Description ~T The new module with .frozen set to False, if inplace is True then self is returned. Source code in treex/treex.py def unfreeze ( self : T , inplace : bool = False ) -> T : \"\"\" Creates a new module with `.frozen` set to False, equivalent to calling `freeze(False)`. Arguments: inplace: Whether to update the module inplace. Returns: The new module with `.frozen` set to False, if `inplace` is `True` then `self` is returned. \"\"\" return self . freeze ( False , inplace = inplace )","title":"unfreeze()"},{"location":"api/add_field_info/","text":"treex.add_field_info A context manager that makes Tree s produce leaves as FieldInfo when flattening. Source code in treeo/api.py @contextmanager def add_field_info (): \"\"\" A context manager that makes `Tree`s produce leaves as `FieldInfo` when flattening. \"\"\" with tree_m . _CONTEXT . update ( add_field_info = True ): yield","title":"add_field_info"},{"location":"api/add_field_info/#treexadd_field_info","text":"A context manager that makes Tree s produce leaves as FieldInfo when flattening. Source code in treeo/api.py @contextmanager def add_field_info (): \"\"\" A context manager that makes `Tree`s produce leaves as `FieldInfo` when flattening. \"\"\" with tree_m . _CONTEXT . update ( add_field_info = True ): yield","title":"treex.add_field_info"},{"location":"api/apply/","text":"treex.apply Applies a function to all to.Tree s in a Pytree. Works very similar to jax.tree_map , but its values are to.Tree s instead of leaves, also f should apply the changes inplace to Tree object. If inplace is False , a copy of the first object is returned with the changes applied. The rest of the objects are always copied. Parameters: Name Type Description Default f Callable[..., NoneType] The function to apply. required obj ~A a pytree possibly containing Trees. required *rest ~A additional pytrees. () inplace bool If True , the input obj is mutated. False Returns: Type Description ~A A new pytree with the updated Trees or the same input obj if inplace is True . Source code in treeo/api.py def apply ( f : tp . Callable [ ... , None ], obj : A , * rest : A , inplace : bool = False ) -> A : \"\"\" Applies a function to all `to.Tree`s in a Pytree. Works very similar to `jax.tree_map`, but its values are `to.Tree`s instead of leaves, also `f` should apply the changes inplace to Tree object. If `inplace` is `False`, a copy of the first object is returned with the changes applied. The `rest` of the objects are always copied. Arguments: f: The function to apply. obj: a pytree possibly containing Trees. *rest: additional pytrees. inplace: If `True`, the input `obj` is mutated. Returns: A new pytree with the updated Trees or the same input `obj` if `inplace` is `True`. \"\"\" rest = tree_m . copy ( rest ) if not inplace : obj = tree_m . copy ( obj ) objs = ( obj ,) + rest def nested_fn ( obj , * rest ): if isinstance ( obj , Tree ): apply ( f , obj , * rest , inplace = True ) jax . tree_map ( nested_fn , * objs , is_leaf = lambda x : isinstance ( x , Tree ) and not x in objs , ) if isinstance ( obj , Tree ): f ( obj , * rest ) return obj","title":"apply"},{"location":"api/apply/#treexapply","text":"Applies a function to all to.Tree s in a Pytree. Works very similar to jax.tree_map , but its values are to.Tree s instead of leaves, also f should apply the changes inplace to Tree object. If inplace is False , a copy of the first object is returned with the changes applied. The rest of the objects are always copied. Parameters: Name Type Description Default f Callable[..., NoneType] The function to apply. required obj ~A a pytree possibly containing Trees. required *rest ~A additional pytrees. () inplace bool If True , the input obj is mutated. False Returns: Type Description ~A A new pytree with the updated Trees or the same input obj if inplace is True . Source code in treeo/api.py def apply ( f : tp . Callable [ ... , None ], obj : A , * rest : A , inplace : bool = False ) -> A : \"\"\" Applies a function to all `to.Tree`s in a Pytree. Works very similar to `jax.tree_map`, but its values are `to.Tree`s instead of leaves, also `f` should apply the changes inplace to Tree object. If `inplace` is `False`, a copy of the first object is returned with the changes applied. The `rest` of the objects are always copied. Arguments: f: The function to apply. obj: a pytree possibly containing Trees. *rest: additional pytrees. inplace: If `True`, the input `obj` is mutated. Returns: A new pytree with the updated Trees or the same input `obj` if `inplace` is `True`. \"\"\" rest = tree_m . copy ( rest ) if not inplace : obj = tree_m . copy ( obj ) objs = ( obj ,) + rest def nested_fn ( obj , * rest ): if isinstance ( obj , Tree ): apply ( f , obj , * rest , inplace = True ) jax . tree_map ( nested_fn , * objs , is_leaf = lambda x : isinstance ( x , Tree ) and not x in objs , ) if isinstance ( obj , Tree ): f ( obj , * rest ) return obj","title":"treex.apply"},{"location":"api/compact/","text":"treex.compact A decorator that enable the definition of Tree subnodes at runtime. Source code in treeo/api.py def compact ( f ): \"\"\" A decorator that enable the definition of Tree subnodes at runtime. \"\"\" @functools . wraps ( f ) def wrapper ( tree , * args , ** kwargs ): with tree_m . _COMPACT_CONTEXT . compact ( f , tree ): return f ( tree , * args , ** kwargs ) wrapper . _treeo_compact = True return wrapper","title":"compact"},{"location":"api/compact/#treexcompact","text":"A decorator that enable the definition of Tree subnodes at runtime. Source code in treeo/api.py def compact ( f ): \"\"\" A decorator that enable the definition of Tree subnodes at runtime. \"\"\" @functools . wraps ( f ) def wrapper ( tree , * args , ** kwargs ): with tree_m . _COMPACT_CONTEXT . compact ( f , tree ): return f ( tree , * args , ** kwargs ) wrapper . _treeo_compact = True return wrapper","title":"treex.compact"},{"location":"api/compact_module/","text":"treex.compact_module A decorator that enable the definition of functional Modules Source code in treex/module.py def compact_module ( f ) -> type : \"\"\" A decorator that enable the definition of functional Modules \"\"\" name = utils . _get_name ( f ) @functools . wraps ( f ) @to . compact def __call__ ( self , * args , ** kwargs ): return f ( * args , ** kwargs ) module_class = type ( name , ( Module ,), dict ( __call__ = __call__ , ), ) return module_class","title":"compact_module"},{"location":"api/compact_module/#treexcompact_module","text":"A decorator that enable the definition of functional Modules Source code in treex/module.py def compact_module ( f ) -> type : \"\"\" A decorator that enable the definition of functional Modules \"\"\" name = utils . _get_name ( f ) @functools . wraps ( f ) @to . compact def __call__ ( self , * args , ** kwargs ): return f ( * args , ** kwargs ) module_class = type ( name , ( Module ,), dict ( __call__ = __call__ , ), ) return module_class","title":"treex.compact_module"},{"location":"api/copy/","text":"treex.copy Returns a deep copy of the tree, almost equivalent to: jax . tree_map ( lambda x : x , self ) but will try to copy static nodes as well. Source code in treeo/tree.py def copy ( obj : A ) -> A : \"\"\" Returns a deep copy of the tree, almost equivalent to: ```python jax.tree_map(lambda x: x, self) ``` but will try to copy static nodes as well. \"\"\" with _CONTEXT . update ( flatten_mode = FlattenMode . all_fields ): return jax . tree_map ( lambda x : x , obj )","title":"copy"},{"location":"api/copy/#treexcopy","text":"Returns a deep copy of the tree, almost equivalent to: jax . tree_map ( lambda x : x , self ) but will try to copy static nodes as well. Source code in treeo/tree.py def copy ( obj : A ) -> A : \"\"\" Returns a deep copy of the tree, almost equivalent to: ```python jax.tree_map(lambda x: x, self) ``` but will try to copy static nodes as well. \"\"\" with _CONTEXT . update ( flatten_mode = FlattenMode . all_fields ): return jax . tree_map ( lambda x : x , obj )","title":"treex.copy"},{"location":"api/field/","text":"treex.field Source code in treeo/utils.py def field ( default : tp . Any = dataclasses . MISSING , * , node : bool , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return dataclasses . field ( default = default , metadata = { \"node\" : node , \"kind\" : kind , \"opaque\" : opaque , }, default_factory = default_factory if default_factory is not None else dataclasses . MISSING , init = init , repr = repr , hash = hash , compare = compare , )","title":"field"},{"location":"api/field/#treexfield","text":"Source code in treeo/utils.py def field ( default : tp . Any = dataclasses . MISSING , * , node : bool , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return dataclasses . field ( default = default , metadata = { \"node\" : node , \"kind\" : kind , \"opaque\" : opaque , }, default_factory = default_factory if default_factory is not None else dataclasses . MISSING , init = init , repr = repr , hash = hash , compare = compare , )","title":"treex.field"},{"location":"api/filter/","text":"treex.filter The filter function allows you to select a subtree by filtering based on a predicate or kind type, leaves that pass all filters are kept, the rest are set to Nothing . For more information see filter's user guide . Parameters: Name Type Description Default obj ~A A pytree (possibly containing to.Tree s) to be filtered. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] Types to filter by, membership is determined by issubclass , or callables that take in a FieldInfo and return a bool . () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation. None Returns: Type Description ~A A new pytree with the filtered fields. If inplace is True , obj is returned. Source code in treeo/api.py def filter ( obj : A , * filters : Filter , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , ) -> A : \"\"\" The `filter` function allows you to select a subtree by filtering based on a predicate or `kind` type, leaves that pass all filters are kept, the rest are set to `Nothing`. For more information see [filter's user guide](https://cgarciae.github.io/treeo/user-guide/api/filter). Arguments: obj: A pytree (possibly containing `to.Tree`s) to be filtered. *filters: Types to filter by, membership is determined by `issubclass`, or callables that take in a `FieldInfo` and return a `bool`. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation. Returns: A new pytree with the filtered fields. If `inplace` is `True`, `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise ValueError ( f \"Cannot filter inplace on objects with no __dict__ property, got { obj } \" ) input_obj = obj filters = tuple ( _get_kind_filter ( f ) if isinstance ( f , tp . Type ) else f for f in filters ) def apply_filters ( info : tp . Any ) -> tp . Any : if not isinstance ( info , FieldInfo ): info = FieldInfo ( name = None , value = info , kind = type ( None ), module = None , ) assert isinstance ( info , FieldInfo ) return info . value if all ( f ( info ) for f in filters ) else types . NOTHING with tree_m . _CONTEXT . update ( add_field_info = True ), _flatten_context ( flatten_mode ): obj = jax . tree_map ( apply_filters , obj ) if inplace : input_obj . __dict__ . update ( obj . __dict__ ) return input_obj else : return obj","title":"filter"},{"location":"api/filter/#treexfilter","text":"The filter function allows you to select a subtree by filtering based on a predicate or kind type, leaves that pass all filters are kept, the rest are set to Nothing . For more information see filter's user guide . Parameters: Name Type Description Default obj ~A A pytree (possibly containing to.Tree s) to be filtered. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] Types to filter by, membership is determined by issubclass , or callables that take in a FieldInfo and return a bool . () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation. None Returns: Type Description ~A A new pytree with the filtered fields. If inplace is True , obj is returned. Source code in treeo/api.py def filter ( obj : A , * filters : Filter , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , ) -> A : \"\"\" The `filter` function allows you to select a subtree by filtering based on a predicate or `kind` type, leaves that pass all filters are kept, the rest are set to `Nothing`. For more information see [filter's user guide](https://cgarciae.github.io/treeo/user-guide/api/filter). Arguments: obj: A pytree (possibly containing `to.Tree`s) to be filtered. *filters: Types to filter by, membership is determined by `issubclass`, or callables that take in a `FieldInfo` and return a `bool`. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation. Returns: A new pytree with the filtered fields. If `inplace` is `True`, `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise ValueError ( f \"Cannot filter inplace on objects with no __dict__ property, got { obj } \" ) input_obj = obj filters = tuple ( _get_kind_filter ( f ) if isinstance ( f , tp . Type ) else f for f in filters ) def apply_filters ( info : tp . Any ) -> tp . Any : if not isinstance ( info , FieldInfo ): info = FieldInfo ( name = None , value = info , kind = type ( None ), module = None , ) assert isinstance ( info , FieldInfo ) return info . value if all ( f ( info ) for f in filters ) else types . NOTHING with tree_m . _CONTEXT . update ( add_field_info = True ), _flatten_context ( flatten_mode ): obj = jax . tree_map ( apply_filters , obj ) if inplace : input_obj . __dict__ . update ( obj . __dict__ ) return input_obj else : return obj","title":"treex.filter"},{"location":"api/flatten_mode/","text":"treex.flatten_mode A context manager that defines how Tree s are flattened. Options are: 'normal' : Fields are selected as nodes as declared in the class definition (default behavior). 'all_fields' : All fields are treated as nodes during flattening. 'no_fields' : All fields are treated as static, Tree s produce no leaves. None : Context is not changed, current flatten mode is preserved. Examples: @dataclass class MyTree ( Tree ): x : int # static y : int = to . node () tree = MyTree ( x = 1 , y = 3 ) jax . tree_map ( lambda x : x * 2 , tree ) # MyTree(x=1, y=6) with flatten_mode ( 'all_fields' ): jax . tree_map ( lambda x : x + 1 , tree ) # MyTree(x=2, y=6) Parameters: Name Type Description Default mode Union[treeo.tree.FlattenMode, str] The new flatten mode. required Source code in treeo/api.py @contextmanager def flatten_mode ( mode : tp . Optional [ tp . Union [ FlattenMode , str ]]): \"\"\" A context manager that defines how `Tree`s are flattened. Options are: * `'normal'`: Fields are selected as nodes as declared in the class definition (default behavior). * `'all_fields'`: All fields are treated as nodes during flattening. * `'no_fields'`: All fields are treated as static, `Tree`s produce no leaves. * `None`: Context is not changed, current flatten mode is preserved. Example: ```python @dataclass class MyTree(Tree): x: int # static y: int = to.node() tree = MyTree(x=1, y=3) jax.tree_map(lambda x: x * 2, tree) # MyTree(x=1, y=6) with flatten_mode('all_fields'): jax.tree_map(lambda x: x + 1, tree) # MyTree(x=2, y=6) ``` Arguments: mode: The new flatten mode. \"\"\" if mode is not None : if isinstance ( mode , str ): mode = FlattenMode ( mode ) with tree_m . _CONTEXT . update ( flatten_mode = mode ): yield else : yield","title":"flatten_mode"},{"location":"api/flatten_mode/#treexflatten_mode","text":"A context manager that defines how Tree s are flattened. Options are: 'normal' : Fields are selected as nodes as declared in the class definition (default behavior). 'all_fields' : All fields are treated as nodes during flattening. 'no_fields' : All fields are treated as static, Tree s produce no leaves. None : Context is not changed, current flatten mode is preserved. Examples: @dataclass class MyTree ( Tree ): x : int # static y : int = to . node () tree = MyTree ( x = 1 , y = 3 ) jax . tree_map ( lambda x : x * 2 , tree ) # MyTree(x=1, y=6) with flatten_mode ( 'all_fields' ): jax . tree_map ( lambda x : x + 1 , tree ) # MyTree(x=2, y=6) Parameters: Name Type Description Default mode Union[treeo.tree.FlattenMode, str] The new flatten mode. required Source code in treeo/api.py @contextmanager def flatten_mode ( mode : tp . Optional [ tp . Union [ FlattenMode , str ]]): \"\"\" A context manager that defines how `Tree`s are flattened. Options are: * `'normal'`: Fields are selected as nodes as declared in the class definition (default behavior). * `'all_fields'`: All fields are treated as nodes during flattening. * `'no_fields'`: All fields are treated as static, `Tree`s produce no leaves. * `None`: Context is not changed, current flatten mode is preserved. Example: ```python @dataclass class MyTree(Tree): x: int # static y: int = to.node() tree = MyTree(x=1, y=3) jax.tree_map(lambda x: x * 2, tree) # MyTree(x=1, y=6) with flatten_mode('all_fields'): jax.tree_map(lambda x: x + 1, tree) # MyTree(x=2, y=6) ``` Arguments: mode: The new flatten mode. \"\"\" if mode is not None : if isinstance ( mode , str ): mode = FlattenMode ( mode ) with tree_m . _CONTEXT . update ( flatten_mode = mode ): yield else : yield","title":"treex.flatten_mode"},{"location":"api/in_compact/","text":"treex.in_compact Returns: Type Description bool True if current inside a function decorated with @compact . Source code in treeo/api.py def in_compact () -> bool : \"\"\" Returns: `True` if current inside a function decorated with `@compact`. \"\"\" return tree_m . _COMPACT_CONTEXT . in_compact","title":"in_compact"},{"location":"api/in_compact/#treexin_compact","text":"Returns: Type Description bool True if current inside a function decorated with @compact . Source code in treeo/api.py def in_compact () -> bool : \"\"\" Returns: `True` if current inside a function decorated with `@compact`. \"\"\" return tree_m . _COMPACT_CONTEXT . in_compact","title":"treex.in_compact"},{"location":"api/map/","text":"treex.map Applies a function to all leaves in a pytree using jax.tree_map , if filters are given then the function will be applied only to the subset of leaves that match the filters. For more information see map's user guide . Parameters: Name Type Description Default f Callable The function to apply to the leaves. required obj ~A a pytree possibly containing to.Tree s. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] The filters used to select the leaves to which the function will be applied. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. None add_field_info Represent the leaves of the tree by a FieldInfo type. This enables values of the field such as required Returns: Type Description ~A A new pytree with the changes applied. If inplace is True , the input obj is returned. Source code in treeo/api.py def map ( f : tp . Callable , obj : A , * filters : Filter , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , is_leaf : tp . Callable [[ tp . Any ], bool ] = None , field_info : tp . Optional [ bool ] = False , ) -> A : \"\"\" Applies a function to all leaves in a pytree using `jax.tree_map`, if `filters` are given then the function will be applied only to the subset of leaves that match the filters. For more information see [map's user guide](https://cgarciae.github.io/treeo/user-guide/api/map). Arguments: f: The function to apply to the leaves. obj: a pytree possibly containing `to.Tree`s. *filters: The filters used to select the leaves to which the function will be applied. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. add_field_info: Represent the leaves of the tree by a `FieldInfo` type. This enables values of the field such as kind and value to be used within the `map` function. Returns: A new pytree with the changes applied. If `inplace` is `True`, the input `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise ValueError ( f \"Cannot map inplace on objects with no __dict__ property, got { obj } \" ) input_obj = obj has_filters = len ( filters ) > 0 with _flatten_context ( flatten_mode ): if has_filters : new_obj = filter ( obj , * filters ) else : new_obj = obj # Conditionally build map function with, or without, the leaf nodes' field info. if field_info : with add_field_info (): new_obj : A = jax . tree_map ( f , new_obj , is_leaf = is_leaf ) else : new_obj : A = jax . tree_map ( f , new_obj , is_leaf = is_leaf ) if has_filters : new_obj = merge ( obj , new_obj ) if inplace : input_obj . __dict__ . update ( new_obj . __dict__ ) return input_obj else : return new_obj","title":"map"},{"location":"api/map/#treexmap","text":"Applies a function to all leaves in a pytree using jax.tree_map , if filters are given then the function will be applied only to the subset of leaves that match the filters. For more information see map's user guide . Parameters: Name Type Description Default f Callable The function to apply to the leaves. required obj ~A a pytree possibly containing to.Tree s. required *filters Union[Type[Any], Callable[[FieldInfo], bool]] The filters used to select the leaves to which the function will be applied. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. None add_field_info Represent the leaves of the tree by a FieldInfo type. This enables values of the field such as required Returns: Type Description ~A A new pytree with the changes applied. If inplace is True , the input obj is returned. Source code in treeo/api.py def map ( f : tp . Callable , obj : A , * filters : Filter , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , is_leaf : tp . Callable [[ tp . Any ], bool ] = None , field_info : tp . Optional [ bool ] = False , ) -> A : \"\"\" Applies a function to all leaves in a pytree using `jax.tree_map`, if `filters` are given then the function will be applied only to the subset of leaves that match the filters. For more information see [map's user guide](https://cgarciae.github.io/treeo/user-guide/api/map). Arguments: f: The function to apply to the leaves. obj: a pytree possibly containing `to.Tree`s. *filters: The filters used to select the leaves to which the function will be applied. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. add_field_info: Represent the leaves of the tree by a `FieldInfo` type. This enables values of the field such as kind and value to be used within the `map` function. Returns: A new pytree with the changes applied. If `inplace` is `True`, the input `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise ValueError ( f \"Cannot map inplace on objects with no __dict__ property, got { obj } \" ) input_obj = obj has_filters = len ( filters ) > 0 with _flatten_context ( flatten_mode ): if has_filters : new_obj = filter ( obj , * filters ) else : new_obj = obj # Conditionally build map function with, or without, the leaf nodes' field info. if field_info : with add_field_info (): new_obj : A = jax . tree_map ( f , new_obj , is_leaf = is_leaf ) else : new_obj : A = jax . tree_map ( f , new_obj , is_leaf = is_leaf ) if has_filters : new_obj = merge ( obj , new_obj ) if inplace : input_obj . __dict__ . update ( new_obj . __dict__ ) return input_obj else : return new_obj","title":"treex.map"},{"location":"api/merge/","text":"treex.merge Creates a new Tree with the same structure but its values merged based on the values from the incoming Trees. For more information see merge's user guide . Parameters: Name Type Description Default obj ~A Main pytree to merge. required other ~A The pytree first to get the values to merge with. required *rest ~A Additional pytree to perform the merge in order from left to right. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. If the current flatten context is None and flatten_mode is not passed then FlattenMode.all_fields is used. None ignore_static bool If True , bypasses static fields during the process and the statics fields for output are taken from the first input ( obj ). False Returns: Type Description ~A A new pytree with the updated values. If inplace is True , obj is returned. Source code in treeo/api.py def merge ( obj : A , other : A , * rest : A , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , ignore_static : bool = False , ) -> A : \"\"\" Creates a new Tree with the same structure but its values merged based on the values from the incoming Trees. For more information see [merge's user guide](https://cgarciae.github.io/treeo/user-guide/api/merge). Arguments: obj: Main pytree to merge. other: The pytree first to get the values to merge with. *rest: Additional pytree to perform the merge in order from left to right. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. If the current flatten context is `None` and `flatten_mode` is not passed then `FlattenMode.all_fields` is used. ignore_static: If `True`, bypasses static fields during the process and the statics fields for output are taken from the first input (`obj`). Returns: A new pytree with the updated values. If `inplace` is `True`, `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise TypeError ( f \"Cannot update inplace on objects with no __dict__ property, got { obj } \" ) if flatten_mode is None and tree_m . _CONTEXT . flatten_mode is None : flatten_mode = FlattenMode . all_fields input_obj = obj def merge_fn ( * xs ): for x in reversed ( xs ): if not isinstance ( x , types . Nothing ): return x return types . NOTHING tree_map_fn = _looser_tree_map if ignore_static else jax . tree_map with _flatten_context ( flatten_mode ): obj = tree_map_fn ( merge_fn , obj , other , * rest , is_leaf = lambda x : isinstance ( x , LEAF_TYPES ), ) if inplace : input_obj . __dict__ . update ( obj . __dict__ ) return input_obj else : return obj","title":"merge"},{"location":"api/merge/#treexmerge","text":"Creates a new Tree with the same structure but its values merged based on the values from the incoming Trees. For more information see merge's user guide . Parameters: Name Type Description Default obj ~A Main pytree to merge. required other ~A The pytree first to get the values to merge with. required *rest ~A Additional pytree to perform the merge in order from left to right. () inplace bool If True , the input obj is mutated and returned. False flatten_mode Union[treeo.tree.FlattenMode, str] Sets a new FlattenMode context for the operation, if None the current context is used. If the current flatten context is None and flatten_mode is not passed then FlattenMode.all_fields is used. None ignore_static bool If True , bypasses static fields during the process and the statics fields for output are taken from the first input ( obj ). False Returns: Type Description ~A A new pytree with the updated values. If inplace is True , obj is returned. Source code in treeo/api.py def merge ( obj : A , other : A , * rest : A , inplace : bool = False , flatten_mode : tp . Union [ FlattenMode , str , None ] = None , ignore_static : bool = False , ) -> A : \"\"\" Creates a new Tree with the same structure but its values merged based on the values from the incoming Trees. For more information see [merge's user guide](https://cgarciae.github.io/treeo/user-guide/api/merge). Arguments: obj: Main pytree to merge. other: The pytree first to get the values to merge with. *rest: Additional pytree to perform the merge in order from left to right. inplace: If `True`, the input `obj` is mutated and returned. flatten_mode: Sets a new `FlattenMode` context for the operation, if `None` the current context is used. If the current flatten context is `None` and `flatten_mode` is not passed then `FlattenMode.all_fields` is used. ignore_static: If `True`, bypasses static fields during the process and the statics fields for output are taken from the first input (`obj`). Returns: A new pytree with the updated values. If `inplace` is `True`, `obj` is returned. \"\"\" if inplace and not hasattr ( obj , \"__dict__\" ): raise TypeError ( f \"Cannot update inplace on objects with no __dict__ property, got { obj } \" ) if flatten_mode is None and tree_m . _CONTEXT . flatten_mode is None : flatten_mode = FlattenMode . all_fields input_obj = obj def merge_fn ( * xs ): for x in reversed ( xs ): if not isinstance ( x , types . Nothing ): return x return types . NOTHING tree_map_fn = _looser_tree_map if ignore_static else jax . tree_map with _flatten_context ( flatten_mode ): obj = tree_map_fn ( merge_fn , obj , other , * rest , is_leaf = lambda x : isinstance ( x , LEAF_TYPES ), ) if inplace : input_obj . __dict__ . update ( obj . __dict__ ) return input_obj else : return obj","title":"treex.merge"},{"location":"api/next_key/","text":"treex.next_key Returns the next key. Returns: Type Description ndarray The next key. Source code in treex/module.py def next_key ( * , axis_name : tp . Optional [ tp . Any ] = None ) -> jnp . ndarray : \"\"\" Returns the next key. Returns: The next key. \"\"\" key : jnp . ndarray if _INIT_CONTEXT . key is None : raise RuntimeError ( \"RNG key not set, you are either calling an uninitialized Module outside `.init` or forgot to call `rng_key` context manager.\" ) key , _INIT_CONTEXT . key = utils . iter_split ( _INIT_CONTEXT . key ) if axis_name is not None : axis_index = jax . lax . axis_index ( axis_name ) key = jax . random . fold_in ( key , axis_index ) return key","title":"next_key"},{"location":"api/next_key/#treexnext_key","text":"Returns the next key. Returns: Type Description ndarray The next key. Source code in treex/module.py def next_key ( * , axis_name : tp . Optional [ tp . Any ] = None ) -> jnp . ndarray : \"\"\" Returns the next key. Returns: The next key. \"\"\" key : jnp . ndarray if _INIT_CONTEXT . key is None : raise RuntimeError ( \"RNG key not set, you are either calling an uninitialized Module outside `.init` or forgot to call `rng_key` context manager.\" ) key , _INIT_CONTEXT . key = utils . iter_split ( _INIT_CONTEXT . key ) if axis_name is not None : axis_index = jax . lax . axis_index ( axis_name ) key = jax . random . fold_in ( key , axis_index ) return key","title":"treex.next_key"},{"location":"api/node/","text":"treex.node Source code in treeo/utils.py def node ( default = dataclasses . MISSING , * , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return field ( default = default , node = True , kind = kind , default_factory = default_factory , init = init , repr = repr , hash = hash , compare = compare , opaque = opaque , )","title":"node"},{"location":"api/node/#treexnode","text":"Source code in treeo/utils.py def node ( default = dataclasses . MISSING , * , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return field ( default = default , node = True , kind = kind , default_factory = default_factory , init = init , repr = repr , hash = hash , compare = compare , opaque = opaque , )","title":"treex.node"},{"location":"api/preserve_state/","text":"treex.preserve_state Takes in a function transformation such as jit or vmap and the function f to be transformed and returns a the transformed function with the expected behaviour but that additionally preserves the state of the first argument of f . For example, within a Module if you try to vmap over a method with stateful operations like this: @jax . vmap def __call__ ( self , x ): self . n += 1 return 2.0 * x It will not work since the composed function vmap(__call__) is a pure function so any change to self will not be reflected outside. To solve you can wrap vmap using preserve_state like this: @preserve_state ( jax . vmap ) def __call__ ( self , x ): self . n += 1 return 2.0 * x This will guarantee that the state of self is propagated to the outside. Parameters: Name Type Description Default transformation ~C The transformation to be applied to the function f . required f The function to be transformed. required *args Additional arguments to be passed to the transformation. required **kwargs Additional keyword arguments to be passed to the transformation. required Returns: Type Description ~C The transformed function. Source code in treex/module.py def preserve_state ( transformation : C , * transformation_args , ** transformation_kwargs ) -> C : \"\"\" Takes in a function transformation such as `jit` or `vmap` and the function `f` to be transformed and returns a the transformed function with the expected behaviour but that additionally preserves the state of the first argument of `f`. For example, within a `Module` if you try to `vmap` over a method with stateful operations like this: ```python @jax.vmap def __call__(self, x): self.n += 1 return 2.0 * x ``` It will not work since the composed function `vmap(__call__)` is a pure function so any change to `self` will not be reflected outside. To solve you can wrap `vmap` using `preserve_state` like this: ```python @preserve_state(jax.vmap) def __call__(self, x): self.n += 1 return 2.0 * x ``` This will guarantee that the state of `self` is propagated to the outside. Arguments: transformation: The transformation to be applied to the function `f`. f: The function to be transformed. *args: Additional arguments to be passed to the transformation. **kwargs: Additional keyword arguments to be passed to the transformation. Returns: The transformed function. \"\"\" @functools . wraps ( transformation ) def new_transformation ( f ): f_original = f f = _return_first ( f ) f = _update_first ( transformation ( f , * transformation_args , ** transformation_kwargs ) ) @functools . wraps ( f_original ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) return wrapper return new_transformation","title":"preserve_state"},{"location":"api/preserve_state/#treexpreserve_state","text":"Takes in a function transformation such as jit or vmap and the function f to be transformed and returns a the transformed function with the expected behaviour but that additionally preserves the state of the first argument of f . For example, within a Module if you try to vmap over a method with stateful operations like this: @jax . vmap def __call__ ( self , x ): self . n += 1 return 2.0 * x It will not work since the composed function vmap(__call__) is a pure function so any change to self will not be reflected outside. To solve you can wrap vmap using preserve_state like this: @preserve_state ( jax . vmap ) def __call__ ( self , x ): self . n += 1 return 2.0 * x This will guarantee that the state of self is propagated to the outside. Parameters: Name Type Description Default transformation ~C The transformation to be applied to the function f . required f The function to be transformed. required *args Additional arguments to be passed to the transformation. required **kwargs Additional keyword arguments to be passed to the transformation. required Returns: Type Description ~C The transformed function. Source code in treex/module.py def preserve_state ( transformation : C , * transformation_args , ** transformation_kwargs ) -> C : \"\"\" Takes in a function transformation such as `jit` or `vmap` and the function `f` to be transformed and returns a the transformed function with the expected behaviour but that additionally preserves the state of the first argument of `f`. For example, within a `Module` if you try to `vmap` over a method with stateful operations like this: ```python @jax.vmap def __call__(self, x): self.n += 1 return 2.0 * x ``` It will not work since the composed function `vmap(__call__)` is a pure function so any change to `self` will not be reflected outside. To solve you can wrap `vmap` using `preserve_state` like this: ```python @preserve_state(jax.vmap) def __call__(self, x): self.n += 1 return 2.0 * x ``` This will guarantee that the state of `self` is propagated to the outside. Arguments: transformation: The transformation to be applied to the function `f`. f: The function to be transformed. *args: Additional arguments to be passed to the transformation. **kwargs: Additional keyword arguments to be passed to the transformation. Returns: The transformed function. \"\"\" @functools . wraps ( transformation ) def new_transformation ( f ): f_original = f f = _return_first ( f ) f = _update_first ( transformation ( f , * transformation_args , ** transformation_kwargs ) ) @functools . wraps ( f_original ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) return wrapper return new_transformation","title":"treex.preserve_state"},{"location":"api/rng_key/","text":"treex.rng_key Source code in treex/module.py @contextmanager def rng_key ( key : types . KeyLike ): key = utils . Key ( key ) with _INIT_CONTEXT . update ( key = key ): yield","title":"rng_key"},{"location":"api/rng_key/#treexrng_key","text":"Source code in treex/module.py @contextmanager def rng_key ( key : types . KeyLike ): key = utils . Key ( key ) with _INIT_CONTEXT . update ( key = key ): yield","title":"treex.rng_key"},{"location":"api/sequence/","text":"treex.sequence Creates a function that applies a sequence of callables to an input. Examples: class Block ( tx . Module ): linear : tx . Linear batch_norm : tx . BatchNorm dropout : tx . Dropout ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : return tx . sequence ( self . linear , self . batch_norm , self . dropout , jax . nn . relu , )( x ) Parameters: Name Type Description Default *layers Callable[..., jax._src.numpy.lax_numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequential.py def sequence ( * layers : CallableModule ) -> CallableModule : \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence ( x : jnp . ndarray ) -> jnp . ndarray : for layer in layers : x = layer ( x ) return x return _sequence","title":"sequence"},{"location":"api/sequence/#treexsequence","text":"Creates a function that applies a sequence of callables to an input. Examples: class Block ( tx . Module ): linear : tx . Linear batch_norm : tx . BatchNorm dropout : tx . Dropout ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : return tx . sequence ( self . linear , self . batch_norm , self . dropout , jax . nn . relu , )( x ) Parameters: Name Type Description Default *layers Callable[..., jax._src.numpy.lax_numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequential.py def sequence ( * layers : CallableModule ) -> CallableModule : \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence ( x : jnp . ndarray ) -> jnp . ndarray : for layer in layers : x = layer ( x ) return x return _sequence","title":"treex.sequence"},{"location":"api/static/","text":"treex.static Source code in treeo/utils.py def static ( default = dataclasses . MISSING , * , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return field ( default , node = False , kind = kind , default_factory = default_factory , init = init , repr = repr , hash = hash , compare = compare , opaque = opaque , )","title":"static"},{"location":"api/static/#treexstatic","text":"Source code in treeo/utils.py def static ( default = dataclasses . MISSING , * , kind : type = type ( None ), default_factory : tp . Optional [ tp . Callable [[], tp . Any ]] = None , init : bool = True , repr : bool = True , hash : tp . Optional [ bool ] = None , compare : bool = True , opaque : tp . Union [ bool , OpaquePredicate ] = False , ) -> tp . Any : return field ( default , node = False , kind = kind , default_factory = default_factory , init = init , repr = repr , hash = hash , compare = compare , opaque = opaque , )","title":"treex.static"},{"location":"api/to_dict/","text":"treex.to_dict Source code in treeo/api.py def to_dict ( obj : tp . Any , * , private_fields : bool = False , static_fields : bool = True , type_info : bool = False , field_info : bool = False , ) -> tp . Any : if field_info : with add_field_info (), flatten_mode ( FlattenMode . all_fields ): flat , treedef = jax . tree_flatten ( obj ) obj = jax . tree_unflatten ( treedef , flat ) obj = apply ( _remove_field_info_from_metadata , obj ) return _to_dict ( obj , private_fields , static_fields , type_info )","title":"to_dict"},{"location":"api/to_dict/#treexto_dict","text":"Source code in treeo/api.py def to_dict ( obj : tp . Any , * , private_fields : bool = False , static_fields : bool = True , type_info : bool = False , field_info : bool = False , ) -> tp . Any : if field_info : with add_field_info (), flatten_mode ( FlattenMode . all_fields ): flat , treedef = jax . tree_flatten ( obj ) obj = jax . tree_unflatten ( treedef , flat ) obj = apply ( _remove_field_info_from_metadata , obj ) return _to_dict ( obj , private_fields , static_fields , type_info )","title":"treex.to_dict"},{"location":"api/to_string/","text":"treex.to_string Converts a pytree to a string representation. Parameters: Name Type Description Default obj Any The pytree to convert. required private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True Returns: Type Description str A string representation of the pytree. Source code in treeo/api.py def to_string ( obj : tp . Any , private_fields : bool = False , static_fields : bool = True , color : bool = False , ) -> str : \"\"\" Converts a pytree to a string representation. Arguments: obj: The pytree to convert. private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. Returns: A string representation of the pytree. \"\"\" dict_ = to_dict ( obj , private_fields = private_fields , static_fields = static_fields , type_info = True , field_info = True , ) global RICH_WARNING_COUNT rep = _to_string ( dict_ , level = 0 , inline = False , color = color , space = \" \" ) rep = _add_padding ( rep ) if color : if Console is None or Text is None : if RICH_WARNING_COUNT < 1 : RICH_WARNING_COUNT += 1 logging . warning ( f \"'rich' library not available, install `rich` to get colors.\" ) else : rep = _get_rich_repr ( Text . from_markup ( rep )) return rep","title":"to_string"},{"location":"api/to_string/#treexto_string","text":"Converts a pytree to a string representation. Parameters: Name Type Description Default obj Any The pytree to convert. required private_fields bool If True , private fields are included. False static_fields bool If True , static fields are included. True Returns: Type Description str A string representation of the pytree. Source code in treeo/api.py def to_string ( obj : tp . Any , private_fields : bool = False , static_fields : bool = True , color : bool = False , ) -> str : \"\"\" Converts a pytree to a string representation. Arguments: obj: The pytree to convert. private_fields: If `True`, private fields are included. static_fields: If `True`, static fields are included. Returns: A string representation of the pytree. \"\"\" dict_ = to_dict ( obj , private_fields = private_fields , static_fields = static_fields , type_info = True , field_info = True , ) global RICH_WARNING_COUNT rep = _to_string ( dict_ , level = 0 , inline = False , color = color , space = \" \" ) rep = _add_padding ( rep ) if color : if Console is None or Text is None : if RICH_WARNING_COUNT < 1 : RICH_WARNING_COUNT += 1 logging . warning ( f \"'rich' library not available, install `rich` to get colors.\" ) else : rep = _get_rich_repr ( Text . from_markup ( rep )) return rep","title":"treex.to_string"},{"location":"api/losses/CosineSimilarity/","text":"treex.losses.CosineSimilarity Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , axis =- 1 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#treexlossescosinesimilarity","text":"Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), )","title":"treex.losses.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#treex.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/CosineSimilarity/#treex.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/Crossentropy/","text":"treex.losses.Crossentropy Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = tx . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , * , from_logits = True , binary = False , label_smoothing = None , reduction = None , check_bounds = True , weight = None , on = None , name = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None check_bounds bool If True (default), checks target for negative values and values larger or equal than the number of channels in preds . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in treex/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , check_bounds : bool = True , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `target` for negative values and values larger or equal than the number of channels in `preds`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) self . _from_logits = from_logits self . _check_bounds = check_bounds self . _binary = binary self . _label_smoothing = label_smoothing call ( self , target , preds , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in treex/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , check_bounds = self . _check_bounds , )","title":"Crossentropy"},{"location":"api/losses/Crossentropy/#treexlossescrossentropy","text":"Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = tx . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"treex.losses.Crossentropy"},{"location":"api/losses/Crossentropy/#treex.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None check_bounds bool If True (default), checks target for negative values and values larger or equal than the number of channels in preds . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in treex/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , check_bounds : bool = True , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `target` for negative values and values larger or equal than the number of channels in `preds`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) self . _from_logits = from_logits self . _check_bounds = check_bounds self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/Crossentropy/#treex.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in treex/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , check_bounds = self . _check_bounds , )","title":"call()"},{"location":"api/losses/Huber/","text":"treex.losses.Huber Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = tx . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"Huber"},{"location":"api/losses/Huber/#treexlosseshuber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = tx . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), )","title":"treex.losses.Huber"},{"location":"api/losses/Huber/#treex.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/Huber/#treex.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/Loss/","text":"treex.losses.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in treex/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"Loss"},{"location":"api/losses/Loss/#treexlossesloss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , target , preds ): return jnp . mean ( jnp . square ( preds - target ), axis =- 1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"treex.losses.Loss"},{"location":"api/losses/Loss/#treex.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Union[float, numpy.ndarray, jax._src.numpy.lax_numpy.ndarray] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in treex/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ types . ScalarLike ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . _get_name ( self ) self . weight = ( jnp . asarray ( weight , dtype = jnp . float32 ) if weight is not None else jnp . array ( 1.0 , dtype = jnp . float32 ) ) self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/","text":"treex.losses.MeanAbsoluteError Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = tx . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#treexlossesmeanabsoluteerror","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = tx . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), )","title":"treex.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#treex.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#treex.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"treex.losses.MeanAbsolutePercentageError Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = tx . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#treexlossesmeanabsolutepercentageerror","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = tx . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), )","title":"treex.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#treex.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsolutePercentageError/#treex.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"treex.losses.MeanSquaredError Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = tx . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) call ( self , target , preds , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#treexlossesmeansquarederror","text":"Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = tx . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"treex.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#treex.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#treex.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/MeanSquaredLogarithmicError/","text":"treex.losses.MeanSquaredLogarithmicError Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = tx . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#treexlossesmeansquaredlogarithmicerror","text":"Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = tx . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), )","title":"treex.losses.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#treex.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredLogarithmicError/#treex.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/Reduction/","text":"treex.losses.Reduction Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"Reduction"},{"location":"api/losses/Reduction/#treexlossesreduction","text":"Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"treex.losses.Reduction"},{"location":"api/losses/cosine_similarity/","text":"treex.losses.cosine_similarity CosineSimilarity Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , axis =- 1 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis ) cosine_similarity ( target , preds , axis ) Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in treex/losses/cosine_similarity.py def cosine_similarity ( target : jnp . ndarray , preds : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity"},{"location":"api/losses/cosine_similarity/#treexlossescosine_similarity","text":"","title":"treex.losses.cosine_similarity"},{"location":"api/losses/cosine_similarity/#treex.losses.cosine_similarity.CosineSimilarity","text":"Computes the mean squared logarithmic errors between target and predictions. loss = -sum(l2_norm(target) * l2_norm(preds)) Usage: target = jnp . array ([[ 0. , 1. ], [ 1. , 1. ]]) preds = jnp . array ([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 ) assert cosine_loss ( target , preds ) == - 0.49999997 # Calling with 'sample_weight'. assert cosine_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == - 0.099999994 # Using 'sum' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . SUM ) assert cosine_loss ( target , preds ) == - 0.99999994 # Using 'none' reduction type. cosine_loss = tx . losses . CosineSimilarity ( axis = 1 , reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( cosine_loss ( target , preds ), jnp . array ([ - 0. , - 0.99999994 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . CosineSimilarity ( axis = 1 ), metrics = elegy . metrics . Mean (), )","title":"CosineSimilarity"},{"location":"api/losses/cosine_similarity/#treex.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/cosine_similarity/#treex.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/cosine_similarity.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( target , preds , self . axis )","title":"call()"},{"location":"api/losses/cosine_similarity/#treex.losses.cosine_similarity.cosine_similarity","text":"Computes the cosine similarity between target and predictions. loss = - sum ( l2_norm ( target ) * l2_norm ( preds )) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . cosine_similarity ( target , preds , axis = 1 ) assert loss . shape == ( 2 ,) target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = 1 , keepdims = True ), jnp . sqrt ( types . EPSILON )) assert jnp . array_equal ( loss , - jnp . sum ( target * preds , axis = 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in treex/losses/cosine_similarity.py def cosine_similarity ( target : jnp . ndarray , preds : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between target and predictions. ```python loss = -sum(l2_norm(target) * l2_norm(preds)) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.cosine_similarity(target, preds, axis=1) assert loss.shape == (2,) target = target / jnp.maximum(jnp.linalg.norm(target, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) preds = preds / jnp.maximum(jnp.linalg.norm(preds, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(target * preds, axis=1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" target = target / jnp . maximum ( jnp . linalg . norm ( target , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) preds = preds / jnp . maximum ( jnp . linalg . norm ( preds , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( target * preds , axis = axis )","title":"cosine_similarity()"},{"location":"api/losses/crossentropy/","text":"treex.losses.crossentropy Crossentropy Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = tx . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), ) __init__ ( self , * , from_logits = True , binary = False , label_smoothing = None , reduction = None , check_bounds = True , weight = None , on = None , name = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None check_bounds bool If True (default), checks target for negative values and values larger or equal than the number of channels in preds . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in treex/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , check_bounds : bool = True , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `target` for negative values and values larger or equal than the number of channels in `preds`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) self . _from_logits = from_logits self . _check_bounds = check_bounds self . _binary = binary self . _label_smoothing = label_smoothing call ( self , target , preds , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in treex/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , check_bounds = self . _check_bounds , )","title":"crossentropy"},{"location":"api/losses/crossentropy/#treexlossescrossentropy","text":"","title":"treex.losses.crossentropy"},{"location":"api/losses/crossentropy/#treex.losses.crossentropy.Crossentropy","text":"Computes the crossentropy loss between the target and predictions. Use this crossentropy loss function when there are two or more label classes. We expect target to be provided as integers. If you want to provide target using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for preds and a single floating point value per feature for target . In the snippet below, there is a single floating point value per example for target and # classes floating pointing values per example for preds . The shape of target is [batch_size] and the shape of preds is [batch_size, num_classes] . Usage: target = jnp . array ([ 1 , 2 ]) preds = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = tx . losses . Crossentropy () result = scce ( target , preds ) # 1.177 assert np . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( target , preds , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert np . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . SUM ) result = scce ( target , preds ) # 2.354 assert np . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = tx . losses . Crossentropy ( reduction = tx . losses . Reduction . NONE ) result = scce ( target , preds ) # [0.0513, 2.303] assert jnp . all ( np . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Crossentropy (), metrics = elegy . metrics . Accuracy (), optimizer = optax . adam ( 1e-3 ), )","title":"Crossentropy"},{"location":"api/losses/crossentropy/#treex.losses.crossentropy.Crossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether preds is expected to be a logits tensor. By default, we assume that preds encodes a probability distribution. Note - Using from_logits=True is more numerically stable. True reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None check_bounds bool If True (default), checks target for negative values and values larger or equal than the number of channels in preds . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in treex/losses/crossentropy.py def __init__ ( self , * , from_logits : bool = True , binary : bool = False , label_smoothing : tp . Optional [ float ] = None , reduction : tp . Optional [ Reduction ] = None , check_bounds : bool = True , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `preds` is expected to be a logits tensor. By default, we assume that `preds` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `target` for negative values and values larger or equal than the number of channels in `preds`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) self . _from_logits = from_logits self . _check_bounds = check_bounds self . _binary = binary self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/crossentropy/#treex.losses.crossentropy.Crossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default target Ground truth values. required preds The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in treex/losses/crossentropy.py def call ( self , target , preds , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: target: Ground truth values. preds: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return crossentropy ( target , preds , binary = self . _binary , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , check_bounds = self . _check_bounds , )","title":"call()"},{"location":"api/losses/huber/","text":"treex.losses.huber Huber Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = tx . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta ) huber ( target , preds , delta ) Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in treex/losses/huber.py def huber ( target : jnp . ndarray , preds : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber"},{"location":"api/losses/huber/#treexlosseshuber","text":"","title":"treex.losses.huber"},{"location":"api/losses/huber/#treex.losses.huber.Huber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: target = jnp . array ([[ 0 , 1 ], [ 0 , 0 ]]) preds = jnp . array ([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = tx . losses . Huber () assert huber_loss ( target , preds ) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss ( target , preds , sample_weight = jnp . array ([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . SUM ) assert huber_loss ( target , preds ) == 0.31 # Using 'none' reduction type. huber_loss = tx . losses . Huber ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( huber_loss ( target , preds ), jnp . array ([ 0.18 , 0.13000001 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . Huber ( delta = 1.0 ), metrics = elegy . metrics . Mean (), )","title":"Huber"},{"location":"api/losses/huber/#treex.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/huber/#treex.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/huber.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( target , preds , self . delta )","title":"call()"},{"location":"api/losses/huber/#treex.losses.huber.huber","text":"Computes the Huber loss between target and predictions. For each value x in error = target - preds: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . huber ( target , preds , delta = 1.0 ) assert loss . shape == ( 2 ,) preds = preds . astype ( float ) target = target . astype ( float ) delta = 1.0 error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) assert jnp . array_equal ( loss , jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic ) ), jnp . multiply ( delta , linear )), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in treex/losses/huber.py def huber ( target : jnp . ndarray , preds : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between target and predictions. For each value x in error = target - preds: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.huber(target, preds, delta=1.0) assert loss.shape == (2,) preds = preds.astype(float) target = target.astype(float) delta = 1.0 error = jnp.subtract(preds, target) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" preds = preds . astype ( float ) target = target . astype ( float ) delta = float ( delta ) error = jnp . subtract ( preds , target ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber()"},{"location":"api/losses/mean_absolute_error/","text":"treex.losses.mean_absolute_error MeanAbsoluteError Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = tx . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds ) mean_absolute_error ( target , preds ) Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_absolute_error.py def mean_absolute_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#treexlossesmean_absolute_error","text":"","title":"treex.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#treex.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs(target - preds)) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = tx . losses . MeanAbsoluteError () assert mae ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mae ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . SUM ) assert mae ( target , preds ) == 1.0 # Using 'none' reduction type. mae = tx . losses . MeanAbsoluteError ( reduction = tx . losses . Reduction . NONE ) assert list ( mae ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsoluteError (), metrics = elegy . metrics . Mean (), )","title":"MeanAbsoluteError"},{"location":"api/losses/mean_absolute_error/#treex.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_absolute_error/#treex.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_error/#treex.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean ( abs ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_absolute_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . abs ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_absolute_error.py def mean_absolute_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between target and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_absolute_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . abs ( preds - target ), axis =- 1 )","title":"mean_absolute_error()"},{"location":"api/losses/mean_absolute_percentage_error/","text":"treex.losses.mean_absolute_percentage_error MeanAbsolutePercentageError Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = tx . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds ) mean_absolute_percentage_error ( target , preds ) Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#treexlossesmean_absolute_percentage_error","text":"","title":"treex.losses.mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#treex.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the mean absolute errors between target and predictions. loss = mean(abs((target - preds) / target)) Usage: target = jnp . array ([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = tx . losses . MeanAbsolutePercentageError () result = mape ( target , preds ) assert np . isclose ( result , 2.78 , rtol = 0.01 ) # Calling with 'sample_weight'. assert np . isclose ( mape ( target , preds , sample_weight = jnp . array ([ 0.1 , 0.9 ])), 2.5 , rtol = 0.01 ) # Using 'sum' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . SUM ) assert np . isclose ( mape ( target , preds ), 5.6 , rtol = 0.01 ) # Using 'none' reduction type. mape = tx . losses . MeanAbsolutePercentageError ( reduction = tx . losses . Reduction . NONE ) assert jnp . all ( np . isclose ( result , [ 0. , 5.6 ], rtol = 0.01 )) Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanAbsolutePercentageError (), metrics = elegy . metrics . Mean (), )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/mean_absolute_percentage_error/#treex.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_absolute_percentage_error/#treex.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_absolute_percentage_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_absolute_percentage_error/#treex.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_absolute_percentage_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , 100. * jnp . mean ( jnp . abs (( preds - target ) / jnp . clip ( target , types . EPSILON , None )))) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between target and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_absolute_percentage_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((preds - target) / jnp.clip(target, types.EPSILON, None)))) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) diff = jnp . abs (( preds - target ) / jnp . maximum ( jnp . abs ( target ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error()"},{"location":"api/losses/mean_squared_error/","text":"treex.losses.mean_squared_error MeanSquaredError Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = tx . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name ) call ( self , target , preds , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds ) mean_squared_error ( target , preds ) Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_squared_error.py def mean_squared_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#treexlossesmean_squared_error","text":"","title":"treex.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#treex.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between target and predictions. loss = square(target - preds) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = tx . losses . MeanSquaredError () assert mse ( target , preds ) == 0.5 # Calling with 'sample_weight'. assert mse ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . SUM ) assert mse ( target , preds ) == 1.0 # Using 'none' reduction type. mse = tx . losses . MeanSquaredError ( reduction = tx . losses . Reduction . NONE ) assert list ( mse ( target , preds )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"MeanSquaredError"},{"location":"api/losses/mean_squared_error/#treex.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , name = name )","title":"__init__()"},{"location":"api/losses/mean_squared_error/#treex.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_error/#treex.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( target - preds ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_squared_error ( target , preds ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( target - preds ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_squared_error.py def mean_squared_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between target and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(target - preds), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_squared_error(target, preds) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(target - preds), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) return jnp . mean ( jnp . square ( preds - target ), axis =- 1 )","title":"mean_squared_error()"},{"location":"api/losses/mean_squared_logarithmic_error/","text":"treex.losses.mean_squared_logarithmic_error MeanSquaredLogarithmicError Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = tx . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , target , preds , sample_weight = None ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds ) mean_squared_logarithmic_error ( target , preds ) Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#treexlossesmean_squared_logarithmic_error","text":"","title":"treex.losses.mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#treex.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError","text":"Computes the mean squared logarithmic errors between target and predictions. loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) Usage: target = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) preds = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = tx . losses . MeanSquaredLogarithmicError () assert msle ( target , preds ) == 0.24022643 # Calling with 'sample_weight'. assert msle ( target , preds , sample_weight = jnp . array ([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . SUM ) assert msle ( target , preds ) == 0.48045287 # Using 'none' reduction type. msle = tx . losses . MeanSquaredLogarithmicError ( reduction = tx . losses . Reduction . NONE ) assert jnp . equal ( msle ( target , preds ), jnp . array ([ 0.24022643 , 0.24022643 ])) . all () Usage with the Elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredLogarithmicError (), metrics = elegy . metrics . Mean (), )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/mean_squared_logarithmic_error/#treex.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[treex.losses.loss.Reduction] (Optional) Type of tx.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `tx.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/mean_squared_logarithmic_error/#treex.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of preds is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in treex/losses/mean_squared_logarithmic_error.py def call ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `preds` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( target , preds )","title":"call()"},{"location":"api/losses/mean_squared_logarithmic_error/#treex.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error","text":"Computes the mean squared logarithmic error between target and predictions. loss = mean ( square ( log ( target + 1 ) - log ( preds + 1 )), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) target = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) preds = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = tx . losses . mean_squared_logarithmic_error ( target , preds ) assert loss . shape == ( 2 ,) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )) Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in treex/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( target : jnp . ndarray , preds : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between target and predictions. ```python loss = mean(square(log(target + 1) - log(preds + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) target = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) preds = jax.random.uniform(rng, shape=(2, 3)) loss = tx.losses.mean_squared_logarithmic_error(target, preds) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(target, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(preds, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" target = target . astype ( preds . dtype ) first_log = jnp . log ( jnp . maximum ( target , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( preds , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error()"},{"location":"api/metrics/Accuracy/","text":"treex.metrics.Accuracy Computes Accuracy_: .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Accepts all input types listed in :ref: references/modules:input types . Parameters: Name Type Description Default num_classes Number of classes. Necessary for 'macro' , 'weighted' and None average methods. required threshold Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. required average Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . required mdmc_average Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. required ignore_index Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . required top_k Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. required multiclass Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. required subset_accuracy Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. required compute_on_step Forward only calls update() and return None if this is set to False . required dist_sync_on_step Synchronize metric state across processes at each forward() before returning the value at the step required process_group Specify the process group on which synchronization is called. default: None (which selects the entire world) required dist_sync_fn Callback that performs the allgather operation on the metric state. When None , DDP will be used to perform the allgather required Examples: >>> import torch >>> from torchmetrics import Accuracy >>> target = torch . tensor ([ 0 , 1 , 2 , 3 ]) >>> preds = torch . tensor ([ 0 , 2 , 1 , 3 ]) >>> accuracy = Accuracy () >>> accuracy ( preds , target ) tensor ( 0.5000 ) >>> target = torch . tensor ([ 0 , 1 , 2 ]) >>> preds = torch . tensor ([[ 0.1 , 0.9 , 0 ], [ 0.3 , 0.1 , 0.6 ], [ 0.2 , 0.5 , 0.3 ]]) >>> accuracy = Accuracy ( top_k = 2 ) >>> accuracy ( preds , target ) tensor ( 0.6667 ) __call__ ( self , preds , target ) special Update state with predictions and targets. See :ref: references/modules:input types for more information on input types. Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Source code in treex/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray ) -> None : # type: ignore \"\"\"Update state with predictions and targets. See :ref:`references/modules:input types` for more information on input types. Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target \"\"\" tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) self . tp += tp self . fp += fp self . tn += tn self . fn += fn compute ( self ) Computes accuracy based on inputs passed in to update previously. Source code in treex/metrics/accuracy.py def compute ( self ) -> jnp . ndarray : \"\"\"Computes accuracy based on inputs passed in to ``update`` previously.\"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , ) update ( self , preds , target ) Update state with predictions and targets. See :ref: references/modules:input types for more information on input types. Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Source code in treex/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray ) -> None : # type: ignore \"\"\"Update state with predictions and targets. See :ref:`references/modules:input types` for more information on input types. Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target \"\"\" tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) self . tp += tp self . fp += fp self . tn += tn self . fn += fn","title":"Accuracy"},{"location":"api/metrics/Accuracy/#treexmetricsaccuracy","text":"Computes Accuracy_: .. math:: \\text{Accuracy} = \\frac{1}{N}\\sum_i^N 1(y_i = \\hat{y}_i) Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. For multi-class and multi-dimensional multi-class data with probability or logits predictions, the parameter top_k generalizes this metric to a Top-K accuracy metric: for each sample the top-K highest probability or logit score items are considered to find the correct label. For multi-label and multi-dimensional multi-class inputs, this metric computes the \"glob\" accuracy by default, which counts all target or sub-samples separately. This can be changed to subset accuracy (which requires all target or sub-samples in the sample to be correctly predicted) by setting subset_accuracy=True . Accepts all input types listed in :ref: references/modules:input types . Parameters: Name Type Description Default num_classes Number of classes. Necessary for 'macro' , 'weighted' and None average methods. required threshold Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities. required average Defines the reduction that is applied. Should be one of the following: 'micro' [default]: Calculate the metric globally, across all samples and classes. 'macro' : Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class). 'weighted' : Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support ( tp + fn ). 'none' or None : Calculate the metric for each class separately, and return the metric for every class. 'samples' : Calculate the metric for each sample, and average the metrics across samples (with equal weights for each sample). .. note:: What is considered a sample in the multi-dimensional multi-class case depends on the value of mdmc_average . .. note:: If 'none' and a given class doesn't occur in the preds or target , the value for the class will be nan . required mdmc_average Defines how averaging is done for multi-dimensional multi-class inputs (on top of the average parameter). Should be one of the following: None [default]: Should be left unchanged if your data is not multi-dimensional multi-class. 'samplewise' : In this case, the statistics are computed separately for each sample on the N axis, and then averaged over samples. The computation for each sample is done by treating the flattened extra axes ... (see :ref: references/modules:input types ) as the N dimension within the sample, and computing the metric for the sample based on that. 'global' : In this case the N and ... dimensions of the inputs (see :ref: references/modules:input types ) are flattened into a new N_X sample axis, i.e. the inputs are treated as if they were (N_X, C) . From here on the average parameter applies as usual. required ignore_index Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and average=None or 'none' , the score for the ignored class will be returned as nan . required top_k Number of highest probability or logit score predictions considered to find the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value ( None ) will be interpreted as 1 for these inputs. Should be left at default ( None ) for all other types of inputs. required multiclass Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref: documentation section <references/modules:using the multiclass parameter> for a more detailed explanation and examples. required subset_accuracy Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types). For multi-label inputs, if the parameter is set to True , then all target for each sample must be correctly predicted for the sample to count as correct. If it is set to False , then all target are counted separately - this is equivalent to flattening inputs beforehand (i.e. preds = preds.flatten() and same for target ). For multi-dimensional multi-class inputs, if the parameter is set to True , then all sub-sample (on the extra axis) must be correct for the sample to be counted as correct. If it is set to False , then all sub-samples are counter separately - this is equivalent, in the case of label predictions, to flattening the inputs beforehand (i.e. preds = preds.flatten() and same for target ). Note that the top_k parameter still applies in both cases, if set. required compute_on_step Forward only calls update() and return None if this is set to False . required dist_sync_on_step Synchronize metric state across processes at each forward() before returning the value at the step required process_group Specify the process group on which synchronization is called. default: None (which selects the entire world) required dist_sync_fn Callback that performs the allgather operation on the metric state. When None , DDP will be used to perform the allgather required Examples: >>> import torch >>> from torchmetrics import Accuracy >>> target = torch . tensor ([ 0 , 1 , 2 , 3 ]) >>> preds = torch . tensor ([ 0 , 2 , 1 , 3 ]) >>> accuracy = Accuracy () >>> accuracy ( preds , target ) tensor ( 0.5000 ) >>> target = torch . tensor ([ 0 , 1 , 2 ]) >>> preds = torch . tensor ([[ 0.1 , 0.9 , 0 ], [ 0.3 , 0.1 , 0.6 ], [ 0.2 , 0.5 , 0.3 ]]) >>> accuracy = Accuracy ( top_k = 2 ) >>> accuracy ( preds , target ) tensor ( 0.6667 )","title":"treex.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#treex.metrics.accuracy.Accuracy.__call__","text":"Update state with predictions and targets. See :ref: references/modules:input types for more information on input types. Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Source code in treex/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray ) -> None : # type: ignore \"\"\"Update state with predictions and targets. See :ref:`references/modules:input types` for more information on input types. Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target \"\"\" tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) self . tp += tp self . fp += fp self . tn += tn self . fn += fn","title":"__call__()"},{"location":"api/metrics/Accuracy/#treex.metrics.accuracy.Accuracy.compute","text":"Computes accuracy based on inputs passed in to update previously. Source code in treex/metrics/accuracy.py def compute ( self ) -> jnp . ndarray : \"\"\"Computes accuracy based on inputs passed in to ``update`` previously.\"\"\" # if self.mode is None: # raise RuntimeError(\"You have to have determined mode.\") return metric_utils . _accuracy_compute ( self . tp , self . fp , self . tn , self . fn , self . average , self . mdmc_average , self . mode , )","title":"compute()"},{"location":"api/metrics/Accuracy/#treex.metrics.accuracy.Accuracy.update","text":"Update state with predictions and targets. See :ref: references/modules:input types for more information on input types. Parameters: Name Type Description Default preds ndarray Predictions from model (logits, probabilities, or target) required target ndarray Ground truth target required Source code in treex/metrics/accuracy.py def update ( self , preds : jnp . ndarray , target : jnp . ndarray ) -> None : # type: ignore \"\"\"Update state with predictions and targets. See :ref:`references/modules:input types` for more information on input types. Args: preds: Predictions from model (logits, probabilities, or target) target: Ground truth target \"\"\" tp , fp , tn , fn = metric_utils . _stat_scores_update ( preds , target , intended_mode = self . mode , average_method = self . average , mdmc_average_method = self . mdmc_average , threshold = self . threshold , num_classes = self . num_classes , top_k = self . top_k , multiclass = self . multiclass , ) self . tp += tp self . fp += fp self . tn += tn self . fn += fn","title":"update()"},{"location":"api/metrics/AuxLosses/","text":"treex.metrics.AuxLosses","title":"AuxLosses"},{"location":"api/metrics/AuxLosses/#treexmetricsauxlosses","text":"","title":"treex.metrics.AuxLosses"},{"location":"api/metrics/AuxMetrics/","text":"treex.metrics.AuxMetrics","title":"AuxMetrics"},{"location":"api/metrics/AuxMetrics/#treexmetricsauxmetrics","text":"","title":"treex.metrics.AuxMetrics"},{"location":"api/metrics/LossAndLogs/","text":"treex.metrics.LossAndLogs","title":"LossAndLogs"},{"location":"api/metrics/LossAndLogs/#treexmetricslossandlogs","text":"","title":"treex.metrics.LossAndLogs"},{"location":"api/metrics/Losses/","text":"treex.metrics.Losses","title":"Losses"},{"location":"api/metrics/Losses/#treexmetricslosses","text":"","title":"treex.metrics.Losses"},{"location":"api/metrics/MAE/","text":"treex.metrics.MAE __call__ ( self , target , preds , sample_weight = None ) special Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , on = None , name = None , dtype = None ) special Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in treex/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MAE"},{"location":"api/metrics/MAE/#treexmetricsmae","text":"","title":"treex.metrics.MAE"},{"location":"api/metrics/MAE/#treex.metrics.mean_absolute_error.MAE.__call__","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"__call__()"},{"location":"api/metrics/MAE/#treex.metrics.mean_absolute_error.MAE.__init__","text":"Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in treex/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MAE/#treex.metrics.mean_absolute_error.MAE.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/MSE/","text":"treex.metrics.MSE __call__ ( self , target , preds , sample_weight = None ) special Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , on = None , name = None , dtype = None ) special Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in treex/metrics/mean_square_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MSE"},{"location":"api/metrics/MSE/#treexmetricsmse","text":"","title":"treex.metrics.MSE"},{"location":"api/metrics/MSE/#treex.metrics.mean_square_error.MSE.__call__","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"__call__()"},{"location":"api/metrics/MSE/#treex.metrics.mean_square_error.MSE.__init__","text":"Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in treex/metrics/mean_square_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MSE/#treex.metrics.mean_square_error.MSE.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/Mean/","text":"treex.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), ) __call__ ( self , values , sample_weight = None ) special Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description Array with the cumulative mean. Source code in treex/metrics/mean.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" super () . update ( values = values , sample_weight = sample_weight , ) __init__ ( self , on = None , name = None , dtype = None ) special Creates a Mean instance. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. required Source code in treex/metrics/mean.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , on = on , name = name , dtype = dtype , ) update ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description Array with the cumulative mean. Source code in treex/metrics/mean.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" super () . update ( values = values , sample_weight = sample_weight , )","title":"Mean"},{"location":"api/metrics/Mean/#treexmetricsmean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( module_fn , loss = tx . losses . MeanSquaredError (), metrics = elegy . metrics . Mean (), )","title":"treex.metrics.Mean"},{"location":"api/metrics/Mean/#treex.metrics.mean.Mean.__call__","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description Array with the cumulative mean. Source code in treex/metrics/mean.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" super () . update ( values = values , sample_weight = sample_weight , )","title":"__call__()"},{"location":"api/metrics/Mean/#treex.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. required Source code in treex/metrics/mean.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . weighted_mean , on = on , name = name , dtype = dtype , )","title":"__init__()"},{"location":"api/metrics/Mean/#treex.metrics.mean.Mean.update","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description Array with the cumulative mean. Source code in treex/metrics/mean.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" super () . update ( values = values , sample_weight = sample_weight , )","title":"update()"},{"location":"api/metrics/MeanAbsoluteError/","text":"treex.metrics.MeanAbsoluteError __call__ ( self , target , preds , sample_weight = None ) special Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , on = None , name = None , dtype = None ) special Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in treex/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#treexmetricsmeanabsoluteerror","text":"","title":"treex.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#treex.metrics.mean_absolute_error.MeanAbsoluteError.__call__","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"__call__()"},{"location":"api/metrics/MeanAbsoluteError/#treex.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Computes Mean Absolute Error _ (MAE): .. math:: ext{MAE} = rac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_absolute_error import MeanAbsolutError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mae = MeanAbsolutError() mae(preds, target) Source code in treex/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Absolute Error`_ (MAE): .. math:: \\text{MAE} = \\frac{1}{N}\\sum_i^N | y_i - \\hat{y_i} | Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_absolute_error import MeanAbsolutError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mae = MeanAbsolutError() >>> mae(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#treex.metrics.mean_absolute_error.MeanAbsoluteError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_absolute_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_absolute_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/MeanSquareError/","text":"treex.metrics.MeanSquareError __call__ ( self , target , preds , sample_weight = None ) special Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight ) __init__ ( self , on = None , name = None , dtype = None ) special Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in treex/metrics/mean_square_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype ) update ( self , target , preds , sample_weight = None ) Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"MeanSquareError"},{"location":"api/metrics/MeanSquareError/#treexmetricsmeansquareerror","text":"","title":"treex.metrics.MeanSquareError"},{"location":"api/metrics/MeanSquareError/#treex.metrics.mean_square_error.MeanSquareError.__call__","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"__call__()"},{"location":"api/metrics/MeanSquareError/#treex.metrics.mean_square_error.MeanSquareError.__init__","text":"Computes Mean Square Error _ (MSE): .. math:: ext{MSE} = rac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math: y is a tensor of target values, and :math: \\hat{y} is a tensor of predictions. Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None name Optional[str] Module name None dtype Optional[numpy.dtype] Metrics states initialization dtype None Examples: import jax.numpy as jnp from treex.metrics.mean_square_error import MeanSquareError target = jnp.array([3.0, -0.5, 2.0, 7.0]) preds = jnp.array([3.0, -0.5, 2.0, 7.0]) mse = MeanSquareError() mse(preds, target) Source code in treex/metrics/mean_square_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" `Computes Mean Square Error`_ (MSE): .. math:: \\text{MSE} = \\frac{1}{N}\\sum_i^N(y_i - \\hat{y_i})^2 Where :math:`y` is a tensor of target values, and :math:`\\hat{y}` is a tensor of predictions. Args: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Module name dtype: Metrics states initialization dtype Example: >>> import jax.numpy as jnp >>> from treex.metrics.mean_square_error import MeanSquareError >>> target = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> preds = jnp.array([3.0, -0.5, 2.0, 7.0]) >>> mse = MeanSquareError() >>> mse(preds, target) \"\"\" super () . __init__ ( on = on , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanSquareError/#treex.metrics.mean_square_error.MeanSquareError.update","text":"Accumulates metric statistics. target and preds should have the same shape. Parameters: Name Type Description Default target ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required preds ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight ndarray Optional weighting of each example. Defaults to 1. shape = [batch_size, d0, .. dN] None Returns: Type Description Any Array with the cumulative mean absolute error. Source code in treex/metrics/mean_square_error.py def update ( self , target : jnp . ndarray , preds : jnp . ndarray , sample_weight : jnp . ndarray = None , ) -> tp . Any : \"\"\" Accumulates metric statistics. `target` and `preds` should have the same shape. Arguments: target: Ground truth values. shape = `[batch_size, d0, .. dN]`. preds: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional weighting of each example. Defaults to 1. shape = `[batch_size, d0, .. dN]` Returns: Array with the cumulative mean absolute error. \"\"\" values = _mean_square_error ( preds , target ) return super () . update ( values , sample_weight )","title":"update()"},{"location":"api/metrics/Metric/","text":"treex.metrics.Metric Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point. __init__ ( self , on = None , name = None , dtype = None ) special Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32 __init_subclass__ () classmethod special This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/metrics/metric.py def __init_subclass__ ( cls ): super () . __init_subclass__ () # add call signature old_call = cls . __call__ @functools . wraps ( cls . update ) def new_call ( self : M , * args , ** kwargs ) -> M : if len ( args ) > 0 : raise TypeError ( f \"All arguments to { cls . __name__ } .__call__ should be passed as keyword arguments.\" ) return old_call ( self , * args , ** kwargs ) cls . __call__ = new_call","title":"Metric"},{"location":"api/metrics/Metric/#treexmetricsmetric","text":"Encapsulates metric logic and state. Metrics accumulate state between calls such that their output value reflect the metric as if calculated on the whole data given up to that point.","title":"treex.metrics.Metric"},{"location":"api/metrics/Metric/#treex.metrics.metric.Metric.__init__","text":"Parameters: Name Type Description Default on Union[str, int, Sequence[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the target and preds arguments before passing them to call . For example if on = \"a\" then target = target[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then target = target[\"a\"][0][\"b\"] , same for preds . For more information check out Keras-like behavior . None Source code in treex/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `target` and `preds` arguments before passing them to `call`. For example if `on = \"a\"` then `target = target[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `target = target[\"a\"][0][\"b\"]`, same for `preds`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . name = name if name is not None else utils . _get_name ( self ) self . dtype = dtype if dtype is not None else jnp . float32","title":"__init__()"},{"location":"api/metrics/Metric/#treex.metrics.metric.Metric.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. Source code in treex/metrics/metric.py def __init_subclass__ ( cls ): super () . __init_subclass__ () # add call signature old_call = cls . __call__ @functools . wraps ( cls . update ) def new_call ( self : M , * args , ** kwargs ) -> M : if len ( args ) > 0 : raise TypeError ( f \"All arguments to { cls . __name__ } .__call__ should be passed as keyword arguments.\" ) return old_call ( self , * args , ** kwargs ) cls . __call__ = new_call","title":"__init_subclass__()"},{"location":"api/metrics/Metrics/","text":"treex.metrics.Metrics","title":"Metrics"},{"location":"api/metrics/Metrics/#treexmetricsmetrics","text":"","title":"treex.metrics.Metrics"},{"location":"api/metrics/Reduce/","text":"treex.metrics.Reduce Encapsulates metrics that perform a reduce operation on the values. __call__ ( self , values , sample_weight = None ) special Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description Array with the cumulative reduce. Source code in treex/metrics/reduce.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) self . total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None self . count = ( self . count + num_values ) . astype ( self . count . dtype ) update ( self , values , sample_weight = None ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description Array with the cumulative reduce. Source code in treex/metrics/reduce.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) self . total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None self . count = ( self . count + num_values ) . astype ( self . count . dtype )","title":"Reduce"},{"location":"api/metrics/Reduce/#treexmetricsreduce","text":"Encapsulates metrics that perform a reduce operation on the values.","title":"treex.metrics.Reduce"},{"location":"api/metrics/Reduce/#treex.metrics.reduce.Reduce.__call__","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description Array with the cumulative reduce. Source code in treex/metrics/reduce.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) self . total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None self . count = ( self . count + num_values ) . astype ( self . count . dtype )","title":"__call__()"},{"location":"api/metrics/Reduce/#treex.metrics.reduce.Reduce.update","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description Array with the cumulative reduce. Source code in treex/metrics/reduce.py def update ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ): \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" # perform update if sample_weight is not None : if sample_weight . ndim > values . ndim : raise Exception ( f \"sample_weight dimention is higher than values, when masking values sample_weight dimention needs to be equal or lower than values dimension, currently values have shape equal to { values . shape } \" ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array values_ndim , weight_ndim = values . ndim , sample_weight . ndim if self . reduction == Reduction . sum : values = jnp . sum ( values , axis = list ( range ( weight_ndim , values_ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , values_ndim )) ) values = values * sample_weight value_sum = jnp . sum ( values ) self . total = ( self . total + value_sum ) . astype ( self . total . dtype ) # Exit early if the reduction doesn't have a denominator. if self . reduction == Reduction . sum : num_values = None # Update `count` for reductions that require a denominator. elif self . reduction == Reduction . sum_over_batch_size : num_values = np . prod ( values . shape ) else : if sample_weight is None : num_values = np . prod ( values . shape ) else : num_values = jnp . sum ( sample_weight ) if self . count is not None : assert num_values is not None self . count = ( self . count + num_values ) . astype ( self . count . dtype )","title":"update()"},{"location":"api/metrics/Reduction/","text":"treex.metrics.Reduction An enumeration.","title":"Reduction"},{"location":"api/metrics/Reduction/#treexmetricsreduction","text":"An enumeration.","title":"treex.metrics.Reduction"},{"location":"api/nn/BatchNorm/","text":"treex.nn.BatchNorm BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training or self.frozen is used unless use_running_average is explicitly passed via __call__ . __call__ ( self , x , use_running_average = None ) special Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , use_running_average : tp . Optional [ bool ] = None ) -> jnp . ndarray : \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , use_running_average = True , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( batch_stats = dict ( mean = self . mean , var = self . var , ), params = params , ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training and not self . frozen and self . initialized ) # call apply output , variables = self . module . apply ( variables , x , mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training , ) variables = variables . unfreeze () # update batch_stats if \"batch_stats\" in variables : self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , axis =- 1 , momentum = 0.99 , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>, axis_name=None, axis_index_groups=None) special Parameters: Name Type Description Default features_in the number of input features. required axis int the feature or non-batch axis of the input. -1 momentum Union[float, jax._src.numpy.lax_numpy.ndarray] decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/norm.py def __init__ ( self , * , axis : int = - 1 , momentum : tp . Union [ float , jnp . ndarray ] = 0.99 , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , axis_name : tp . Optional [ str ] = None , axis_index_groups : tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . axis = axis self . momentum = jnp . asarray ( momentum ) self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . axis_name = axis_name self . axis_index_groups = axis_index_groups self . mean = None self . var = None self . scale = None self . bias = None","title":"BatchNorm"},{"location":"api/nn/BatchNorm/#treexnnbatchnorm","text":"BatchNorm Module. BatchNorm is implemented as a wrapper over flax.linen.BatchNorm , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: use_running_average is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how BatchNorm should behave, interally use_running_average = not self.training or self.frozen is used unless use_running_average is explicitly passed via __call__ .","title":"treex.nn.BatchNorm"},{"location":"api/nn/BatchNorm/#treex.nn.norm.BatchNorm.__call__","text":"Normalizes the input using batch statistics. Parameters: Name Type Description Default x ndarray the input to be normalized. required use_running_average Optional[bool] if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. None Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , use_running_average : tp . Optional [ bool ] = None ) -> jnp . ndarray : \"\"\"Normalizes the input using batch statistics. Arguments: x: the input to be normalized. use_running_average: if true, the statistics stored in batch_stats will be used instead of computing the batch statistics on the input. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , use_running_average = True , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( batch_stats = dict ( mean = self . mean , var = self . var , ), params = params , ) # use_running_average = True means batch_stats will not be mutated # self.training = True means batch_stats will be mutated training = ( not use_running_average if use_running_average is not None else self . training and not self . frozen and self . initialized ) # call apply output , variables = self . module . apply ( variables , x , mutable = [ \"batch_stats\" ] if training else [], use_running_average = not training , ) variables = variables . unfreeze () # update batch_stats if \"batch_stats\" in variables : self . mean = variables [ \"batch_stats\" ][ \"mean\" ] self . var = variables [ \"batch_stats\" ][ \"var\" ] return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/BatchNorm/#treex.nn.norm.BatchNorm.__init__","text":"Parameters: Name Type Description Default features_in the number of input features. required axis int the feature or non-batch axis of the input. -1 momentum Union[float, jax._src.numpy.lax_numpy.ndarray] decay rate for the exponential moving average of the batch statistics. 0.99 epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> axis_name Optional[str] the axis name used to combine batch statistics from multiple devices. See jax.pmap for a description of axis names (default: None). None axis_index_groups Any groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, [[0, 1], [2, 3]] would independently batch-normalize over the examples on the first two and last two devices. See jax.lax.psum for more details. None Source code in treex/nn/norm.py def __init__ ( self , * , axis : int = - 1 , momentum : tp . Union [ float , jnp . ndarray ] = 0.99 , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , axis_name : tp . Optional [ str ] = None , axis_index_groups : tp . Any = None , ): \"\"\" Arguments: features_in: the number of input features. axis: the feature or non-batch axis of the input. momentum: decay rate for the exponential moving average of the batch statistics. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. axis_name: the axis name used to combine batch statistics from multiple devices. See `jax.pmap` for a description of axis names (default: None). axis_index_groups: groups of axis indices within that named axis representing subsets of devices to reduce over (default: None). For example, `[[0, 1], [2, 3]]` would independently batch-normalize over the examples on the first two and last two devices. See `jax.lax.psum` for more details. \"\"\" self . axis = axis self . momentum = jnp . asarray ( momentum ) self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . axis_name = axis_name self . axis_index_groups = axis_index_groups self . mean = None self . var = None self . scale = None self . bias = None","title":"__init__()"},{"location":"api/nn/Conv/","text":"treex.nn.Conv Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x ) special Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" if self . initializing (): variables = self . module . init ({ \"params\" : next_key ()}, x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , features_out , kernel_size , * , strides = None , padding = 'SAME' , input_dilation = None , kernel_dilation = None , feature_group_count = 1 , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>) special Parameters: Name Type Description Default features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/conv.py def __init__ ( self , features_out : int , kernel_size : tp . Union [ int , tp . Iterable [ int ]], * , strides : tp . Optional [ tp . Iterable [ int ]] = None , padding : tp . Union [ str , tp . Iterable [ tp . Tuple [ int , int ]]] = \"SAME\" , input_dilation : tp . Optional [ tp . Iterable [ int ]] = None , kernel_dilation : tp . Optional [ tp . Iterable [ int ]] = None , feature_group_count : int = 1 , use_bias : bool = True , dtype : flax_module . Dtype = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_out = features_out self . kernel_size = kernel_size self . strides = strides self . padding = padding self . input_dilation = input_dilation self . kernel_dilation = kernel_dilation self . feature_group_count = feature_group_count self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . kernel = None self . bias = None","title":"Conv"},{"location":"api/nn/Conv/#treexnnconv","text":"Convolution Module wrapping lax.conv_general_dilated. Conv is implemented as a wrapper over flax.linen.Conv , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.nn.Conv"},{"location":"api/nn/Conv/#treex.nn.conv.Conv.__call__","text":"Applies a convolution to the inputs. Parameters: Name Type Description Default x ndarray input data with dimensions (batch, spatial_dims..., features). required Returns: Type Description ndarray The convolved data. Source code in treex/nn/conv.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a convolution to the inputs. Arguments: x: input data with dimensions (batch, spatial_dims..., features). Returns: The convolved data. \"\"\" if self . initializing (): variables = self . module . init ({ \"params\" : next_key ()}, x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/Conv/#treex.nn.conv.Conv.__init__","text":"Parameters: Name Type Description Default features_out int number of convolution filters. required kernel_size Union[int, Iterable[int]] shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. required strides Optional[Iterable[int]] a sequence of n integers, representing the inter-window strides. None padding Union[str, Iterable[Tuple[int, int]]] either the string 'SAME' , the string 'VALID' , or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. 'SAME' input_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of inputs . Convolution with input dilation d is equivalent to transposed convolution with stride d . None kernel_dilation Optional[Iterable[int]] None , or a sequence of n integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. None feature_group_count int integer, default 1. If specified divides the input features into groups. 1 use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer for the convolutional kernel. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/conv.py def __init__ ( self , features_out : int , kernel_size : tp . Union [ int , tp . Iterable [ int ]], * , strides : tp . Optional [ tp . Iterable [ int ]] = None , padding : tp . Union [ str , tp . Iterable [ tp . Tuple [ int , int ]]] = \"SAME\" , input_dilation : tp . Optional [ tp . Iterable [ int ]] = None , kernel_dilation : tp . Optional [ tp . Iterable [ int ]] = None , feature_group_count : int = 1 , use_bias : bool = True , dtype : flax_module . Dtype = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features_out: number of convolution filters. kernel_size: shape of the convolutional kernel. For 1D convolution, the kernel size can be passed as an integer. For all other cases, it must be a sequence of integers. strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `'SAME'`, the string `'VALID'`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. input_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `inputs`. Convolution with input dilation `d` is equivalent to transposed convolution with stride `d`. kernel_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of the convolution kernel. Convolution with kernel dilation is also known as 'atrous convolution'. feature_group_count: integer, default 1. If specified divides the input features into groups. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer for the convolutional kernel. bias_init: initializer for the bias. \"\"\" self . features_out = features_out self . kernel_size = kernel_size self . strides = strides self . padding = padding self . input_dilation = input_dilation self . kernel_dilation = kernel_dilation self . feature_group_count = feature_group_count self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . kernel = None self . bias = None","title":"__init__()"},{"location":"api/nn/Dropout/","text":"treex.nn.Dropout Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training or self.frozen is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ . __call__ ( self , x , deterministic = None , rng = None ) special Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x : jnp . ndarray , deterministic : tp . Optional [ bool ] = None , rng = None ) -> jnp . ndarray : \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = ( not deterministic if deterministic is not None else self . training and not self . frozen ) if rng is None : rng = self . next_key () if training else self . next_key . key # call apply output = self . module . apply ( variables , x , deterministic = not training , rngs = { \"dropout\" : rng }, ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , rate , broadcast_dims = ()) special Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate : float , broadcast_dims : tp . Iterable [ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . rate = rate self . broadcast_dims = broadcast_dims self . next_key = KeySeq ()","title":"Dropout"},{"location":"api/nn/Dropout/#treexnndropout","text":"Create a dropout layer. Dropout is implemented as a wrapper over flax.linen.Dropout , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: deterministic is not a constructor argument, but remains a __call__ argument. self.training state is used to indicate how Dropout should behave, interally deterministic = not self.training or self.frozen is used unless deterministic is explicitly passed via __call__ . Dropout maintains an rng: Rng state which is used to generate random masks unless rng is passed via __call__ .","title":"treex.nn.Dropout"},{"location":"api/nn/Dropout/#treex.nn.dropout.Dropout.__call__","text":"Applies a random dropout mask to the input. Parameters: Name Type Description Default x ndarray the inputs that should be randomly masked. required deterministic Optional[bool] if false the inputs are scaled by 1 / (1 - rate) and masked, whereas if true, no mask is applied and the inputs are returned as is. None rng an optional jax.random.PRNGKey . By default self.rng will be used. None Returns: Type Description ndarray The masked inputs reweighted to preserve mean. Source code in treex/nn/dropout.py def __call__ ( self , x : jnp . ndarray , deterministic : tp . Optional [ bool ] = None , rng = None ) -> jnp . ndarray : \"\"\"Applies a random dropout mask to the input. Arguments: x: the inputs that should be randomly masked. deterministic: if false the inputs are scaled by `1 / (1 - rate)` and masked, whereas if true, no mask is applied and the inputs are returned as is. rng: an optional `jax.random.PRNGKey`. By default `self.rng` will be used. Returns: The masked inputs reweighted to preserve mean. \"\"\" variables = dict () training = ( not deterministic if deterministic is not None else self . training and not self . frozen ) if rng is None : rng = self . next_key () if training else self . next_key . key # call apply output = self . module . apply ( variables , x , deterministic = not training , rngs = { \"dropout\" : rng }, ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/Dropout/#treex.nn.dropout.Dropout.__init__","text":"Create a dropout layer. Parameters: Name Type Description Default rate float the dropout probability. ( not the keep rate!) required broadcast_dims Iterable[int] dimensions that will share the same dropout mask () Source code in treex/nn/dropout.py def __init__ ( self , rate : float , broadcast_dims : tp . Iterable [ int ] = (), ): \"\"\" Create a dropout layer. Arguments: rate: the dropout probability. (_not_ the keep rate!) broadcast_dims: dimensions that will share the same dropout mask \"\"\" self . rate = rate self . broadcast_dims = broadcast_dims self . next_key = KeySeq ()","title":"__init__()"},{"location":"api/nn/Embed/","text":"treex.nn.Embed A linear transformation applied over the last dimension of the input. Embed is implemented as a wrapper over flax.linen.Embed , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. __call__ ( self , x ) special Embeds the inputs along the last dimension. Parameters: Name Type Description Default inputs input data, all dimensions are considered batch dimensions. required Returns: Type Description ndarray Output which is embedded input data. The output shape follows the input, with an additional features dimension appended. Source code in treex/nn/embed.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Embeds the inputs along the last dimension. Arguments: inputs: input data, all dimensions are considered batch dimensions. Returns: Output which is embedded input data. The output shape follows the input, with an additional `features` dimension appended. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ()} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . embedding = params [ \"embedding\" ] assert self . embedding is not None params = { \"embedding\" : self . embedding } output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , num_embeddings , features , * , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, embedding_init=<function variance_scaling.<locals>.init at 0x7fec3ff74040>, name=None) special Parameters: Name Type Description Default num_embeddings int number of embeddings. required features int number of feature dimensions for each embedding. required dtype Any the dtype of the embedding vectors (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> embedding_init Callable[[Any, Iterable[int], Any], Any] embedding initializer. <function variance_scaling.<locals>.init at 0x7fec3ff74040> Source code in treex/nn/embed.py def __init__ ( self , num_embeddings : int , features : int , * , dtype : flax_module . Dtype = jnp . float32 , embedding_init : tp . Callable [ [ PRNGKey , Shape , Dtype ], Array ] = flax_module . default_embed_init , name : tp . Optional [ str ] = None , ): \"\"\" Arguments: num_embeddings: number of embeddings. features: number of feature dimensions for each embedding. dtype: the dtype of the embedding vectors (default: float32). embedding_init: embedding initializer. \"\"\" super () . __init__ ( name = name ) self . num_embeddings = num_embeddings self . features = features self . dtype = dtype self . embedding_init = embedding_init self . embedding = None","title":"Embed"},{"location":"api/nn/Embed/#treexnnembed","text":"A linear transformation applied over the last dimension of the input. Embed is implemented as a wrapper over flax.linen.Embed , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers.","title":"treex.nn.Embed"},{"location":"api/nn/Embed/#treex.nn.embed.Embed.__call__","text":"Embeds the inputs along the last dimension. Parameters: Name Type Description Default inputs input data, all dimensions are considered batch dimensions. required Returns: Type Description ndarray Output which is embedded input data. The output shape follows the input, with an additional features dimension appended. Source code in treex/nn/embed.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Embeds the inputs along the last dimension. Arguments: inputs: input data, all dimensions are considered batch dimensions. Returns: Output which is embedded input data. The output shape follows the input, with an additional `features` dimension appended. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ()} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . embedding = params [ \"embedding\" ] assert self . embedding is not None params = { \"embedding\" : self . embedding } output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/Embed/#treex.nn.embed.Embed.__init__","text":"Parameters: Name Type Description Default num_embeddings int number of embeddings. required features int number of feature dimensions for each embedding. required dtype Any the dtype of the embedding vectors (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> embedding_init Callable[[Any, Iterable[int], Any], Any] embedding initializer. <function variance_scaling.<locals>.init at 0x7fec3ff74040> Source code in treex/nn/embed.py def __init__ ( self , num_embeddings : int , features : int , * , dtype : flax_module . Dtype = jnp . float32 , embedding_init : tp . Callable [ [ PRNGKey , Shape , Dtype ], Array ] = flax_module . default_embed_init , name : tp . Optional [ str ] = None , ): \"\"\" Arguments: num_embeddings: number of embeddings. features: number of feature dimensions for each embedding. dtype: the dtype of the embedding vectors (default: float32). embedding_init: embedding initializer. \"\"\" super () . __init__ ( name = name ) self . num_embeddings = num_embeddings self . features = features self . dtype = dtype self . embedding_init = embedding_init self . embedding = None","title":"__init__()"},{"location":"api/nn/Flatten/","text":"treex.nn.Flatten","title":"Flatten"},{"location":"api/nn/Flatten/#treexnnflatten","text":"","title":"treex.nn.Flatten"},{"location":"api/nn/FlaxModule/","text":"treex.nn.FlaxModule","title":"FlaxModule"},{"location":"api/nn/FlaxModule/#treexnnflaxmodule","text":"","title":"treex.nn.FlaxModule"},{"location":"api/nn/GroupNorm/","text":"treex.nn.GroupNorm Group normalization Module (arxiv.org/abs/1803.08494). GroupNorm is implemented as a wrapper over flax.linen.GroupNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics. The user should either specify the total number of channel groups or the number of channels per group.. __call__ ( self , x ) special Normalizes the individual input over equally-sized group of channels. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes the individual input over equally-sized group of channels. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , num_groups = 32 , group_size = None , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>) special Parameters: Name Type Description Default num_groups Optional[int] the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. 32 group_size Optional[int] the number of channels in a group. None epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , num_groups : tp . Optional [ int ] = 32 , group_size : tp . Optional [ int ] = None , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: num_groups: the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. group_size: the number of channels in a group. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . num_groups = num_groups self . group_size = group_size self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"GroupNorm"},{"location":"api/nn/GroupNorm/#treexnngroupnorm","text":"Group normalization Module (arxiv.org/abs/1803.08494). GroupNorm is implemented as a wrapper over flax.linen.GroupNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. This op is similar to batch normalization, but statistics are shared across equally-sized groups of channels and not shared across batch dimension. Thus, group normalization does not depend on the batch composition and does not require maintaining internal state for storing statistics. The user should either specify the total number of channel groups or the number of channels per group..","title":"treex.nn.GroupNorm"},{"location":"api/nn/GroupNorm/#treex.nn.norm.GroupNorm.__call__","text":"Normalizes the individual input over equally-sized group of channels. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes the individual input over equally-sized group of channels. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/GroupNorm/#treex.nn.norm.GroupNorm.__init__","text":"Parameters: Name Type Description Default num_groups Optional[int] the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. 32 group_size Optional[int] the number of channels in a group. None epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , num_groups : tp . Optional [ int ] = 32 , group_size : tp . Optional [ int ] = None , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: num_groups: the total number of channel groups. The default value of 32 is proposed by the original group normalization paper. group_size: the number of channels in a group. epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . num_groups = num_groups self . group_size = group_size self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"__init__()"},{"location":"api/nn/HaikuModule/","text":"treex.nn.HaikuModule","title":"HaikuModule"},{"location":"api/nn/HaikuModule/#treexnnhaikumodule","text":"","title":"treex.nn.HaikuModule"},{"location":"api/nn/Lambda/","text":"treex.nn.Lambda A Module that applies a pure function to its input. __call__ ( self , x ) special Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequential.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f ( x ) __init__ ( self , f ) special Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequential.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], jnp . ndarray ]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f","title":"Lambda"},{"location":"api/nn/Lambda/#treexnnlambda","text":"A Module that applies a pure function to its input.","title":"treex.nn.Lambda"},{"location":"api/nn/Lambda/#treex.nn.sequential.Lambda.__call__","text":"Parameters: Name Type Description Default x ndarray The input to the function. required Returns: Type Description ndarray The output of the function. Source code in treex/nn/sequential.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Arguments: x: The input to the function. Returns: The output of the function. \"\"\" return self . f ( x )","title":"__call__()"},{"location":"api/nn/Lambda/#treex.nn.sequential.Lambda.__init__","text":"Parameters: Name Type Description Default f Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] A function to apply to the input. required Source code in treex/nn/sequential.py def __init__ ( self , f : tp . Callable [[ jnp . ndarray ], jnp . ndarray ]): \"\"\" Arguments: f: A function to apply to the input. \"\"\" self . f = f","title":"__init__()"},{"location":"api/nn/LayerNorm/","text":"treex.nn.LayerNorm LayerNorm Module. LayerNorm is implemented as a wrapper over flax.linen.LayerNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. It normalizes the activations of the layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1. __call__ ( self , x ) special Normalizes individual input on the last axis (channels) of the input data. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes individual input on the last axis (channels) of the input data. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , * , epsilon = 1e-05 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, use_bias=True, use_scale=True, bias_init=<function zeros at 0x7fec48f4fee0>, scale_init=<function ones at 0x7fec48f59040>) special Parameters: Name Type Description Default epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"LayerNorm"},{"location":"api/nn/LayerNorm/#treexnnlayernorm","text":"LayerNorm Module. LayerNorm is implemented as a wrapper over flax.linen.LayerNorm , its constructor arguments accept the same arguments including any Flax artifacts such as initializers. It normalizes the activations of the layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.","title":"treex.nn.LayerNorm"},{"location":"api/nn/LayerNorm/#treex.nn.norm.LayerNorm.__call__","text":"Normalizes individual input on the last axis (channels) of the input data. Parameters: Name Type Description Default x ndarray the input to be normalized. required Returns: Type Description ndarray Normalized inputs (the same shape as inputs). Source code in treex/nn/norm.py def __call__ ( self , x : jnp . ndarray , ) -> jnp . ndarray : \"\"\"Normalizes individual input on the last axis (channels) of the input data. Arguments: x: the input to be normalized. Returns: Normalized inputs (the same shape as inputs). \"\"\" if self . initializing (): variables = self . module . init ( next_key (), x , ) . unfreeze () # Extract collections if \"params\" in variables : params = variables [ \"params\" ] if self . use_bias : self . bias = params [ \"bias\" ] if self . use_scale : self . scale = params [ \"scale\" ] params = {} if self . use_bias : params [ \"bias\" ] = self . bias if self . use_scale : params [ \"scale\" ] = self . scale variables = dict ( params = params , ) # call apply output = self . module . apply ( variables , x , ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/LayerNorm/#treex.nn.norm.LayerNorm.__init__","text":"Parameters: Name Type Description Default epsilon float a small float added to variance to avoid dividing by zero. 1e-05 dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> use_bias bool if True, bias (beta) is added. True use_scale bool if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. True bias_init Callable[[Any, Tuple[int], Any], Any] initializer for bias, by default, zero. <function zeros at 0x7fec48f4fee0> scale_init Callable[[Any, Tuple[int], Any], Any] initializer for scale, by default, one. <function ones at 0x7fec48f59040> Source code in treex/nn/norm.py def __init__ ( self , * , epsilon : float = 1e-5 , dtype : flax_module . Dtype = jnp . float32 , use_bias : bool = True , use_scale : bool = True , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . zeros , scale_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . initializers . ones , ): \"\"\" Arguments: epsilon: a small float added to variance to avoid dividing by zero. dtype: the dtype of the computation (default: float32). use_bias: if True, bias (beta) is added. use_scale: if True, multiply by scale (gamma). When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer. bias_init: initializer for bias, by default, zero. scale_init: initializer for scale, by default, one. \"\"\" self . epsilon = epsilon self . dtype = dtype self . use_bias = use_bias self . use_scale = use_scale self . bias_init = bias_init self . scale_init = scale_init self . scale = None self . bias = None","title":"__init__()"},{"location":"api/nn/Linear/","text":"treex.nn.Linear A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out . __call__ ( self , x ) special Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ( axis_name = self . axis_name )} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output ) __init__ ( self , features_out , * , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>, name=None, axis_name=None) special Parameters: Name Type Description Default features_in the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/linear.py def __init__ ( self , features_out : int , * , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , name : tp . Optional [ str ] = None , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" super () . __init__ ( name = name ) self . features_out = features_out self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . axis_name = axis_name self . kernel = None self . bias = None","title":"Linear"},{"location":"api/nn/Linear/#treexnnlinear","text":"A linear transformation applied over the last dimension of the input. Linear is implemented as a wrapper over flax.linen.Dense , its constructor arguments accept almost the same arguments including any Flax artifacts such as initializers. Main differences: receives features_in as a first argument since shapes must be statically known. features argument is renamed to features_out .","title":"treex.nn.Linear"},{"location":"api/nn/Linear/#treex.nn.linear.Linear.__call__","text":"Applies a linear transformation to the inputs along the last dimension. Parameters: Name Type Description Default x ndarray The nd-array to be transformed. required Returns: Type Description ndarray The transformed input. Source code in treex/nn/linear.py def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Applies a linear transformation to the inputs along the last dimension. Arguments: x: The nd-array to be transformed. Returns: The transformed input. \"\"\" if self . initializing (): rngs = { \"params\" : next_key ( axis_name = self . axis_name )} variables = self . module . init ( rngs , x ) # Extract collections params = variables [ \"params\" ] . unfreeze () self . kernel = params [ \"kernel\" ] if self . use_bias : self . bias = params [ \"bias\" ] assert self . kernel is not None params = { \"kernel\" : self . kernel } if self . use_bias : assert self . bias is not None params [ \"bias\" ] = self . bias output = self . module . apply ({ \"params\" : params }, x ) return tp . cast ( jnp . ndarray , output )","title":"__call__()"},{"location":"api/nn/Linear/#treex.nn.linear.Linear.__init__","text":"Parameters: Name Type Description Default features_in the number of input features. required features_out int the number of output features. required use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/linear.py def __init__ ( self , features_out : int , * , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , name : tp . Optional [ str ] = None , axis_name : tp . Optional [ tp . Any ] = None ): \"\"\" Arguments: features_in: the number of input features. features_out: the number of output features. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" super () . __init__ ( name = name ) self . features_out = features_out self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init self . axis_name = axis_name self . kernel = None self . bias = None","title":"__init__()"},{"location":"api/nn/MLP/","text":"treex.nn.MLP A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear. __call__ ( self , x ) special Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py @to . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" last_layer_idx = len ( self . features ) - 1 for i , features_out in enumerate ( self . features ): x = Linear ( features_out = features_out , use_bias = self . use_bias , dtype = self . dtype , precision = self . precision , kernel_init = self . kernel_init , bias_init = self . bias_init , )( x ) if i < last_layer_idx : x = self . activation ( x ) return x __init__ ( self , features , activation =< jax . _src . custom_derivatives . custom_jvp object at 0x7fec48f27040 > , use_bias = True , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, precision=None, kernel_init=<function variance_scaling.<locals>.init at 0x7fec3ff65040>, bias_init=<function zeros at 0x7fec48f4fee0>) special Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7fec48f27040> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/mlp.py def __init__ ( self , features : tp . Sequence [ int ], activation : tp . Callable [[ jnp . ndarray ], jnp . ndarray ] = jax . nn . relu , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len ( features ) == 0 : raise ValueError ( \"features must have at least 1 element\" ) self . features = features self . activation = activation self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init","title":"MLP"},{"location":"api/nn/MLP/#treexnnmlp","text":"A Multi-Layer Perceptron (MLP) that applies a sequence of linear layers with a given activation (relu by default), the last layer is linear.","title":"treex.nn.MLP"},{"location":"api/nn/MLP/#treex.nn.mlp.MLP.__call__","text":"Applies the MLP to the input. Parameters: Name Type Description Default x ndarray input array. required Returns: Type Description ndarray The output of the MLP. Source code in treex/nn/mlp.py @to . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\" Applies the MLP to the input. Arguments: x: input array. Returns: The output of the MLP. \"\"\" last_layer_idx = len ( self . features ) - 1 for i , features_out in enumerate ( self . features ): x = Linear ( features_out = features_out , use_bias = self . use_bias , dtype = self . dtype , precision = self . precision , kernel_init = self . kernel_init , bias_init = self . bias_init , )( x ) if i < last_layer_idx : x = self . activation ( x ) return x","title":"__call__()"},{"location":"api/nn/MLP/#treex.nn.mlp.MLP.__init__","text":"Parameters: Name Type Description Default features Sequence[int] a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. required activation Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray] the activation function to use. <jax._src.custom_derivatives.custom_jvp object at 0x7fec48f27040> use_bias bool whether to add a bias to the output (default: True). True dtype Any the dtype of the computation (default: float32). <class 'jax._src.numpy.lax_numpy.float32'> precision Any numerical precision of the computation see jax.lax.Precision for details. None kernel_init Callable[[Any, Iterable[int], Any], Any] initializer function for the weight matrix. <function variance_scaling.<locals>.init at 0x7fec3ff65040> bias_init Callable[[Any, Iterable[int], Any], Any] initializer function for the bias. <function zeros at 0x7fec48f4fee0> Source code in treex/nn/mlp.py def __init__ ( self , features : tp . Sequence [ int ], activation : tp . Callable [[ jnp . ndarray ], jnp . ndarray ] = jax . nn . relu , use_bias : bool = True , dtype : tp . Any = jnp . float32 , precision : tp . Any = None , kernel_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . default_kernel_init , bias_init : tp . Callable [ [ flax_module . PRNGKey , flax_module . Shape , flax_module . Dtype ], flax_module . Array , ] = flax_module . zeros , ): \"\"\" Arguments: features: a sequence of L+1 integers, where L is the number of layers, the first integer is the number of input features and all subsequent integers are the number of output features of the respective layer. activation: the activation function to use. use_bias: whether to add a bias to the output (default: True). dtype: the dtype of the computation (default: float32). precision: numerical precision of the computation see `jax.lax.Precision` for details. kernel_init: initializer function for the weight matrix. bias_init: initializer function for the bias. \"\"\" if len ( features ) == 0 : raise ValueError ( \"features must have at least 1 element\" ) self . features = features self . activation = activation self . use_bias = use_bias self . dtype = dtype self . precision = precision self . kernel_init = kernel_init self . bias_init = bias_init","title":"__init__()"},{"location":"api/nn/Sequential/","text":"treex.nn.Sequential A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequential ( tx . Linear ( 2 , 32 ), jax . nn . relu , tx . Linear ( 32 , 8 ), jax . nn . relu , tx . Linear ( 8 , 4 ), ) . init ( 42 ) x = np . random . uniform ( size = ( 10 , 2 )) y = mlp ( x ) assert y . shape == ( 10 , 4 ) __init__ ( self , * layers ) special Parameters: Name Type Description Default *layers Union[Callable[..., jax._src.numpy.lax_numpy.ndarray], Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequential.py def __init__ ( self , * layers : tp . Union [ CallableModule , tp . Callable [[ jnp . ndarray ], jnp . ndarray ]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance ( layer , Module ) else Lambda ( layer ) for layer in layers ]","title":"Sequential"},{"location":"api/nn/Sequential/#treexnnsequential","text":"A Module that applies a sequence of Modules or functions in order. Examples: mlp = tx . Sequential ( tx . Linear ( 2 , 32 ), jax . nn . relu , tx . Linear ( 32 , 8 ), jax . nn . relu , tx . Linear ( 8 , 4 ), ) . init ( 42 ) x = np . random . uniform ( size = ( 10 , 2 )) y = mlp ( x ) assert y . shape == ( 10 , 4 )","title":"treex.nn.Sequential"},{"location":"api/nn/Sequential/#treex.nn.sequential.Sequential.__init__","text":"Parameters: Name Type Description Default *layers Union[Callable[..., jax._src.numpy.lax_numpy.ndarray], Callable[[jax._src.numpy.lax_numpy.ndarray], jax._src.numpy.lax_numpy.ndarray]] A list of layers or callables to apply to apply in sequence. () Source code in treex/nn/sequential.py def __init__ ( self , * layers : tp . Union [ CallableModule , tp . Callable [[ jnp . ndarray ], jnp . ndarray ]] ): \"\"\" Arguments: *layers: A list of layers or callables to apply to apply in sequence. \"\"\" self . layers = [ layer if isinstance ( layer , Module ) else Lambda ( layer ) for layer in layers ]","title":"__init__()"},{"location":"api/nn/sequence/","text":"treex.nn.sequence Creates a function that applies a sequence of callables to an input. Examples: class Block ( tx . Module ): linear : tx . Linear batch_norm : tx . BatchNorm dropout : tx . Dropout ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : return tx . sequence ( self . linear , self . batch_norm , self . dropout , jax . nn . relu , )( x ) Parameters: Name Type Description Default *layers Callable[..., jax._src.numpy.lax_numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequential.py def sequence ( * layers : CallableModule ) -> CallableModule : \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence ( x : jnp . ndarray ) -> jnp . ndarray : for layer in layers : x = layer ( x ) return x return _sequence","title":"sequence"},{"location":"api/nn/sequence/#treexnnsequence","text":"Creates a function that applies a sequence of callables to an input. Examples: class Block ( tx . Module ): linear : tx . Linear batch_norm : tx . BatchNorm dropout : tx . Dropout ... def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : return tx . sequence ( self . linear , self . batch_norm , self . dropout , jax . nn . relu , )( x ) Parameters: Name Type Description Default *layers Callable[..., jax._src.numpy.lax_numpy.ndarray] A sequence of callables to apply. () Source code in treex/nn/sequential.py def sequence ( * layers : CallableModule ) -> CallableModule : \"\"\" Creates a function that applies a sequence of callables to an input. Example: ```python class Block(tx.Module): linear: tx.Linear batch_norm: tx.BatchNorm dropout: tx.Dropout ... def __call__(self, x: jnp.ndarray) -> jnp.ndarray: return tx.sequence( self.linear, self.batch_norm, self.dropout, jax.nn.relu, )(x) ``` Arguments: *layers: A sequence of callables to apply. \"\"\" def _sequence ( x : jnp . ndarray ) -> jnp . ndarray : for layer in layers : x = layer ( x ) return x return _sequence","title":"treex.nn.sequence"},{"location":"api/regularizers/L1/","text":"treex.regularizers.L1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|","title":"L1"},{"location":"api/regularizers/L1/#treexregularizersl1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|","title":"treex.regularizers.L1"},{"location":"api/regularizers/L1L2/","text":"treex.regularizers.L1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 call ( self , parameters ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in treex/regularizers/l1l2.py def call ( self , parameters : tp . Any ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"L1L2"},{"location":"api/regularizers/L1L2/#treexregularizersl1l2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2","title":"treex.regularizers.L1L2"},{"location":"api/regularizers/L1L2/#treex.regularizers.l1l2.L1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in treex/regularizers/l1l2.py def call ( self , parameters : tp . Any ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization leaves = jax . tree_leaves ( parameters ) if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in leaves ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in leaves ) return regularization","title":"call()"},{"location":"api/regularizers/L2/","text":"treex.regularizers.L2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ```","title":"L2"},{"location":"api/regularizers/L2/#treexregularizersl2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 <span><span class=\"MathJax_Preview\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2</span><script type=\"math/tex\">\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 ```","title":"treex.regularizers.L2"},{"location":"user-guide/defining-modules/","text":"Basic Modules Modules in Treex usually follow this recipe: They inherit from tx.Module . Parameter-like fields are declared with a tx.TreePart subclass kind e.g. tx.Parameter.node() Hyper-parameters fields usually don't contain a declaration so they are static. Modules can be defined as dataclasses or regular classes without any limitations. While not mandatory, they usually perform shape inference. For example, a basic Module will tend to look like this: import treex as tx class Linear ( tx . Module ): # use Treeo's API to define Parameter nodes w : jnp . ndarray = tx . Parameter . node () b : jnp . ndarray = tx . Parameter . node () def __init__ ( self , features_out : int ): self . features_out = features_out def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : # init will call forward, we can know if we are inside it if self . initializing (): # `next_key` only available during `init` key = tx . next_key () # leverage shape inference self . w = jax . random . uniform ( key , shape = [ x . shape [ - 1 ], self . features_out ] ) self . b = jnp . zeros ( shape = [ self . features_out ]) # linear forward return jnp . dot ( x , self . w ) + self . b model = Linear ( 10 ) . init ( key = 42 , inputs = x ) Composite Modules Composite modules have the following characteristics: Their submodule fields are usually not declared, they are usually detected by their runtime value. Submodules are either created during __init__ or directly in __call__ when using @compact . class MLP ( tx . Module ): def __init__ ( self , features : Sequence [ int ]): self . features = features @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for units in self . features [: - 1 ]: x = Linear ( units )( x ) x = jax . nn . relu ( x ) return Linear ( self . features [ - 1 ])( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) If you don't want to use compact, you can create a list of Linear modules during __init__ and use them in __call__ . While in Pytorch you would create a ModuleList or ModuleDict to do this, in Treex you just need to use a (possibly generic) type annotation on the class field that contains a Module type (e.g. Linear ). class MLP ( tx . Module ): layers : List [ Linear ] # mandatory: registers field as a node def __init__ ( self , features : Sequence [ int ]): self . layers = [ Linear ( units ) for units in features ] @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for layer in self . layers [: - 1 ]: x = layer ( x ) x = jax . nn . relu ( x ) return self . layers [ - 1 ]( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) For more information check out Treeo's Node Policy .","title":"Defining Modules"},{"location":"user-guide/defining-modules/#basic-modules","text":"Modules in Treex usually follow this recipe: They inherit from tx.Module . Parameter-like fields are declared with a tx.TreePart subclass kind e.g. tx.Parameter.node() Hyper-parameters fields usually don't contain a declaration so they are static. Modules can be defined as dataclasses or regular classes without any limitations. While not mandatory, they usually perform shape inference. For example, a basic Module will tend to look like this: import treex as tx class Linear ( tx . Module ): # use Treeo's API to define Parameter nodes w : jnp . ndarray = tx . Parameter . node () b : jnp . ndarray = tx . Parameter . node () def __init__ ( self , features_out : int ): self . features_out = features_out def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : # init will call forward, we can know if we are inside it if self . initializing (): # `next_key` only available during `init` key = tx . next_key () # leverage shape inference self . w = jax . random . uniform ( key , shape = [ x . shape [ - 1 ], self . features_out ] ) self . b = jnp . zeros ( shape = [ self . features_out ]) # linear forward return jnp . dot ( x , self . w ) + self . b model = Linear ( 10 ) . init ( key = 42 , inputs = x )","title":"Basic Modules"},{"location":"user-guide/defining-modules/#composite-modules","text":"Composite modules have the following characteristics: Their submodule fields are usually not declared, they are usually detected by their runtime value. Submodules are either created during __init__ or directly in __call__ when using @compact . class MLP ( tx . Module ): def __init__ ( self , features : Sequence [ int ]): self . features = features @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for units in self . features [: - 1 ]: x = Linear ( units )( x ) x = jax . nn . relu ( x ) return Linear ( self . features [ - 1 ])( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) If you don't want to use compact, you can create a list of Linear modules during __init__ and use them in __call__ . While in Pytorch you would create a ModuleList or ModuleDict to do this, in Treex you just need to use a (possibly generic) type annotation on the class field that contains a Module type (e.g. Linear ). class MLP ( tx . Module ): layers : List [ Linear ] # mandatory: registers field as a node def __init__ ( self , features : Sequence [ int ]): self . layers = [ Linear ( units ) for units in features ] @tx . compact def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : for layer in self . layers [: - 1 ]: x = layer ( x ) x = jax . nn . relu ( x ) return self . layers [ - 1 ]( x ) model = MLP ([ 32 , 10 ]) . init ( key = 42 , inputs = x ) For more information check out Treeo's Node Policy .","title":"Composite Modules"},{"location":"user-guide/freezing-modules/","text":"Freezing Modules Module s have a .frozen property that specifies whether the module is frozen or not, Modules such as Dropout and BatchNorm which will behave differently based on its value. To switch between modes, use the .freeze() and .unfreeze() methods, they return a new Module whose frozen state and the state of all of its submodules (recursively) are set to the desired value. class ConvBlock ( tx . Module ): ... model = tx . Sequential ( ConvBlock ( 3 , 32 ), ConvBlock ( 32 , 64 ), ConvBlock ( 64 , 128 ), ... ) # train model ... # freeze some layers for layer in model . layers [: - 1 ]: layer . freeze ( inplace = True ) # fine-tune the model ... In this example we can leveraged the fact that Sequential has its submodules in .layers to freeze all but the last layers. Freezing modules is useful for tasks such as Transfer Learning where you want to keep most of the weights in a model unchange and train only a few of them on a new dataset. If you have a backbone you can just freeze the entire model. backbone = get_pretrained_model () backbone = backbone . freeze () model = tx . Sequential ( backbone , tx . Linear ( backbone . output_features , 10 ) ) . init ( 42 ) ... # Initialize optimizer with only the trainable set of parameters optimizer = optimizer . init ( model . trainable_parameters ()) ... @jax . jit def train_step ( model , x , y , optimizer ): # only differentiate w.r.t. parameters whose module is not frozen params = model . trainable_parameters () ( loss , model ), grads = loss_fn ( params , model , x , y ) ...","title":"Freezing Modules"},{"location":"user-guide/freezing-modules/#freezing-modules","text":"Module s have a .frozen property that specifies whether the module is frozen or not, Modules such as Dropout and BatchNorm which will behave differently based on its value. To switch between modes, use the .freeze() and .unfreeze() methods, they return a new Module whose frozen state and the state of all of its submodules (recursively) are set to the desired value. class ConvBlock ( tx . Module ): ... model = tx . Sequential ( ConvBlock ( 3 , 32 ), ConvBlock ( 32 , 64 ), ConvBlock ( 64 , 128 ), ... ) # train model ... # freeze some layers for layer in model . layers [: - 1 ]: layer . freeze ( inplace = True ) # fine-tune the model ... In this example we can leveraged the fact that Sequential has its submodules in .layers to freeze all but the last layers. Freezing modules is useful for tasks such as Transfer Learning where you want to keep most of the weights in a model unchange and train only a few of them on a new dataset. If you have a backbone you can just freeze the entire model. backbone = get_pretrained_model () backbone = backbone . freeze () model = tx . Sequential ( backbone , tx . Linear ( backbone . output_features , 10 ) ) . init ( 42 ) ... # Initialize optimizer with only the trainable set of parameters optimizer = optimizer . init ( model . trainable_parameters ()) ... @jax . jit def train_step ( model , x , y , optimizer ): # only differentiate w.r.t. parameters whose module is not frozen params = model . trainable_parameters () ( loss , model ), grads = loss_fn ( params , model , x , y ) ...","title":"Freezing Modules"},{"location":"user-guide/initialization/","text":"Initialization Initialization is performed by calling the Module.init method, init returns a new Module with all fields initialized. There are three initialization mechanisms for Modules in Treex: Using Module.initializing inside __call__ . Using a field Initializer object. Defining the rng_init method. Module.initializing During the forward pass you can check if the Module is initialized by calling self.initializing() and assign the fields that need to be initialized then and there. If you need access to a RNG key, you can use tx.next_key() inside a self.initializing() block ONLY, this will use the key passed during init . def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : if self . initializing (): self . w = jax . random . uniform ( key = tx . next_key (), shape = [ x . shape [ - 1 ], self . features_out ] ) The benefit of this approach is that you can leverage the shape of the input to initialize the parameters. This method is used by most core layers provided by Treex. Field Initializer Initializer s contain a function that take a key and return an initial value, init will replace leaves with Initializer objects with the initial value their function outputs for the given key: class MyModule ( tx . Module ): a : Union [ tx . Initializer , jnp . ndarray ] = tx . Parameter . node () def __init__ ( self ): self . a = tx . Initializer ( lambda key : jax . random . uniform ( key , shape = ( 1 ,)) ) module = MyModule () . init ( 42 ) # > MyModule(a=array([0.034...])) This method is use for fields who's intialization doesn't require shape inference and doesn't depend on information of other fields. rng_init If you Module doesn't require shape inference but Initializer is not enough, you can override the rng_init method. class MyModule ( tx . Module ): a : Optional [ jnp . ndarray ] = tx . Parameter . node () b : Optional [ jnp . ndarray ] = tx . Parameter . node () def __init__ ( self ): self . a = None self . b = None def rng_init ( self ): self . a = jax . random . uniform ( tx . next_key (), shape = ( 1 ,))) self . b = 10.0 * self . a + jax . random . normal ( key , shape = ( 1 ,)) module = MyModule () . init ( 42 ) module # MyModule(a=array([0.3...]), b=array([3.2...])) Intialization order The order of initialization is: First all field Initializers are called. Second all rng_init methods are called. Lastly the __call__ method is called.","title":"Initialization"},{"location":"user-guide/initialization/#initialization","text":"Initialization is performed by calling the Module.init method, init returns a new Module with all fields initialized. There are three initialization mechanisms for Modules in Treex: Using Module.initializing inside __call__ . Using a field Initializer object. Defining the rng_init method.","title":"Initialization"},{"location":"user-guide/initialization/#moduleinitializing","text":"During the forward pass you can check if the Module is initialized by calling self.initializing() and assign the fields that need to be initialized then and there. If you need access to a RNG key, you can use tx.next_key() inside a self.initializing() block ONLY, this will use the key passed during init . def __call__ ( self , x : jnp . ndarray ) -> jnp . ndarray : if self . initializing (): self . w = jax . random . uniform ( key = tx . next_key (), shape = [ x . shape [ - 1 ], self . features_out ] ) The benefit of this approach is that you can leverage the shape of the input to initialize the parameters. This method is used by most core layers provided by Treex.","title":"Module.initializing"},{"location":"user-guide/initialization/#field-initializer","text":"Initializer s contain a function that take a key and return an initial value, init will replace leaves with Initializer objects with the initial value their function outputs for the given key: class MyModule ( tx . Module ): a : Union [ tx . Initializer , jnp . ndarray ] = tx . Parameter . node () def __init__ ( self ): self . a = tx . Initializer ( lambda key : jax . random . uniform ( key , shape = ( 1 ,)) ) module = MyModule () . init ( 42 ) # > MyModule(a=array([0.034...])) This method is use for fields who's intialization doesn't require shape inference and doesn't depend on information of other fields.","title":"Field Initializer"},{"location":"user-guide/initialization/#rng_init","text":"If you Module doesn't require shape inference but Initializer is not enough, you can override the rng_init method. class MyModule ( tx . Module ): a : Optional [ jnp . ndarray ] = tx . Parameter . node () b : Optional [ jnp . ndarray ] = tx . Parameter . node () def __init__ ( self ): self . a = None self . b = None def rng_init ( self ): self . a = jax . random . uniform ( tx . next_key (), shape = ( 1 ,))) self . b = 10.0 * self . a + jax . random . normal ( key , shape = ( 1 ,)) module = MyModule () . init ( 42 ) module # MyModule(a=array([0.3...]), b=array([3.2...]))","title":"rng_init"},{"location":"user-guide/initialization/#intialization-order","text":"The order of initialization is: First all field Initializers are called. Second all rng_init methods are called. Lastly the __call__ method is called.","title":"Intialization order"},{"location":"user-guide/intro/","text":"User Guide Module is the main construct in Treex, it inherits from treeo.Tree and adds a couple of convenient methods. We recommend that you review the core concepts of Treeo but we will provide a brief overview. Terminology These are the core concepts from Treeo: Type Annotation : ( type hints ) types you set while defining a variable after the : symbol. Field Declaration : default values for class variables that are set using the field function. Node Field : A field that is declared as a node, that is, its content is part of the tree leaves. Static Field : A field that is declared as a static, that is, its content is not part of the leaves. Field Kind : An associated type, separate from the type annotation, that gives semantic meaning to the field. In code these terms map to the following: class MyModule ( tx . Module ): # field annotation ------------declaration--------------- # v v v v some_field : jnp . ndarray = to . field ( node = True , kind = tx . Parameter ) # ^ ^ # node status field kind Here if node=False it would mean that the field is static, else is a node. The previous is written more compactly as: class MyModule ( tx . Module ): some_field : jnp . ndarray = tx . Parameter . node ()","title":"User Guide"},{"location":"user-guide/intro/#user-guide","text":"Module is the main construct in Treex, it inherits from treeo.Tree and adds a couple of convenient methods. We recommend that you review the core concepts of Treeo but we will provide a brief overview.","title":"User Guide"},{"location":"user-guide/intro/#terminology","text":"These are the core concepts from Treeo: Type Annotation : ( type hints ) types you set while defining a variable after the : symbol. Field Declaration : default values for class variables that are set using the field function. Node Field : A field that is declared as a node, that is, its content is part of the tree leaves. Static Field : A field that is declared as a static, that is, its content is not part of the leaves. Field Kind : An associated type, separate from the type annotation, that gives semantic meaning to the field. In code these terms map to the following: class MyModule ( tx . Module ): # field annotation ------------declaration--------------- # v v v v some_field : jnp . ndarray = to . field ( node = True , kind = tx . Parameter ) # ^ ^ # node status field kind Here if node=False it would mean that the field is static, else is a node. The previous is written more compactly as: class MyModule ( tx . Module ): some_field : jnp . ndarray = tx . Parameter . node ()","title":"Terminology"},{"location":"user-guide/optimizer/","text":"Optimizer Optax is an amazing library however, its optimizers are not pytrees, this means that their state and computation are separate and you cannot jit them. To solve this Treex provides a Optimizer class which inherits from treeo.Tree and can wrap any Optax optimizer. Optimizer follows a similar API as optax.GradientTransformation except that: There is no separate opt_state , the Optimizer contains the state. update by default applies the update the parameters, if you want the gradient updates instead you can set apply_updates=False . update also updates the internal state of the Optimizer in-place. While in Optax you would define something like this: def main (): ... optimizer = optax . adam ( 1e-3 ) opt_state = optimizer . init ( params ) ... @partial ( jax . jit , static_argnums = ( 4 ,)) def train_step ( model , x , y , opt_state , optimizer ): # optimizer has to be static ... updates , opt_state = optimizer . update ( grads , opt_state , params ) params = optax . apply_updates ( params , updates ) ... return model , loss , opt_state With tx.Optimizer you it can be simplified to: def main (): ... optimizer = tx . Optimizer ( optax . adam ( 1e-3 )) . init ( params ) ... jax . jit # no static_argnums needed def train_step ( model , x , y , optimizer ): ... params = optimizer . update ( grads , params ) ... return model , loss , optimizer Notice that since tx.Optimizer is a Pytree it was passed through jit naturally without the need to specify static_argnums .","title":"Optimizer"},{"location":"user-guide/optimizer/#optimizer","text":"Optax is an amazing library however, its optimizers are not pytrees, this means that their state and computation are separate and you cannot jit them. To solve this Treex provides a Optimizer class which inherits from treeo.Tree and can wrap any Optax optimizer. Optimizer follows a similar API as optax.GradientTransformation except that: There is no separate opt_state , the Optimizer contains the state. update by default applies the update the parameters, if you want the gradient updates instead you can set apply_updates=False . update also updates the internal state of the Optimizer in-place. While in Optax you would define something like this: def main (): ... optimizer = optax . adam ( 1e-3 ) opt_state = optimizer . init ( params ) ... @partial ( jax . jit , static_argnums = ( 4 ,)) def train_step ( model , x , y , opt_state , optimizer ): # optimizer has to be static ... updates , opt_state = optimizer . update ( grads , opt_state , params ) params = optax . apply_updates ( params , updates ) ... return model , loss , opt_state With tx.Optimizer you it can be simplified to: def main (): ... optimizer = tx . Optimizer ( optax . adam ( 1e-3 )) . init ( params ) ... jax . jit # no static_argnums needed def train_step ( model , x , y , optimizer ): ... params = optimizer . update ( grads , params ) ... return model , loss , optimizer Notice that since tx.Optimizer is a Pytree it was passed through jit naturally without the need to specify static_argnums .","title":"Optimizer"},{"location":"user-guide/pytrees/","text":"","title":"Pytrees"},{"location":"user-guide/state-management/","text":"State Management Treex takes a \"direct\" approach to state management, i.e., state is updated in-place by the Module whenever it needs to. For example, this module will calculate the running average of its input: class Average ( tx . Module ): count : jnp . ndarray = tx . State . node () total : jnp . ndarray = tx . State . node () def __init__ ( self ): self . count = jnp . array ( 0 ) self . total = jnp . array ( 0.0 ) def __call__ ( self , x ): self . count += np . prod ( x . shape ) self . total += jnp . sum ( x ) return self . total / self . count Treex Modules that require random state will often keep a rng key internally and update it in-place when needed: class Dropout ( tx . Module ): key : jnp . ndarray = tx . Rng . node () def __init__ ( self , key : jnp . ndarray ): self . key = key ... def __call__ ( self , x ): key , self . key = jax . random . split ( self . key ) ... Finally Optimizer also performs inplace updates inside the update method, here is a sketch of how it works: class Optimizer ( tx . Module ): opt_state : Any = tx . OptState . node () optimizer : optax . GradientTransformation def update ( self , grads , params ): ... updates , self . opt_state = self . optimizer . update ( grads , self . opt_state , params ) ... As you the the opt_state contains the Optax's optimizer state and is update inplace on every call to update . What is the catch? State management is one of the most challenging things in JAX because of its functional nature, however here it seems effortless. What is the catch? As always there are trade-offs to consider: The Pytree approach requires the user to be aware that if a Module is stateful it should propagate its state by having mutated object be outputs of jitted functions, on the other hand implementation and usage if very simple. Frameworks like Flax and Haiku are more explicit as to when state is updated but introduce a lot of complexity to do so. A standard solution to this problem is: always output the Module to update its state . For example, a typical loss function that contains a stateful model would look like this: @partial ( jax . value_and_grad , has_aux = True ) def loss_fn ( params , model , x , y ): model = model . update ( params ) preds = model ( x ) loss = jnp . mean (( preds - y ) ** 2 ) return loss , model params = model . parameters () ( loss , model ), grads = loss_fn ( params , model , x , y ) ... Here model is returned along with the loss through value_and_grad to update model on the outside thus persisting any changes to the state performed on the inside.","title":"State Management"},{"location":"user-guide/state-management/#state-management","text":"Treex takes a \"direct\" approach to state management, i.e., state is updated in-place by the Module whenever it needs to. For example, this module will calculate the running average of its input: class Average ( tx . Module ): count : jnp . ndarray = tx . State . node () total : jnp . ndarray = tx . State . node () def __init__ ( self ): self . count = jnp . array ( 0 ) self . total = jnp . array ( 0.0 ) def __call__ ( self , x ): self . count += np . prod ( x . shape ) self . total += jnp . sum ( x ) return self . total / self . count Treex Modules that require random state will often keep a rng key internally and update it in-place when needed: class Dropout ( tx . Module ): key : jnp . ndarray = tx . Rng . node () def __init__ ( self , key : jnp . ndarray ): self . key = key ... def __call__ ( self , x ): key , self . key = jax . random . split ( self . key ) ... Finally Optimizer also performs inplace updates inside the update method, here is a sketch of how it works: class Optimizer ( tx . Module ): opt_state : Any = tx . OptState . node () optimizer : optax . GradientTransformation def update ( self , grads , params ): ... updates , self . opt_state = self . optimizer . update ( grads , self . opt_state , params ) ... As you the the opt_state contains the Optax's optimizer state and is update inplace on every call to update .","title":"State Management"},{"location":"user-guide/state-management/#what-is-the-catch","text":"State management is one of the most challenging things in JAX because of its functional nature, however here it seems effortless. What is the catch? As always there are trade-offs to consider: The Pytree approach requires the user to be aware that if a Module is stateful it should propagate its state by having mutated object be outputs of jitted functions, on the other hand implementation and usage if very simple. Frameworks like Flax and Haiku are more explicit as to when state is updated but introduce a lot of complexity to do so. A standard solution to this problem is: always output the Module to update its state . For example, a typical loss function that contains a stateful model would look like this: @partial ( jax . value_and_grad , has_aux = True ) def loss_fn ( params , model , x , y ): model = model . update ( params ) preds = model ( x ) loss = jnp . mean (( preds - y ) ** 2 ) return loss , model params = model . parameters () ( loss , model ), grads = loss_fn ( params , model , x , y ) ... Here model is returned along with the loss through value_and_grad to update model on the outside thus persisting any changes to the state performed on the inside.","title":"What is the catch?"},{"location":"user-guide/training-state/","text":"Training State Modules have a training: bool property that specifies whether the module is in training mode or not. This property conditions the behavior of Modules such as Dropout and BatchNorm , which behave differently between training and evaluation. # training loop for step in range ( 1000 ): loss , model , opt_state = train_step ( model , x , y , opt_state ) # prepare for evaluation model = model . eval () # make predictions preds = model ( X_test ) To switch between modes, use the .train() and .eval() methods, they return a new Module whose training state and the state of all of its submodules (recursively) are set to the desired value.","title":"Training State"},{"location":"user-guide/training-state/#training-state","text":"Modules have a training: bool property that specifies whether the module is in training mode or not. This property conditions the behavior of Modules such as Dropout and BatchNorm , which behave differently between training and evaluation. # training loop for step in range ( 1000 ): loss , model , opt_state = train_step ( model , x , y , opt_state ) # prepare for evaluation model = model . eval () # make predictions preds = model ( X_test ) To switch between modes, use the .train() and .eval() methods, they return a new Module whose training state and the state of all of its submodules (recursively) are set to the desired value.","title":"Training State"},{"location":"user-guide/api/filter/","text":"Filter The filter method allows you to select a subtree by filtering based on a kind , all leaves whose field kind is a subclass of such type are kept, the rest are set to a special Nothing value. tree = MyModule ( a = 1 , b = 2 ) module . filter ( Parameter ) # MyModule(a=1, b=Nothing) module . filter ( BatchStat ) # MyModule(a=Nothing, b=2) Since Nothing is an empty Pytree it gets ignored by tree operations, this effectively allows you to easily operate on a subset of the fields: negative = lambda x : - x jax . tree_map ( negative , module . filter ( Parameter )) # MyModule(a=-1, b=Nothing) jax . tree_map ( negative , module . filter ( BatchStat )) # MyModule(a=Nothing, b=-2) Shortcuts As simple filters using the standard TreePart types are used often, some shortcuts are provided: Shortcut Equivalence .parameters() .filter(tx.Parameter) .batch_stats() .filter(tx.BatchStat) .rngs() .filter(tx.RNG) .model_states() .filter(tx.ModelState) .states() .filter(tx.State) .metrics() .filter(tx.Metric) .losses() .filter(tx.Loss) .logs() .filter(tx.Log) Based on this the first example can be written as: module . parameters () # MyModule(a=1, b=Nothing) module . batch_stats () # MyModule(a=Nothing, b=2) filter predicates If you need to do more complex filtering, you can pass callables with the signature FieldInfo -> bool instead of types: # all Parameters whose field name is \"kernel\" module . filter ( lambda field : issubclass ( field . kind , Parameter ) and field . name == \"kernel\" ) # MyModule(a=Nothing, b=Nothing) multiple filters You can some queries by using multiple filters as *args . For a field to be kept it will required that all filters pass . Since passing types by themselves are \"kind filters\", one of the previous examples could be written as: # all Parameters whose field name is \"kernel\" module . filter ( Parameter , lambda field : field . name == \"kernel\" ) # MyModule(a=Nothing, b=Nothing) inplace If inplace is True , the input obj is mutated and returned. You can only update inplace if the input obj has a __dict__ attribute, else a TypeError is raised.","title":"Filter"},{"location":"user-guide/api/filter/#filter","text":"The filter method allows you to select a subtree by filtering based on a kind , all leaves whose field kind is a subclass of such type are kept, the rest are set to a special Nothing value. tree = MyModule ( a = 1 , b = 2 ) module . filter ( Parameter ) # MyModule(a=1, b=Nothing) module . filter ( BatchStat ) # MyModule(a=Nothing, b=2) Since Nothing is an empty Pytree it gets ignored by tree operations, this effectively allows you to easily operate on a subset of the fields: negative = lambda x : - x jax . tree_map ( negative , module . filter ( Parameter )) # MyModule(a=-1, b=Nothing) jax . tree_map ( negative , module . filter ( BatchStat )) # MyModule(a=Nothing, b=-2)","title":"Filter"},{"location":"user-guide/api/filter/#shortcuts","text":"As simple filters using the standard TreePart types are used often, some shortcuts are provided: Shortcut Equivalence .parameters() .filter(tx.Parameter) .batch_stats() .filter(tx.BatchStat) .rngs() .filter(tx.RNG) .model_states() .filter(tx.ModelState) .states() .filter(tx.State) .metrics() .filter(tx.Metric) .losses() .filter(tx.Loss) .logs() .filter(tx.Log) Based on this the first example can be written as: module . parameters () # MyModule(a=1, b=Nothing) module . batch_stats () # MyModule(a=Nothing, b=2)","title":"Shortcuts"},{"location":"user-guide/api/filter/#filter-predicates","text":"If you need to do more complex filtering, you can pass callables with the signature FieldInfo -> bool instead of types: # all Parameters whose field name is \"kernel\" module . filter ( lambda field : issubclass ( field . kind , Parameter ) and field . name == \"kernel\" ) # MyModule(a=Nothing, b=Nothing)","title":"filter predicates"},{"location":"user-guide/api/filter/#multiple-filters","text":"You can some queries by using multiple filters as *args . For a field to be kept it will required that all filters pass . Since passing types by themselves are \"kind filters\", one of the previous examples could be written as: # all Parameters whose field name is \"kernel\" module . filter ( Parameter , lambda field : field . name == \"kernel\" ) # MyModule(a=Nothing, b=Nothing)","title":"multiple filters"},{"location":"user-guide/api/filter/#inplace","text":"If inplace is True , the input obj is mutated and returned. You can only update inplace if the input obj has a __dict__ attribute, else a TypeError is raised.","title":"inplace"},{"location":"user-guide/api/map/","text":"Map Applies a function to all leaves in the Module using jax.tree_map . If filters are given then the function will be applied only to the subset of leaves that match the filters. For example, if we want to zero all batch stats we can do: Example: @dataclass class MyModule ( x . Module ): a : int = tx . Parameter . node () b : int = tx . BatchStat . node () module = MyModule ( a = 1 , b = 2 ) module . map ( lambda _ : 0 , tx . BatchStat ) # MyTree(a=1, b=0) map is equivalent to filter -> tree_map -> merge in sequence. If inplace is True , the input obj is mutated and returned. You can only update inplace if the input obj has a __dict__ attribute, else a TypeError is raised.","title":"Map"},{"location":"user-guide/api/map/#map","text":"Applies a function to all leaves in the Module using jax.tree_map . If filters are given then the function will be applied only to the subset of leaves that match the filters. For example, if we want to zero all batch stats we can do: Example: @dataclass class MyModule ( x . Module ): a : int = tx . Parameter . node () b : int = tx . BatchStat . node () module = MyModule ( a = 1 , b = 2 ) module . map ( lambda _ : 0 , tx . BatchStat ) # MyTree(a=1, b=0) map is equivalent to filter -> tree_map -> merge in sequence. If inplace is True , the input obj is mutated and returned. You can only update inplace if the input obj has a __dict__ attribute, else a TypeError is raised.","title":"Map"},{"location":"user-guide/api/merge/","text":"Merge Creates a new Module with the same structure but its values merged based on the values from the incoming Modules. @dataclass class MyModule ( tx . Module ): a : int = tx . field ( node = True , kind = Parameter ) b : int = tx . field ( node = True , kind = BatchStat ) m1 = MyModule ( x = Nothing , y = 2 , z = 3 ) m2 = MyModule ( x = 1 , y = Nothing , z = 4 ) m1 . merge ( m2 ) # MyModule(x=1, y=2, z=4) Updates are performed using the following rules: For a list of equivalent leaves l1, l2, ..., ln , it returns the first non- Nothing leaf from right to left. If no flatten_mode() context manager is active and flatten_mode is not given, all fields will be updated. If flatten_mode=\"normal\" is set then static fields won't be updated and the output will have the exact same static components as the first input ( obj ). When using merge with multiple Modules the following equivalence holds: m1.merge(m2, m3) = m1.merge(m2.merge(m3)) If you want to merge the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map ( lambda x : 2 * x , self ) Instead do this: def double_params ( self ): doubled = jax . tree_map ( lambda x : 2 * x , self ) self . merge ( doubled , inplace = True ) If inplace is True , the input obj is mutated and returned. You can only merge inplace if the input obj has a __dict__ attribute, else a TypeError is raised. If ignore_static is True , static fields (according to the flattening mode) will be bypassed during the merge process, the final output will have the same static components as the first input ( obj ). This strategy is a bit less safe in general as it will flatten all trees using jax.tree_leaves instead of PyTreeDef.flatten_up_to , this skips some checks so it effectively ignores their static components, the only requirement is that the flattened struture of all trees matches.","title":"Merge"},{"location":"user-guide/api/merge/#merge","text":"Creates a new Module with the same structure but its values merged based on the values from the incoming Modules. @dataclass class MyModule ( tx . Module ): a : int = tx . field ( node = True , kind = Parameter ) b : int = tx . field ( node = True , kind = BatchStat ) m1 = MyModule ( x = Nothing , y = 2 , z = 3 ) m2 = MyModule ( x = 1 , y = Nothing , z = 4 ) m1 . merge ( m2 ) # MyModule(x=1, y=2, z=4) Updates are performed using the following rules: For a list of equivalent leaves l1, l2, ..., ln , it returns the first non- Nothing leaf from right to left. If no flatten_mode() context manager is active and flatten_mode is not given, all fields will be updated. If flatten_mode=\"normal\" is set then static fields won't be updated and the output will have the exact same static components as the first input ( obj ). When using merge with multiple Modules the following equivalence holds: m1.merge(m2, m3) = m1.merge(m2.merge(m3)) If you want to merge the current module instead of creating a new one use inplace=True . This is useful when applying transformation inside a method where reassigning self is not possible: def double_params ( self ): # this is not doing what you expect self = jax . tree_map ( lambda x : 2 * x , self ) Instead do this: def double_params ( self ): doubled = jax . tree_map ( lambda x : 2 * x , self ) self . merge ( doubled , inplace = True ) If inplace is True , the input obj is mutated and returned. You can only merge inplace if the input obj has a __dict__ attribute, else a TypeError is raised. If ignore_static is True , static fields (according to the flattening mode) will be bypassed during the merge process, the final output will have the same static components as the first input ( obj ). This strategy is a bit less safe in general as it will flatten all trees using jax.tree_leaves instead of PyTreeDef.flatten_up_to , this skips some checks so it effectively ignores their static components, the only requirement is that the flattened struture of all trees matches.","title":"Merge"}]}