{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Treex\n",
    "\n",
    "**Main features**:\n",
    "* Modules contain their parameters\n",
    "* Easy transfer learning\n",
    "* Simple initialization\n",
    "* No metaclass magic\n",
    "* No apply method\n",
    "* No need special versions of `vmap`, `jit`, and friends.\n",
    "\n",
    "We will showcase each of the above features by creating a very contrived but complete module that will use everything from parameters, states, and random states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import treex as tx\n",
    "\n",
    "\n",
    "class NoisyLinear(tx.Module):\n",
    "    # tree parts are defined by treex annotations\n",
    "    w: tx.Parameter\n",
    "    b: tx.Parameter\n",
    "    rng: tx.Rng  # tx.Rng inherits from tx.State\n",
    "\n",
    "    # other annotations are possible but ignored by type\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, din, dout):\n",
    "        # Initializers only expect RNG key\n",
    "        self.w = tx.Initializer(lambda k: jax.random.uniform(k, shape=(din, dout)))\n",
    "        self.b = tx.Initializer(lambda k: jax.random.uniform(k, shape=(dout,)))\n",
    "\n",
    "        # random state is JUST state, we can keep it locally\n",
    "        self.rng = tx.Initializer(lambda k: k)\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert isinstance(self.rng, jnp.ndarray)\n",
    "\n",
    "        # update state in place\n",
    "        key, self.rng = jax.random.split(self.rng, 2)\n",
    "\n",
    "        # your typical linear operation\n",
    "        y = jnp.dot(x, self.w) + self.b\n",
    "\n",
    "        # add noise for fun\n",
    "        return y + 0.8 * jax.random.normal(key, shape=y.shape)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"NoisyLinear(w={self.w}, b={self.b}, rng={self.rng})\"\n",
    "\n",
    "\n",
    "model = NoisyLinear(1, 1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Initialization is straightforward. The only thing you need to do is to call `init` on your module with a random key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "model = model.init(key=42)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we will be reusing the previous NoisyLinear model, and we will create an optax optimizer that is used to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "optimizer = optax.adam(1e-2)\n",
    "\n",
    "params = model.slice(tx.Parameter)\n",
    "states = model.slice(tx.State)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "print(f\"{params=}\")\n",
    "print(f\"{states=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice that we are already splitting the model into `params` and `states` since we only need to pass the `params` to the optimizer. Next, we will create the loss function, it will take the model parts and the data parts and return the loss plus the new states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.value_and_grad, has_aux=True)\n",
    "def loss_fn(params: NoisyLinear, states: NoisyLinear, x, y):\n",
    "    # merge params and states to get a full model\n",
    "    model: NoisyLinear = params.merge(states)\n",
    "    # apply model\n",
    "    pred_y = model(x)\n",
    "    # MSE loss\n",
    "    loss = jnp.mean((y - pred_y) ** 2)\n",
    "    # new states\n",
    "    states = model.slice(tx.State)\n",
    "\n",
    "    return loss, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Notice that we are merging the `params` and `states` into the complete model since we need everything in place to perform the forward pass. Also, we return the updated states from the model. The above steps are required because JAX functional API requires us to be explicit about state management.\n",
    "\n",
    "**Note**: inside `loss_fn` (wrapped by `value_and_grad`) module can behave like a regular mutable Python object. However, every time it is treated as a pytree a new reference will be created in `jit`, `grad`, `vmap`, etc. It is essential to consider this when using functions like `vmap` inside a module, as JAX will need specific bookkeeping to manage the state correctly.\n",
    "\n",
    "Next, we will implement the `update` function, it will look indistinguishable from your standard Haiku update, which also separates weights into `params` and `states`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params: NoisyLinear, states: NoisyLinear, opt_state, x, y):\n",
    "    (loss, states), grads = loss_fn(params, states, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "\n",
    "    # use regular optax\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, states, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Before we start training lets get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def get_data(dataset_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x = np.random.normal(size=(dataset_size, 1))\n",
    "    y = 5 * x - 2 + 0.4 * np.random.normal(size=(dataset_size, 1))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    data: Tuple[np.ndarray, np.ndarray], batch_size: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    idx = np.random.choice(len(data[0]), batch_size)\n",
    "    return jax.tree_map(lambda x: x[idx], data)\n",
    "\n",
    "\n",
    "data = get_data(1000)\n",
    "\n",
    "plt.scatter(data[0], data[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Finally, we create a simple training loop that performs a few thousand updates and merge `params` and `states` back into a single `model` at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10_000\n",
    "\n",
    "for step in range(steps):\n",
    "    x, y = get_batch(data, batch_size=32)\n",
    "\n",
    "    params, states, opt_state, loss = update(params, states, opt_state, x, y)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"[{step}] loss = {loss}\")\n",
    "\n",
    "# get the final model\n",
    "model = params.merge(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now lets generate some test data and see how our model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.linspace(data[0].min(), data[0].max(), 100)[:, None]\n",
    "y_pred = model(X_test)\n",
    "\n",
    "plt.scatter(data[0], data[1], label=\"data\", color=\"k\")\n",
    "plt.plot(X_test, y_pred, label=\"prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can see that the model has learned the general trend, but because of the `NoisyLinear` modules we have a bit of noise in the predictions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0f3d5a3883d8636fe87fe002a5fbfda4d626248072973d7503d976a08fb4ac9"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}